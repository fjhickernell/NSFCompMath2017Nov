% !TEX TS-program = PDFLatexBibtex
%&LaTeX
%Greg and Fred's NSF Grant Dec 2014
\documentclass[11pt]{NSFamsart}
\usepackage{latexsym,amsfonts,amsmath,epsfig,multirow,stackrel,natbib,tabularx,enumitem,xspace}

% This package prints the labels in the margin
%\usepackage[notref,notcite]{showkeys}


%\pagestyle{empty}
\thispagestyle{plain}
\pagestyle{plain}

\headsep-0.6in
%\headsep-0.45in

\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\textheight9in
%\textheight9.1in

\providecommand{\FJHickernell}{Hickernell}
\newcommand{\hatf}{\hat{f}}
\newcommand{\hatg}{\hat{g}}
\newcommand{\tf}{\tilde{f}}
\newcommand{\tbf}{\tilde{\bff}}
\DeclareMathOperator{\cost}{cost}
\DeclareMathOperator{\loss}{loss}
\DeclareMathOperator{\lof}{lof}
\DeclareMathOperator{\reg}{reg}


\def\reals{{\mathbb{R}}}
\def\field{{\mathbb{F}}}
\def\complex{{\mathbb{C}}}
\def\naturals{{\mathbb{N}}}
\def\integer{{\mathbb{Z}}}
\def\expect{{\mathbb{E}}}
\def\il{\left<}
\def\ir{\right>}
\def\e{\varepsilon}
\def\g{\gamma}
\def\l{\lambda}
\def\b{\beta}
\def\a{\alpha}
\def\lall{\Lambda^{{\rm all}}}
\def\lstd{\Lambda^{{\rm std}}}

\newcommand{\bbE}{\mathbb{E}}
\newcommand{\tQ}{\widetilde{Q}}
\newcommand{\mB}{\mathsf{B}}
\newcommand{\mC}{\mathsf{C}}
\newcommand{\mG}{\mathsf{G}}
\newcommand{\mH}{\mathsf{H}}
\newcommand{\mI}{\mathsf{I}}
\newcommand{\mK}{\mathsf{K}}
\newcommand{\tmK}{\widetilde{\mathsf{K}}}
\newcommand{\mL}{\mathsf{L}}
\newcommand{\mM}{\mathsf{M}}
\newcommand{\mP}{\mathsf{P}}
\newcommand{\mQ}{\mathsf{Q}}
\newcommand{\mR}{\mathsf{R}}
\newcommand{\mX}{\mathsf{X}}
\newcommand{\mPhi}{\mathsf{\Phi}}
\newcommand{\mPsi}{\mathsf{\Psi}}
\newcommand{\mLambda}{\mathsf{\Lambda}}

\DeclareMathOperator{\APP}{APP}
\DeclareMathOperator{\INT}{INT}
\DeclareMathOperator{\app}{app}
\DeclareMathOperator{\integ}{int}
\DeclareMathOperator{\hAPP}{\widehat{\APP}}
\DeclareMathOperator{\hINT}{\widehat{\INT}}
\DeclareMathOperator{\happ}{\widehat{\app}}
\DeclareMathOperator{\hint}{\widehat{\integ}}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\oerr}{\overline{\err}}
\DeclareMathOperator{\apperr}{aerr}
\DeclareMathOperator{\oaerr}{\overline{\apperr}}
\DeclareMathOperator{\interr}{ierr}
\DeclareMathOperator{\oierr}{\overline{\interr}}

\DeclareMathOperator{\Spl}{Spline}
\DeclareMathOperator{\SSpl}{SmSpline}
\DeclareMathOperator{\Power}{Power}
\DeclareMathOperator{\RegSpl}{RegSpline}
\DeclareMathOperator{\MLS}{MLS}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\rms}{rms}
\DeclareMathOperator*{\RMSE}{RMSE}
\DeclareMathOperator*{\bias}{bias}
\DeclareMathOperator*{\var}{var}

\newcommand{\bone}{\boldsymbol{1}}
\newcommand{\bzero}{\boldsymbol{0}}
\newcommand{\binf}{\boldsymbol{\infty}}
\newcommand{\ba}{{\boldsymbol{a}}}
\newcommand{\bb}{{\boldsymbol{b}}}
\newcommand{\bc}{{\boldsymbol{c}}}
\newcommand{\bff}{{\boldsymbol{f}}}
\newcommand{\beps}{{\boldsymbol{\varepsilon}}}
\newcommand{\tbeps}{\tilde{\beps}}
\newcommand{\bx}{{\boldsymbol{x}}}
\newcommand{\bX}{{\boldsymbol{X}}}
\newcommand{\bh}{{\boldsymbol{h}}}
\newcommand{\bk}{{\boldsymbol{k}}}
\newcommand{\bg}{{\boldsymbol{g}}}
\newcommand{\bv}{{\boldsymbol{v}}}
\newcommand{\bu}{{\boldsymbol{u}}}
\newcommand{\by}{{\boldsymbol{y}}}
\newcommand{\bt}{{\boldsymbol{t}}}
\newcommand{\bz}{{\boldsymbol{z}}}
\newcommand{\bvarphi}{{\boldsymbol{\varphi}}}
\newcommand{\bgamma}{{\boldsymbol{\gamma}}}
\newcommand{\bphi}{{\boldsymbol{\phi}}}
\newcommand{\bpsi}{{\boldsymbol{\psi}}}
\newcommand{\balpha}{{\boldsymbol{\alpha}}}
\newcommand{\bbeta}{{\boldsymbol{\beta}}}
\newcommand{\bo}{{\boldsymbol{\omega}}}  %GF added
\newcommand{\newton}[2]{\left(\begin{array}{c} #1\\ #2\end{array}\right)}
\newcommand{\anor}[2]{\| #1\|_{\mu_{#2}}}
\newcommand{\satop}[2]{\stackrel{\scriptstyle{#1}}{\scriptstyle{#2}}}
\newcommand{\setu}{{\mathfrak{u}}}

\newcommand{\me}{\textup{e}}
\newcommand{\mi}{\textup{i}}
\def\d{\textup{d}}
\def\dif{\textup{d}}
%\def\d{{\rm d}}

\def\eavg{{\rm error^{avg}}}
\def\ewc{{\rm error^{wor}}}
\def\ewcavg{{\rm error^{\left\{\substack{\rm wor \\ \rm avg}\right\}}}}
\def\eran{{\rm error^{ran}}}
%\newcommand{\cl}{{\mathfrak{L}}}
\newcommand{\cc}{\mathcal{C}}
\newcommand{\cb}{\mathcal{B}}
\newcommand{\cl}{L}
\newcommand{\cx}{{\Omega}}
\newcommand{\calc}{{\mathcal{C}}}
\newcommand{\calf}{{\mathcal{F}}}
\newcommand{\calfd}{{\calf_d}}
\newcommand{\calh}{{\mathcal{H}}}
\newcommand{\tcalh}{{\widetilde{\calh}}}
\newcommand{\calhk}{\calh_d(K)}
\newcommand{\calg}{{\mathcal{G}}}
\newcommand{\calgd}{{\calg_d}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\fA}{\mathfrak{A}}
\newcommand{\fC}{\mathfrak{C}}
\newcommand{\fF}{\mathfrak{F}}
\newcommand{\fL}{\mathfrak{L}}
\newcommand{\hS}{\widehat{S}}
\DeclareMathOperator{\Prob}{\mathbb{P}}

\def\abs#1{\ensuremath{\left \lvert #1 \right \rvert}}
\newcommand{\bigabs}[1]{\ensuremath{\bigl \lvert #1 \bigr \rvert}}
\newcommand{\norm}[2][{}]{\ensuremath{\left \lVert #2 \right \rVert}_{#1}}
\newcommand{\ip}[3][{}]{\ensuremath{\left \langle #2, #3 \right \rangle_{#1}}}
\newcommand{\bignorm}[2][{}]{\ensuremath{\bigl \lVert #2 \bigr \rVert}_{#1}}
\newcommand{\calm}{{\mathfrak{M}}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\filldis}{fill}
\DeclareMathOperator{\sep}{sep}
\DeclareMathOperator{\avg}{avg}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\cov}{cov}

\newcommand{\des}{\{\bx_i\}}
\newcommand{\desn}{\{\bx_i\}_{i=1}^N}
\newcommand{\wts}{\{g_i\}_{i=1}^N}
\newcommand{\wtsn}{\{g_i\}_{i=1}^N}
\newcommand{\datan}{\{y_i\}_{i=1}^N}

%FJH added
\newcommand{\Order}{\mathcal{O}}
\newcommand{\ch}{\mathcal{H}}
\newcommand{\tch}{{\widetilde{\ch}}}
\newcommand{\veps}{\boldsymbol{\varepsilon}}
\DeclareMathOperator{\best}{best}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\hsigma}{\hat{\sigma}}
\newcommand{\tK}{\widetilde{K}}
\newcommand{\Matlab}{{\sc Matlab}\xspace}
\newcommand{\abstol}{\varepsilon_{\text{a}}}
\newcommand{\reltol}{\varepsilon_{\text{r}}}

\newtheorem{resproblem}{Research Problem}
\newtheorem{research}{Research Objectives}

%\setcounter{page}{1}


\begin{document}
%\setlength{\leftmargini}{2.5ex}

\centerline{\Large \bf Project Description}

Function approximation (recovery) and integration problems arise in several application areas, including the construction of surrogates for computer experiments \citep{FangEtAl06,ForEtal09,SantnerWilliamsNotz03}, numerical solution of partial differential equations \citep{ChenEtAl14,Fas07a,FornbergFlyer15,SarraKansa09,SchWen06a,Wendland04}, and financial risk management \citep{Gla03}.  While there has been a substantial effort to construct efficient algorithms to solve these problems, even the best algorithms may fail to produce answers within the user-specified error tolerance in a reliable and efficient manner.  \emph{We aim to develop algorithms that are:}
\begin{description}[leftmargin=2.5ex]
\item[Stable] The computations do not suffer from catastrophic round-off error.

\item[Efficient] The computational cost is asymptotically optimal compared to the best possible algorithm.

\item[Adaptive] The algorithm adjust its parameters based on the data it observes---rather than a priori knowledge---to meet a specified error tolerance.
    
\item[What about Accurate/Guaranteed?]
\end{description}

\subsection*{Two Typical Examples From Our Earlier NSF-Funded Work}
Kernel methods are used in many different applications, and the function approximation problem may be considered as the fundamental application since it directly applies in the context of scattered data fitting, surrogate modeling and regression approaches in machine learning. Moreover, the function approximation problem is also the starting point for various approaches to the solution of partial differential equations, such as, e.g., pseudospectral collocation methods, partition of unity methods, or RBF-based finite difference methods. One of the main accomplishments of \emph{NSF-DMS-1115392 Kernel Methods for Numerical Computation} (see Section~\ref{SectPrevious}) was the development of the so-called \emph{Hilbert--Schmidt SVD}. This algorithm provides a framework for finding a new stable basis for often ill-conditioned kernel methods.

%Since the Hilbert--Schmidt SVD depends on the availability of the Mercer series of the kernel $K$, one obvious research thrust is to extend the class of kernels for which the Hilbert--Schmidt SVD can be implemented. We plan on doing this along three different paths: (1) analytically computing eigenvalues and eigenfunctions for a given kernel $K$, (2) numerically computing eigenvalues and eigenfunctions for a given kernel $K$, and (3) building \emph{designer kernels} starting from a given orthogonal set of functions $\{\varphi_n\}_{n=1}^\infty$ and appropriately decaying scalars $\{\lambda_n\}_{n=1}^\infty$ which will serve as eigenfunctions and eigenvalues for $K$.

A second major accomplishment of \emph{NSF-DMS-1115392 Kernel Methods for Numerical Computation} was the development of guaranteed numerical integration algorithms such as the guaranteed quasi-Monte Carlo method of (?? Hickernell and Jimenez).

\section{Proposed Research}
Based on our previous research accomplishments, we propose to extend our work in several different directions:

\begin{description}[leftmargin=2.5ex]

\item[Research Project 1] Extend our research on the Hilbert--Schmidt SVD by identifying more kernels and their Mercer series for which the Hilbert--Schmidt SVD can be implemented by
\begin{itemize}
\item analytically computing eigenvalues and eigenfunctions for a given kernel $K$,
\item numerically computing eigenvalues and eigenfunctions for a given kernel $K$, and
\item building \emph{designer kernels} starting from a given orthogonal set of functions $\{\varphi_n\}_{n=1}^\infty$ and appropriately decaying scalars $\{\lambda_n\}_{n=1}^\infty$ which will serve as eigenfunctions and eigenvalues for $K$.
\end{itemize}

\item[Research Project 2] Extend the research on guaranteed algorithms.

\item[Research Project 3] Combine earlier results to obtain guaranteed and automatic algorithms for kernel methods.
\begin{itemize}
\item Use cones to obtain error bounds and guarantees for function approximation with kernels.
\item Use the Hilbert--Schmidt SVD for automatic and adaptive data-based parameter selection of kernel methods.
\item Use the Hilbert--Schmidt SVD to obtain good automatic and adaptive data-based designs for kernel methods.
\end{itemize}

\item[Research Project 4] Application of earlier results.
\begin{itemize}
\item Use kernel methods (and the Hilbert--Schmidt SVD) for the solution of PDEs (such as for (W)ENO).
\item Use kernel methods for the solution of the MEG/EEG problem.
\item Use the eigenfunction expansion for low-rank approximations in machine learning.
\end{itemize}

\end{description}


\section{Introduction to the Problem}
\subsection*{Algorithm Requirements} The approximation and integration problems that we wish to solve may be described as
\begin{equation}
\APP: f (\in \calf) \mapsto f (\in \calg), \qquad \INT: f (\in \calf) \mapsto \int_{\Omega} f(\bx) \varrho(\bx) \, \dif \bx (\in \reals).
\end{equation}
Here, $\calf$ is a Banach space of real-valued candidate functions defined on $\Omega \in \reals^d$, $\calg$ is some other normed vector space of functions, $\APP$ is the embedding operator, and $\INT$ is an integration functional, which depends on the probability density $\varrho$.  Our aim is to find adaptive algorithms, $\hAPP$ and $\hINT$, such that
\begin{subequations} \label{adaptalgoerr}
\begin{gather}
\label{hAPPerr}
\norm[\calg]{\APP(f)-\hAPP(f,\abstol,\reltol)} \le \max(\abstol,\reltol \norm[\calg]{f}), \qquad \forall f \in \calc \subseteq \calf\\
\label{hINTerr}
\abs{\INT(f) -\hINT(f,\abstol,\reltol)} \le \max(\abstol,\reltol \abs{\INT(f)}), \qquad \forall f \in \calc \subseteq \calf
\end{gather}
\end{subequations}


The algorithms $\hAPP$ and $\hINT$ are assumed to depend on $f$ only via a finite number of data points, not on a more complete knowledge of $f$.  This mirrors the situation in many applications.  The algorithms are \emph{adaptive} in the sense that the number of of data points, $N$, is not an input, but rather is determined by based on the absolute and relative error tolerances, $\abstol$ and $\reltol$, as well as the observed function data.

For a sequence of data sites $\{\bx_i\}_{i=1}^{\infty}$, where the $\bx_j$ may depend on function values $\{f(\bx_i)\}_{i=1}^{j-1}$, we will use fixed sample size algorithms $\happ(f,N)$ and $\hint(f,N)$, which depend on the function values $\{f(\bx_i)\}_{i=1}^N$.  Multidimensional integrals well be approximated by averages of function values:
\begin{equation} \label{QMCcub}
\hint(f,N) = \frac 1N \sum_{i=1}^N f(\bx_i), \qquad N \in \naturals,\ f \in \calf.
\end{equation}
Function approximation algorithms, $\happ$, are discussed more below.  We will also construct data-based error bounds, $\oaerr$ and $\oierr$ such that
\begin{equation*}
\norm[\calg]{\APP(f)-\happ(f,N)} \le \oaerr(f,N), \ \
\bigabs{\INT(f)-\hint(f,N)} \le \oierr(f,N), \quad N \in \naturals,\ f \in \calc.
\end{equation*}
Practicality requires that $\oaerr(f,N)$ and $\oierr(f,N)$ depend only on the function data $\{f(\bx_i)\}_{i=1}^N$. Then $\hAPP(f,\abstol,\reltol)$ will choose $N$ to satisfy $\oaerr(f,N) \le \max(\abstol,\reltol \norm[\calg]{f})$ and return the answer $\happ(f,N)$. An analogous procedure will hold for $\hINT(f,\abstol,\reltol)$.

The computational costs of our adaptive algorithms will be measured in terms of the number of data required. For $S \in \{\APP, \INT\}$, let $\$[\hS(f,\abstol,\reltol)]$ denote the number of data required for the given inputs.  The cost of algorithm $\hS$ is then
\begin{equation}
\cost(\hS,\sigma,\abstol,\reltol) := \min\{N \in \naturals : \$[\hS(f,\abstol,\reltol)] \le N \text{ for all } f \in \calc \text{ with } \norm[\calf]{f} \le \sigma\}.
\end{equation}
This computational cost is allowed to depend on the $\calf$-norm of $f$ since we expect that a given absolute error tolerance a larger $f$ requires more computational effort. However, $\hS$ has no knowledge of $\sigma$. The computational cost may also depend on the dimension of the number of function inputs, $d$.  We aim to construct algorithms that can be shown to have asymptotically optimal cost as $\abstol^{-1}, \reltol^{-1}, \sigma$, and $d$ tend to infinity.

Algorithms---even adaptive ones---that only use a finite number of data cannot be expected to provide the correct answer for all $f\in \calf$.  One can easily construct a (spiky) function $f$ that vanishes at all data sites $\bx_1, \ldots, \bx_N$, but that is not zero and has a nonzero integral.  Thus, our algorithms will only be correct for some subset, $\calc$, of functions in $\calf$.  We must identify a reasonable $\calc$, which we expect to be a \emph{cone}.

Although function approximation and integration are relatively simple compared to some other numerical problems, the demands that we are making on our algorithms are quite high and the answers to the questions that we are posing are not yet known.  However, we believe that these questions are important.

\subsection*{Interpolation}
To illustrate some of the challenges further we consider the problem of function approximation ($\APP$) via interpolation.  Suppose that the space of candidate functions, $\calf$, is a Hilbert space with a basis $\{\varphi_n\}_{n=1}^{\infty}$ that is orthonormal with respect to the $L^2:=L^2(\Omega, \varrho)$ inner product $\ip[L^2]{f}{g} := \int_{\Omega} f(\bx) g(\bx) \, \varrho(\bx) \, \dif \bx$.  Moreover, suppose that $\{\varphi_n\}_{n=1}^{\infty}$ is orthogonal with respect to the $\calf$-inner product:
\begin{equation}
\ip[\calf]{f}{g}=\sum_{n=1}^{\infty} \lambda_n \hatf(n)\hatg(n), \qquad \text{where }  f=\sum_{n=1}^{\infty} \hatf(n) \varphi_n, \quad g=\sum_{n=1}^{\infty} \hatg(n) \varphi_n.
\end{equation}
(For simplicity, we have assumed that the series coefficients are real-valued.)  Here, the $\lambda_n:=\norm[\calf]{\phi_n}^2$ are assumed to decay quickly enough so that the function evaluation at any point $\bx \in \Omega$ is a bounded functional.

Given the data $(\bx_1,y_1), \ldots, (\bx_N,y_N)$ with $\by_i=f(\bx_i)$, one might approximate $f$ by interpolation using the first $N$ elements of the basis:
\[
\happ(f,N)(\bx) := \sum_{n=1}^N a_n \varphi_n(\bx),  \text{ where }\ba=\mPhi^{-1} \by, \ \ba:=(a_n)_{n=1}^{N}, \ \mPhi:=(\varphi_n(\bx_i))_{i,n=1}^{N}, \ \by:=(y_i)_{i=1}^{N}.
\]
Two disadvantages of this approach are i) $\happ(f,N)$ is not the optimal approximation to $f$, and ii) for $d>1$ there are choices of the data sites for which $\mPhi$ is singular by the Mairhuber-Curtis Theorem \cite{???}

An alternative is to fit the data by the element of $\calf$ with minimum norm.  This means that $\happ(f,N)$ is an infinite series including all basis elements
\begin{gather*}
\happ(f,N)(\bx) := \sum_{n=1}^\infty a_n \varphi_n(\bx), \quad \text{where }\ba=\argmin_{\bb: \mPhi\bb=\by} \sum_{n=1}^{\infty} \lambda_n b_n^2 = \mPhi^T \mLambda (\mPhi^T\mLambda\mPhi)^{-1}  \by, \\
\bb:=(b_n)_{n=1}^{N}, \ \mPhi:=(\varphi_n(\bx_i))_{i,n=1}^{N,\infty}, \ \mLambda=\diag(\lambda_1, \lambda_2, \ldots), \ \by:=(y_i)_{i=1}^{N}
\end{gather*}
This formula may look numerically impractical since it involves manipulations with vectors and matrices of infinite extent.  However, the situation is not so bad.

There are interesting cases where the infinite sum defining the \emph{kernel}
\begin{equation}\label{HSseries}
K(\bx,\bt) := \sum_{n=1}^\infty \lambda_n \varphi_n(\bx) \varphi_n(\bt),\qquad \bx,\bt \in \Omega,
\end{equation}
can be expressed in closed form.  See \cite{???} for some typical examples.  One such example is
\begin{equation}  \label{gausskernel}
K(\bx,\bt) = \exp\bigl(-\gamma_1^2 (x_1-t_1)^2 - \cdots - \gamma_d^2 (x_d-t_d)^2 \bigr), \qquad \gamma_1,\ldots,\gamma_d >0,
\end{equation}
where the corresponding $\varphi_n$ are Hermite polynomials multiplied by an exponential decay \citep{???}.

The kernel $K$ is the \emph{reproducing kernel} for the Hilbert space $\calf$, i.e., $K(\cdot, \bt) \in \calf$ and $f(t)=\ip[\calf]{K(\cdot, \bt)}{f}$ for all $f \in \calf, \bt \in \Omega$.  This means that the formula above for the minimum norm interpolant may be written in term of vectors and matrices of finite size:
\begin{gather}
\nonumber
\bk(\bx) := \bigl(K(\bx,\bx_i)\bigr)_{i=1}^N = \mPhi \mLambda \bvarphi(\bx), \quad \bvarphi(\bx):=\bigl(\varphi_n(\bx)\bigr)_{n=1}^{\infty} \qquad \mK:=\bigl(K(\bx_i,\bx_j)\bigr)_{i,j=1}^N=\mPhi^T\mLambda\mPhi, \\
\label{meshfreeapprox}
\happ(f,N)(\bx) := \bvarphi_n^T(\bx) \ba  = \bvarphi_n^T(\bx)\mPhi^T \mLambda (\mPhi^T\mLambda\mPhi)^{-1}  \by = \bk^T(\bx)\bc, \quad \text{where }\bc:= \mK^{-1} \by.
\end{gather}

This numerical algorithm for function recovery goes by different names, such as, radial basis function approximation \citep{Buh03a}, scattered data approximation \citep{Wen05a}, meshfree approximation \citep{Fas07a}, and kriging \citep{Ste99}. It is part of the JMP \citep{JMP11} and R \citep{R3.03_2013} software packages.  It is flexible because the data sites need not follow a regular pattern. It is known that $\happ(f,N)$ is the best approximation to $f$ in $\calf$ given the the data \citep{Fas07a,Wen05a}.  There is also a stochastic optimality if one considers $f$ to be a Gaussian stochastic process whose covariance kernel is $K$ \citep{BerT-A04,Wah90}.  The procedure outlined above can be extended to include the exact reproduction of polynomials and also the situation where the $y_i$ are noisy observations of the $f(\bx_i)$ \citep{Wah90}.

\subsection*{Numerical Stability} Unfortunately, this meshfree approach also has the disadvantage of numerical instability {\bf Greg, please insert as much as you want}

\subsection*{Error Bounds}
The pointwise error of the meshfree function approximation $\happ$ given in \eqref{meshfreeapprox} is known to have the following upper bound \citep{???}:
\begin{equation} \label{errbda}
\abs{\APP(f)(\bx)-\happ(f,N)(\bx)} \le \sqrt{K(\bx,\bx)-\bk^T(\bx)\mK^{-1} \bk(\bx)} \norm[\calf]{f}, \qquad \bx \in \Omega.
\end{equation}
Using this inequality to find a data-driven error bound, $\oaerr(f,N)$, poses several challenges.  The numerical singularity of $\mK^{-1}$ mentioned above makes the computation of $\mK^{-1}\bk(\bx)$ inaccurate.  Bounding $\bignorm[\infty,\Omega]{\APP(f) - \happ(f,N)}$ requires the supremum of the quantity under the square root in \eqref{errbda}, which is difficult.  Although \eqref{errbda} does depend on the placement of the data sites, $\{\bx_i\}_{i=1}^N$, it does not depend on the $\{f(\bx_i)\}_{i=1}^N$.  This means that an adaptive algorithm based on \eqref{errbda} has no way of detecting a spike in $f$ and deciding to put more data sites near that spike.  Finally, there is no known upper bound on $\norm[\calf]{f}$, unless one is assumed a priori.

To recap, we intend to construct adaptive algorithms, $\hAPP$ and $\hINT$ for function approximation and integration that satisfy error bounds of the form \eqref{adaptalgoerr}.  These require fixed sample size algorithms, $\happ$ and $\hint$. For the function approximation problem we will focus on interpolation and must somehow choose a basis so that $\happ(f,n)$ can be computed stably.  For both $\happ$ and $\hint$ we need reliable data-driven error bounds, $\oaerr$ and $\oierr$.  We want to show that our adaptive algorithms has a reasonable computational cost, paying particular attention to the problem of large $d$.

Some of these problems have partial answers obtained during previous NSF-funded research, which is summarized in the next section.  Our plan to complete the solution of these problems is given in Sections ???

\section{Results of Previous NSF-Funded Research}\label{SectPrevious}

Greg Fasshauer (GEF) and Fred Hickernell (FJH) are the PIs for \emph{NSF-DMS-1115392 Kernel Methods for Numerical Computation}, July 1, 2011 -- June 30, 2014, \$320,000.  Since Fall 2005, we have organized a weekly Meshfree Methods research seminar that draws regular participation from all of our local collaborators, including faculty members, visitors, and students.  All participants take turns posing interesting research problems, reporting work-in-progress, and presenting relevant work of others.  The atmosphere is informal; interruption and discussion is encouraged.  Occasionally we have speakers from outside applied mathematics and/or IIT. Regular participants for a month or more during the timeframe of this project include the following (``$\fF$'' = female,  ``$\fA$'' = African-American, ``$\fL$'' = Latina/Latino).

\begin{itemize}[leftmargin=2.5ex]
\item IIT Full-Time Faculty: GEF, FJH, Igor Cialenco (IC), John Erickson (JE, $\fA$), and Lulu Kang (LK, $\fF$);
\item Long-Term Visitors: Roberto Cavoretto (RC), University of Torino; Sou-Cheng Choi (SCC, $\fF$), Research Assistant Professor, IIT, National Opinion Research Center, University of Chicago; Salvatore Ganci (SG), University of Palermo; YungWook Jung, Gyeonggi College of Science and Technology; Junbin Li, Dalian University of Technology; Yiuwei Liu (YiL), Lanzhou University; Michael McCourt (MM), Cornell University, Argonne National Laboratory, University of Colorado, Denver; Jinming Wu, Zhejiang Gongshang University;
\item IIT Graduate Students: Aleks Borkovskiy (AB), Siyuan Deng (SD), Yuhan Ding (YD, $\fF$), Llu\'is Antoni Jim\'enez Rugama (LAJR), Lan Jiang (LJ, $\fF$), Yao Li (YaL), Yiou Li (YL, $\fF$), Jagadeeswaran Rathinavel (JR), Tiago Silva (TS), Xin Tong (XT, $\fF$), Qi Ye (QY), Xiaodong Zhang (XiZ), Yizhi Zhang (YZ), Xuan Zhou (XZ);
\item IIT Undergraduate Students: Haocheng Bian (HB), Nick Clancy (NC), Martin Dillon (MD), Caleb Hamilton (CH), Joseph Kupiec, Barrett Leslie (BL), Timothy McCollam (TM), Martha Razo (MR, $\fL$, $\fF$);
\item Other Undergraduate Students: Casey Bylund (CB, $\fF$), University of San Francisco; Matthew Gliebe (MG), Northwestern University; Palmer Lao (PL), Clarkson University; William Mayner (WM), Brown University;
\item High School Student: Sunny Yun (SY).
\end{itemize}

\subsection{Intellectual Merit from Current NSF Funding}

Together with a collaborator, we have investigated the convergence and tractability of meshfree methods, \eqref{meshfreeapprox}, using the isotropic Gaussian reproducing kernel and its extension with non-isotropic shape parameters, $\{\gamma_{\ell}\}_{\ell =1}^{\infty}$, \eqref{gausskernel}. Dimension \emph{independent} convergent rates for function approximation (the problem $\APP$) are possible and they depend on the rate of decay of the $\gamma_{\ell}$ as $\ell \to \infty$, not on the smoothness of the reproducing kernel \citep{FasHicWoz12b, FasHicWoz12a}. These results have been extended by XZ to a more general class of reproducing kernels with that have a sequence of scale parameters $\{\alpha_{\ell}\}_{\ell=1}^{\infty}$ as well as shape parameters.  In this case the dimension independent convergence rates dependon the rate of decay of the product $\alpha_{\ell} \gamma_{\ell}$ as $\ell \to \infty$ \citep{ZhoHic15a}.

Together with QY and other collaborators, we have shown how the optimal solution of problems posed on reproducing kernel Hilbert spaces can be extended to Banach spaces \citep{SonZhaHic12a, FasHicYe13a}.

Existing algorithms for multivariate or even infinite-dimensional approximation and integration problems require hand-tuning based on the parameters that define the spaces of functions, $\calf$, e.g., the shape or scale parameters of the associated reproducing kernel.  In our search for adaptive algorithms that automatically select these parameters, we started by looking at simpler problems and algorithms.  FJH, JL, YiL, and a collaborator developed an  adaptive algorithm for Monte Carlo calculation of means of random variables and of multidimensional integrals, which has a rigorous guarantee of success for a cone of random variables \citep{HicEtal14a}.  FJH and LAJR developed adaptive quasi-Monte Carlo cubature algorithms based on digital nets \citep{HicJim16a} and rank-1 integration lattices \citep{JimHic16a}, which bound the error in terms of the discrete Fourier coefficients of the integrands.
FJH, NC, YD, CH, and YZ constructed adaptive algorithms for univariate integration and function approximation \citep{HicEtal14b}.  This paper also sets out a general framework for constructing guaranteed adaptive algorithms for any problem where the solution operator is homogeneous by considering cones if input functions.  FJH, MR, and SY are preparing a paper to explain these ideas to a broader audience.  XT completed her MS thesis \citep{Ton14a} on guaranteed adaptive optimization of univariate functions.
FJH, YD, LJ, LJAR, XT, and YZ have implemented their algorithms in the Guaranteed Automated Integration Library (GAIL) \citep{ChoEtal14a}.  This \Matlab library is publicly available and under ongoing development.

GEF and QY have formulated a theory of generalized Sobolev spaces for reproducing kernel Hilbert spaces that connects kernel-based approximation methods to fundamental solutions of differential operators \cite{FasshauerYe11, FasshauerYe13}. An important contribution is the fact that the definition of these generalized Sobolev spaces involves both a notion of smoothness and of scale, while traditional Sobolev spaces take into account only the smoothness of functions in the space.

As an application of the generalized Sobolev space theory, GEF, QY and IC have developed a framework for the numerical solution of stochastic partial differential equations with kernel-based methods \citep{CFY12,FasshauerYe13b,FasshauerYe14}.

GEF and MM have developed an RBF-QR algorithm for the stable computation with ``flat'' kernels as explained in Sect.~\ref{SectRBFQR}. This approach takes advantage of the Hilbert-Schmidt series of the kernels \eqref{HSseries}. The basic framework was reported in a paper on stable computations with Gaussian kernels \citep{FMcC12}, and further extensions to the solution of boundary value problems for PDEs \citep{McCourt13}, a fast recursive regression algorithm \citep{McCourt13b}, and coupled PDE problems \citep{McCF13} have been submitted for publication. A publicly available library of \Matlab code \citep{McCFBG13} is under ongoing development.

A new class of so-called \emph{compact Mat\'ern kernels} was investigated by RC, GEF and MM \citep{CFMcC13}. The paper discusses these kernels, their connection to piecewise polynomial splines and an implementation of the associated Hilbert-Schmidt SVD algorithm. Special cases of these kernels were studied by CB and WM during their 2012 summer REU under the guidance of GEF and MM. During summer 2013, GEF and MM worked with two other undergraduate REU students, HB and MG, on the numerical computation of kernel eigenvalues and eigenfunctions and on an application of the Hilbert-Schmidt SVD to the study of the Hilbert space norm of the kernel interpolant. The latter problem is important for the error estimation of kernel-based methods, and for the determination of optimal kernel parameters such as the shape parameter.

During SG's visit to IIT (08/2012-02/2013) he performed joint work with GEF and MM on the use of kernel-based methods for the solution of coupled boundary value problems for the application in EEG and MEG (see Sect.~\ref{SecMEEG}). This work made use of the coupling framework of \cite{McCF13}. A joint paper \citep{AFFGM13} has been submitted.

Understanding the covariance kernels, eigenfunctions and associated Hilbert spaces of certain stochastic processes, in particular fractional Brownian motion, was the focus of a research project during summer 2013 involving MD under the guidance of JE and GEF. Some outcomes of this research will be presented at the 2014 Joint Mathematics Meeting.

\subsection{Broader Impacts from Current NSF Funding}

We have trained a number of PhD, MS, BS, and high school students in research, as evidenced by the list above.  QY defended his PhD thesis in April 2012 and became a Philip T. Church postdoc in the Department of Mathematics at Syracuse University starting August 2012. He has four published papers, with an additional two accepted, one submitted, and two more in preparation.  YL defended her thesis in August 2014 and is a visiting assistant professor at DePaul University.  Of the other nine IIT PhD students listed, YD, YL, XZ, and YZ have completed their comprehensive exams and are expected to graduate by the end of 2015.  AB has passed his qualifying exams.  LAJR, JR, TS, and XiZ are in the earlier stages of their PhD studies. SD, YaL, TS, XT, and XiZ completed their MS theses. The latter three are now PhD students.  QY, YD, LJ, LAJR, YL, XZ, HB, MR, SY, NC, MD, CH, BL, CB, WM, and SY have all presented their work at academic meetings in talks.

We have been pleased to have two underrepresented minority scientists and several female scientists (undergraduate through to faculty members) in our research group.  To their credit, both LJ and YL have successfully pursued their PhDs while at the same time carrying the responsibility of marriage and motherhood.

GEF presented a week-long tutorial to students from many different engineering disciplines at the University of Palermo. As a result, SG, a PhD student from Palermo supported by Italian funds, visited IIT from 08/2012-02/2013 to receive guidance on his research from GEF and MM.

RC, a postdoctoral researcher from the University of Torino supported by Italian funds, visited IIT from 03/2012-05/2012 and also in April 2013. He received advanced training in the area of meshfree approximation and has since been able to secure a position as a research fellow at the University of Torino.

Every fall semester in even years GEF teaches the graduate course MATH 590: Meshfree Approximation Methods, which focuses on kernel based methods, and uses his monograph \citep{Fas07a} as a text. The new developments using eigenfunction expansions of positive definite kernels were integrated for the first time in fall 2012. GEF and MM have used the updated lecture notes of this course as a basis for a new book manuscript entitled ``Kernel-based Approximation Methods using \Matlab'', currently under contract with World Scientific Publishers.

Every fall semester FJH teaches the graduate course MATH 565: Monte Carlo Methods in Finance.  The new results on automatic (quasi-)Monte Carlo algorithms have been taught there since fall 2012, and the GAIL library is used by the students.

SCC and FJH offered an experimental graduate seminar course for seven students, MATH 573: Reliable Mathematical Software, during fall 2013. Our goal was to help computational mathematicians understand how to develop software that can be used by others.  Topics included guarantees for automatic algorithms, reproducible computational science research, efficient coding, thorough documentation, careful testing, convenient user interfaces with parameter parsing and validation, and software publication.  We intend to offer this course again to an expanded audience because the topics are not normally covered in standard mathematics or computer science courses.

Members of the research team have organized conferences. Together with Larry L.~Schumaker, GEF organized the NSF-supported 14th International Conference on Approximation Theory in San Antonio, TX, in April 2013. A special mentoring session for PhD students and postdocs was included in the program of this conference. QY was the main organizer for the SIAM Student Chapter Conference on Recent Advances in Computational Science and Statistics held at IIT Oct. 29--30, 2011 ({\tt http://www.math.iit.edu/$\sim$siam/workshop/}). BL was a co-organizer. This conference brought together graduate students and postdocs from various Chicago area universities and Argonne National Lab and featured three plenary lectures (by Jerry L. Bona, Charles K. Chui, and Wei B. Wu) along with 25 contributed talks and a poster session. Since then, other Chicago-Area SIAM chapters have joined the IIT chapter to organize this conference on a yearly basis.

The research team has attended conferences, presented talks at departmental colloquia, and given talks to general audiences, both inside the mathematical sciences and beyond. Here are the highlights of the PIs.

\begin{itemize}[leftmargin=2.5ex]
\item GEF gave minisymposium talks at the 2012, 2013, and 2014 SIAM Annual Meetings.  He also gave invited presentations at the workshop on Multivariate Approximation and Interpolation with Applications in Erice, Italy in Sept.~2013, at the Freeform Optics Incubator Meeting of the Optical Society of America, Washington, D.C., in Oct.~2011, and at the NSF-CBMS Conference on Radial Basis Functions: Mathematical Developments and Applications, at UMass-Dartmouth, in June 2011. He gave a talk to the Level Set Collective at the NSF-sponsored Institute of Pure and Applied Mathematics (IPAM) and an invited talk at the 2013 Midwest Numerical Analysis Day. GEF is also a steering committee member of the Midwest Numerical Analysis Conference series. In 2011, he gave a week-long workshop at the U of Palermo (Italy, engineering). GEF gave departmental colloquium/seminar talks at Chapman U (math \& computer science), U Chicago (statistics), Middle Tennessee State U (math), Northwestern U (industrial engineering \& management sciences), and U of Padua (Italy, math).  GEF attended the 2010-2012 Intel International Science and Engineering Fairs (2011-2012 as chair of the AMS Menger Prize committee), where he judged projects in the mathematical sciences and interviewed high school students about their research. These interviews can be viewed on the YouTube channel of the AMS. He has also given presentations to students at Neuqua Valley and Carl Sandburg high schools.

\item FJH gave a talk at a special session at the 2012 Joint Mathematics Meetings (JMM), a plenary lecture at the Tenth International Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing (MCQMC) in 2012, an invited talk at the 2013 Midwest Numerical Analysis Day, and an invited session talk at the Eleventh MCQMC in April, 2014, an invited session talk at the 2014 SIAM Annual Meeting, and an invited talk at an ICERM workshop in September, 2014.  He spoke at the 2014 JMM and will speak at the 2015 JMM. FJH gave departmental colloquium/seminar talks at Argonne National Laboratory (mathematics and computer science), U Chicago (statistics), DePaul U (mathematics), Georgia Tech (industrial and systems engineering), and Illinois Institute of Technology (computer science, mathematics).  FJH was invited to speak to STEM students at Oakton Community College and to students at Hinsdale Central High School.

\end{itemize}

\section{Stable Computation and Designer Kernels???} {\bf Greg perhaps you can put the relevant material here and arrange as you see fit.}

\section{Error Bounds and Adaptive Algorithms}
\subsection*{Why Cones}
\subsection*{Adaptive Algorithms for Integration Problems ($\INT$)}  We have already developed an adaptive simple Monte Carlo algorithm \citep{HicEtal14b} (\texttt{cubMC\_g.m} and \texttt{meanMC\_g.m} in the GAIL \citep{ChoEtal14a}) for multivariate integration problems, but \emph{only for the pure absolute error} case ($\reltol=0$).  The integral is approximated by IID samples of $f(\bX)$ where $\bX$ is generated by probability density function (PDF) $\varrho$. Because this is a random algorithm, the deterministic guarantee of success in \eqref{hINTerr} is replaced by a probabilistic one:
\begin{equation} \label{cubMCguar}
 \Prob\bigl[\bigabs{\INT(f) -\hINT(f,\abstol,\reltol)} \bigr ] \le \max(\abstol,\reltol \abs{\INT(f)}) \bigr] \le 1-\alpha.
\end{equation}
This guarantee holds for the cone of integrands with bounded kurtosis:
\begin{equation}
\calc := \bigl \{ f : \INT(\abs{f - \INT(f)}^4) \le \kappa_{\max} [\INT(\abs{f - \INT(f)}^2)]^2 \bigr\}
\end{equation}
The bounded kurtosis allows us to reliably bound the population variance $f(\bX)$ by the an inflation of the sample variance of the $f(\bX_i)$. This variance bound along with the kurtosis bound are input into a Berry-Esseen inequality (a finite sample version of the Central Limit Theorem) to guarantee \eqref{cubMCguar}.

We are now trying to extend our results to the more general error tolerance that includes nonzero $\reltol$.  The difficulty is that we must bound $\abs{\INT(f)}$ at the same time that we are trying to estimate it.  This requires an algorithm with a varying number of iterations.  Since the intermediate results are probabilistic, we must split the uncertainty, $\alpha$, among all of these intermediate iterations.




\vspace{2cm}{\huge \section*{Everything below here is the old proposal}}\vspace{2cm}

\subsection*{Data-Driven Error Estimation} One may estimate the error of the meshfree approximation \eqref{rbfapprox} in a data-driven way by using cross-validation (CV).  The parameters of the kernel may be estimated by maximum likelihood estimation (MLE).  However, neither of these methods have finite-sample size guarantees of success.

On the other hand, there exist error bounds of the form \citep{Wen05a}
\begin{equation} \label{rbferrbd}
\sup_{\bx \in \cx} \abs{f(\bx) - \tf(\bx)} \le C \norm[\ch]{f} [\filldis(\desn)]^r, \qquad \filldis(\desn): = \max_{\bx \in \cx} \min_{i=1,  \ldots, N} \norm[2]{\bx - \bx_i},
\end{equation}
Here $C$ is some constant, and $\filldis(\{\bx_i\}_{i=1}^N)$ is the fill or maximin distance of the data sites.  Moreover, $\norm[\ch]{f}$ is the Hilbert space norm of $f$, where the reproducing kernel for $\ch$ is the $K$ used in \eqref{rbfapprox}. Finally, $r$ reflects the smoothness of $K$.  Error bound \eqref{rbferrbd} tells us that the error depends on both the roughness (Hilbert space norm) of $f$ and the unevenness (fill distance) of the data sites.

However, the error of the meshfree approximation cannot be bounded in terms of the data, $(\bx_1, y_1), \ldots, (\bx_N,y_N)$, because $\norm[\ch]{f}$ is unknown a priori.  The user does not know how large the error is in practice and whether $N$ needs to be increased to meet a desired tolerance. Thus, \eqref{rbferrbd} by itself does not lead to a guaranteed, automatic, adaptive algorithm.

Sect.\ \ref{SectCones} introduces the idea of \emph{cones} of input functions, where a stronger norm of $f$ is bounded above in terms of a weaker norm of $f$.  We have already successfully used this idea to construct guaranteed, adaptive, automatic algorithms for integration and for univariate function approximation.  Moreover, we have derived known upper bounds on their computational costs. \emph{We aim to apply the cones idea to multivariate function approximation using meshfree methods.}

\subsection*{Stable Kernel Computation and Data-Driven Parameter Optimization} Another serious practical problem with meshfree approximation is that the matrix $\mK$ in \eqref{rbfcoef} can easily become \emph{ill-conditioned for moderate $N$}---especially if the kernel $K$ contains shape parameters, $\gamma_\ell>0$, such as the anisotropic Gaussian kernel in \eqref{gausskernel}.  The ill-conditioning will arise if the kernel has a high degree of smoothness ($r$ is large), and/or if it becomes ``flat'' (the $\gamma_\ell$ are near zero). This means that $f$ is very smooth and the discretization error \eqref{rbferrbd} vanishes rapidly as the fill distance decreases.
Traditionally, this has led to a widely accepted trade-off or uncertainty principle, according to which one must find values of the shape parameters that optimize the balance between (decreased) stability and (increased) discretization error.  This is explained in Sect.\ \ref{SectMisconcept}.

Moreover, the CV and MLE computations often used to optimize the parameters in the kernel also depend on working with the possibly ill-conditioned $\mK$.  This can make CV and MLE unreliable.

We intend to use equivalent, but ``better'' basis representations for kernel-based approximation methods of the form \eqref{rbfform} that lead to \emph{reliable and numerically stable algorithms} and to new and more dependable ways for estimating the shape parameters that provide optimal accuracy. We note here that such non-zero optima have been shown to exist in many cases (see, e.g., Fig.~\ref{Fig_HSSVD})---due to reasons other than the trade-off principle such as Runge-like boundary effects. In particular, we plan to develop practical, reliable and automatic algorithms that take advantage of the additional flexibility provided by the kernel parameters, instead of viewing them as a nuisance.

\subsection*{Two Big Research Thrusts} To solve the problems mentioned above, plus related problems, we will employ the following two big ideas.
\begin{description}[leftmargin=2.5ex]

\item[Cones (Sect.\ \ref{SectCones})] Identify suitable cones of input functions that can be used to develop theoretically justified, data-driven error bounds for approximation and integration algorithms.  Use these to construct guaranteed, automatic algorithms with upper bounds on the computational cost and lower bounds on the computational complexity.

\item [Hilbert-Schmidt SVD (Sect.\ \ref{SectHSSVD})] Develop stable kernel-based algorithms together with associated parameter estimation methods which result in practical and reliable algorithms that can be applied to a wide range of function recovery problems.

\end{description}

\section{Reliable Error Estimation Via Cones}\label{SectCones}

\subsection{The Flaws in {\tt integral.m} and Other Automatic, Adaptive Algorithms} \label{drugssubsect} Computations are prevalent in research, development and operations.  These computations solve large complex problems by assembling basic numerical algorithms for sub-problems. Errors in one piece can propagate throughout.  Our focus is on some of these basic algorithms for function approximation and integration.  As explained at the beginning of the proposal, we want these algorithms to be automatic, adaptive, guaranteed, efficient, and stable.

\Matlab's {\tt integral.m} \citep{MAT8.2} is a typical automatic, adaptive quadrature algorithm.  According to its documentation ``{\tt integral} attempts to satisfy {\tt |Q - I| <= max(AbsTol, RelTol*|Q|)}, where {\tt I} denotes the exact value of the integral.'' Although {\tt integral.m} is automatic, adaptive and stable, it has no guarantees of success and no known bound on its computational cost. Automatic quadrature algorithms in other common software packages suffer the same flaws.  We have developed an alternative to {\tt integral.m}, described in Sect.\ \ref{integral_g_sec}, which satisfies all of our desired criteria for reliability and practicality.

\Matlab's {\tt integral.m} begins by splitting the integration interval into ten sub-intervals and applies Gauss-Kronrod quadrature using $15$ nodes on each sub-interval.  The absolute error on a sub-interval is estimated in terms of the difference between two Gauss quadrature rules using the same nodes, $\bigl \lvert Q(f)-\tQ(f) \bigr \rvert$. If the estimated error is too large, the sub-interval is further subdivided and the process is repeated.

This kind of error estimation can fail in two ways as depicted in Fig.\ \ref{MatIntegFailFig}. The first way is for the sampling scheme (in this case $150$ initial data sites) to miss significant \emph{spikes} in the integrand.  Any quadrature scheme is susceptible to this problem, but {\tt integral.m} fails to quantify how spiky integrands can be and still be integrated correctly.

%\begin{figure}[h]
%\centering
%\includegraphics[width=6cm]{SpikyFoolIntegral.eps} \qquad
%\includegraphics[width=6cm]{FlukyFoolIntegral.eps} \\
%$\int_0^1 f_{\text{spiky}}(x) \, \dif x=0.5$, \qquad \qquad $\int_0^1 f_{\text{fluky}}(x) \, \dif x=0.278827$ \\
%{\tt integral(fspiky,0,1,'AbsTol',1e-13,'RelTol',1e-13)=0} \\
%{\tt integral(ffluky,0,1,'AbsTol',1e-13,'RelTol',1e-13)=0.278799}
%\caption{Integrands for which {\tt integral.m} fails to give the answer to within the user-specified tolerance. \label{MatIntegFailFig}}
%\end{figure}

The fluctuations of the \emph{fluky} integrand in Fig.\ \ref{MatIntegFailFig} are observed by the sampling scheme, but the error estimate, $\bigl \lvert Q(f)-\tQ(f) \bigr \rvert$, just happens to vanish on each sub-interval. Again, {\tt integral.m} fails to say when this might happen. Thirty years ago, \cite{Lyn83} warned that the error estimates of the form $\bigl \lvert Q(f)-\tQ(f) \bigr \rvert$---used by virtually all automatic quadrature rules---were flawed because they could be fooled by fluky integrands.  Lyness offered no alternative.  \emph{We have one.}

\subsection{Guaranteed Automatic Quadrature via {\tt integral\_g.m}} \label{integral_g_sec} Our guaranteed automatic algorithm for evaluating $\int_0^1 f(x) \, \dif x$ is based on the trapezoidal rule with $n$ trapezoids ($N=n+1$ data) and its well-known error bound \cite[(7.15)]{BraPet11a}:
\begin{equation} \label{traprule}
T_n(f) = \frac{1}{n} \left[f(0) + 2 f\left(\frac 1n \right) + \cdots + 2 f\left(\frac {n-1} n \right) + f(1) \right], \ \
\abs{\int_0^1 f(x) \, \dif x - T_n(f)} \le \frac{\norm[1]{f''}}{8 n^2}.
\end{equation}
Like error bound \eqref{rbferrbd}, this error bound by itself is insufficient for constructing an automatic, adaptive quadrature algorithm because $\norm[1]{f''}$ is unknown in advance.

Our approach assumes that the integrand lies in the cone of functions \citep{HicEtal14b}
\begin{equation} \label{integralcone}
\cc_{n^*} := \{f : \norm[1]{f''} \le 2 n^* \norm[1]{f'-f(1)+f(0)} \},
\end{equation}
namely functions whose stronger semi-norm is bounded in terms of a weaker semi-norm.  We then approximate the weaker norm by replacing $f$ by it linear spline, $S_n(f)$.  Because $\norm[1]{f'}-\norm[1]{S_n(f)'} \le \norm[1]{f''}/(2n)$, we can derive this data-driven upper bound on the error of the trapezoidal rule:
\begin{equation} \label{traperrupbd}
\abs{\int_0^1 f(x) \, \dif x - T_n(f)} \le  \frac{\norm[1]{f''}}{8 n^2} \le \frac{n^* \norm[1]{S_n(f)'-f(1)+f(0)}}{4 n(n - n^*)}, \qquad n>n^*.
\end{equation}

Algorithm 4 in \cite{HicEtal14b} increases $n$ until the right hand side of \eqref{traperrupbd} is no greater than the error tolerance, $\varepsilon$.  This algorithm has been implemented as {\tt integral\_g.m} in the Guaranteed Automatic Integration Library (GAIL) \citep{ChoEtal14a}.  Like \Matlab's {\tt integral.m}, algorithm {\tt integral\_g.m} is automatic, adaptive, and numerically stable.  However, {\tt integral\_g.m} also satisfies the other required criteria stated at the beginning of the proposal \citep{HicEtal14b}.
\begin{description}[leftmargin=2.5ex]
\item[Guarantee] $\displaystyle \abs{\int_0^1 f(x) \dif x - T_n(f)} \le \varepsilon$ for all $f \in \cc_{n^*}$.
\item[Computational Cost Upper Bound] The number of trapezoids needed does not exceed $\displaystyle 3+2n^*+\sqrt{\frac{n^* \norm[1]{f''}}{2 \varepsilon}}$, where $\norm[1]{f''}$ is not known a priori but bounded by the algorithm using data.
\item[Complexity Lower Bound] Any algorithm that succeeds for all $f \in \cc_{n^*}$ must use at least $\displaystyle -1 + \sqrt{\frac{(n^* - 1) \norm[1]{f''}}{16 n^* \varepsilon}}$ integrand values.  Our {\tt integral\_g.m} has asymptotically optimal computational cost as $\varepsilon/\norm[1]{f''} \to 0$.
\end{description}

The spikiness of the integrands, $f$, for which {\tt integral\_g.m} succeeds is precisely quantified by the condition $f \in \cc_{n^*}$. The parameter $n^*$ has an intuitive interpretation:  the minimum number of trapezoids is $n^*+1$, and $1/n^*$ is the width of the spikes that can barely be observed.  Fluky integrands like the one in Fig.\ \ref{MatIntegFailFig} cannot fool {\tt integral\_g.m} because its error estimation is not based on the difference of two quadrature rules.

Although {\tt integral\_g.m} has the desirable properties listed above, it has \emph{deficiencies that we intend to rectify.}
\begin{itemize}[leftmargin=2.5ex]
\item Our {\tt integral\_g.m} needs to be extended to include a relative error tolerance as well as the present absolute error tolerance.

\item Now {\tt integral\_g.m} is globally adaptive, meaning that the data sites are uniformly spaced, but the number of sites is chosen adaptively.  We need should local adaptivity to improve efficiency for integrands that are spikier in only part of the interval of integration.

\item The trapezoidal rule underlying {\tt integral\_g.m} should be replaced with a higher order rule for greater efficiency.
\end{itemize}

\subsection{Cones Are the Key}
Constructing an automatic, adaptive algorithm that is also \emph{guaranteed}, such as {\tt integral\_g.m}, depends on defining the right cone of input functions.  Adaptive algorithms have no significant advantage over non-adaptive algorithms for problems defined for convex sets of input functions and satisfying certain other reasonable conditions \cite[Chapter 4, Theorem 5.2.1]{TraWasWoz88}. For example, if we want to construct a guaranteed, automatic algorithm for functions in the (convex) ball, $\cb_{\sigma} = \{f : \norm[1]{f''} \le \sigma \}$, then using the trapezoidal rule, $T_n(f)$, with $n=\bigl\lceil\sqrt{\sigma/(8 \varepsilon)}\, \bigr \rceil$ has nearly optimal computational cost, but it is \emph{non-adaptive}.  By requiring integrands to lie inside a non-convex cone, we allow adaption to be useful. We are also able to prove that the algorithm must succeed, bound its computational cost, and bound the computational complexity of the problem.

The arguments leading to {\tt integral\_g.m} are not restricted to univariate integration but have wider possible application \citep{HicEtal14b}.  The main constraint is that the solution operator be positively homogeneous.  \emph{Thus, we plan to develop guaranteed, automatic, adaptive algorithms for other approximation and integration problems as well.}

\subsection{Guaranteed Monte Carlo} \label{MC_g_sec}
\cite{HicEtal14a} have developed {\tt meanMC\_g.m} in GAIL for constructing guaranteed $\varepsilon$-half-width confidence intervals for the mean, $\mu=\bbE(Y)$, of a random variable, $Y$, via independent and identically distributed (IID) Monte Carlo simulation.  We use the sample mean and sample variance:
\begin{equation} \label{samplemean}
\hmu_{N}:=\frac 1{N} \sum_{i=1}^{N} Y_i, \quad \hsigma_{N}^2:=\frac 1{N-1} \sum_{i=1}^N (Y_i-\hmu_{N})^2, \qquad Y_1, Y_2, \ldots \text{ IID}.
\end{equation}
A typical approach is to pick an $n_{\sigma}$, compute $\hsigma_{n_{\sigma}}^2$, perhaps multiply it by an inflation factor $\fC^2>1$ to approximate $\var(Y)=\bbE[(Y-\mu)^2]$, and then appeal to the  Central Limit Theorem (CLT), to construct an approximate confidence interval.  There are no guarantees because the estimate of the variance may be wrong and the CLT is not a finite sample result.

Instead, we assume that $Y$ lies in the \emph{cone} of random variables whose kurtosis, $\bbE[(Y-\mu)^4]/[\var(Y)]^2$, is no greater than a parameter $\kappa_{\max}$, which depends explicitly on $n_{\sigma}$ and $\fC^2$.  The cone condition allows us to use Cantelli's inequality to prove that $\var(Y)\le \fC^2\hsigma_{n_\sigma}^2$ with high probability.  The cone condition also allows us to use a Berry-Esseen inequality to choose $n_{\mu}$ that \emph{guarantees} that $\Pr[\abs{\mu-\hmu_{n_{\mu}}} \le \varepsilon]\le 1-\alpha$, where $\hmu_{n_{\mu}}$ is based on a sample that is independent of the one used to compute $\hsigma_{n_\sigma}^2$.

The multidimensional integral $\int_{\reals^d} f(\bx) \rho(\bx) \, \dif \bx$ can be interpreted as $\mu=\bbE(Y)$ with $Y=f(\bX)$ and $\bX$ having probability density $\rho$. In this way {\tt meanMC\_g.m} can be used for guaranteed automatic, adaptive multidimensional integration (cubature) \citep{HicEtal14a}.  The cone condition $Y$ translates into a cone condition on $f$.

Our {\tt meanMC\_g.m} also lacks the ability to satisfy a relative error tolerance. \emph{We aim to rectify this deficiency.}

\subsection{Other Monte Carlo Research Topics} We intend to develop guaranteed versions of two other versions of Monte Carlo methods:
\begin{description}[leftmargin=2.5ex]
\item[Multi-Level Monte Carlo] In some situations, generating an exact instance of the random variable, $Y$, would take an infinite amount of time, e.g., if $Y$ is an option payoff that depends on the numerical solution of a stochastic differential equation.  Multi-level methods overcome this problem by computing a large number of coarse approximations to $Y$ and a small number of fine approximations to $Y$ (see the PI's work \citep{NiuHic09a, NiuHic09b} and the review by \cite{Gil14a}).  \emph{We will develop guaranteed multi-level Monte Carlo methods.}

\item[Quasi-Monte Carlo] Quasi-Monte Carlo methods \citep{DicEtal14a} reduce the error in approximating $\mu=\bbE(Y)=\int_{\reals^d} f(\bx) \rho(\bx) \, \dif \bx$, by replacing the IID samples, $\bX_1, \bX_2, \ldots$ in $Y_i=f(\bX_i)$ with evenly distributed samples, such as integration lattices or digital nets.  The error can be expressed in terms of the Fourier (complex exponential or Walsh) coefficients of $f$.  We will use the discrete Fourier coefficients of the integrand values, which can be computed in $N \log(N)$ operations, to reliably estimate the error and \emph{construct guaranteed, automatic, adaptive quasi-Monte Carlo cubature algorithms.} The PI has used these fast discrete Fourier transforms in earlier work \citep{LiHic03a,DicHicLiu07a}.

\end{description}

\subsection{Multivariate Function Approximation (Recovery)}\label{SecRecovery}
Constructing reliable and practical  univariate and multivariate integration algorithms as described in the previous subsections are important objectives in their own right.  As relatively easier problems, they also support our \emph{objective of constructing a guaranteed, adaptive, automatic multivariate function approximation algorithm,} as discussed in the introduction.

There exist univariate function approximation algorithms, such as cubic splines, in widely used software packages, but those algorithms are not automatic.  GAIL \citep{ChoEtal14a} includes a guaranteed, adaptive linear spline algorithm, {\tt funappx\_g.m}, whose theoretical underpinnings resemble those of {\tt integral\_g.m} \citep{HicEtal14b}.  Like {\tt integral\_g.m}, we intend to derive \emph{a higher order version of {\tt funappx\_g.m} that is also locally adaptive and satisfies a relative error tolerance.}

For multivariate function approximation we focus on meshfree approximation \eqref{rbfapprox}.  Here is how we might proceed.  As in Sect.\ \ref{integral_g_sec}, we need to identify a cone of input functions where the stronger norm, i.e., the Hilbert space norm, $\norm[\ch]{\cdot}$, is bounded above by some multiple of a weaker norm.  We might choose the weaker norm to correspond to a Hilbert space, $\tch$, whose reproducing kernel $\tK$ has a Hilbert-Schmidt series involving the same eigenfunctions as $K$ but a more slowly decaying sequence of eigenvalues (see \eqref{HSseries} below). See \eqref{CMatern} and the kernels in \citep[Chap.\ 2]{Wah90} as examples of such kernels.  Analogously to Sect.\ \ref{integral_g_sec}, we would then need to find a bound on $\norm[\tch]{f} - \bigl\lVert \tf \bigr \rVert_{\tch}$ in terms of $\norm[\ch]{f}$ and use the data-driven $\bigl\lVert \tf \bigr \rVert_{\tch}$ to approximate $\norm[\tch]{f}$, which by the cone condition would give an upper bound on $\norm[\ch]{f}$. These calculations would involve the matrices $\mK$ and $\tmK$, which may be ill-conditioned, as mentioned in the introduction.  To make our proposed algorithm numerically stable we need to employ the techniques introduced in the next section.


\section{Stable Computation via the Hilbert-Schmidt SVD}\label{SectHSSVD}

\subsection{The Uncertainty Principle---A Major Misconception}\label{SectMisconcept}
Before we discuss how to compute stably with ``flat'' kernels, let us first consider \emph{why} one might want to care about ``flat'' kernels in the first place. In fact, let's start by looking at what happens if we consider using ``flat'' kernels with the standard approach as outlined in the introduction. It has been a long-held belief among many practitioners that ``one can't simultaneously have high accuracy and stability when computing with radial basis functions'' (or other positive definite kernels).
This belief is
(i) based on the observations made by countless practitioners, and
(ii) assumed to rest on a rigorous mathematical foundation, the so-called \emph{uncertainty principle} due to \cite{Schaback95b, Schaback95c}.
Unfortunately, item (ii) is the cause of a major misconception. Taking into consideration the hypotheses for Schaback's work, one should more carefully paraphrase his uncertainty principle as
\[
\text{``\emph{Using the standard basis}, one can't simultaneously have high accuracy and stability.''}
\]
Therefore, use of a ``better'' basis may very well allow us to obtain results that are both stable \emph{and} accurate. This idea should not come as a surprise to anyone familiar with the use of B-splines instead of truncated power functions, or Chebyshev polynomials instead of monomials. The quest for such a ``better'' basis was the starting point for some of the research performed under NSF Grant DMS-1115392, and it is the fundamental idea behind the research proposed in this section.

\subsection{The Hilbert-Schmidt SVD---A Framework for Stable Computation}\label{SectRBFQR}
Every positive definite kernel $K$ has a Hilbert-Schmidt (or Mercer) series decomposition \citep{CourantHilbert53,RasWil06a}
\begin{equation}\label{HSseries}
K(\bx,\bt) = \sum_{n=1}^\infty \lambda_n \varphi_n(\bx) \varphi_n(\bt),
\end{equation}
where the $\lambda_n$ and $\varphi_n$ are the eigenvalues and eigenfunctions of the associated integral operator $\cT_K : L_2(\Omega, \rho) \to L_2(\Omega, \rho)$ defined by
\begin{equation}\label{TK}
(\cT_Kf)(\bx) = \int_\Omega K(\bx, \bt)f(\bt)\, \rho(\bt)\, \d \bt.
\end{equation}

\cite{FMcC12} and \cite{CFMcC13} combined this decomposition of the kernel $K$ with some basic block matrix manipulations to show that the kernel matrix $\mK$ from \eqref{rbfcoef} can be formally decomposed into its \emph{Hilbert-Schmidt SVD}
\[
\mK = \mPsi \mLambda_1 \mPhi_1^T,
\]
where all matrices are of size $N\times N$, but the matrix $\mPsi$ is formed as a product of semi-infinite and bi-infinite matrices, i.e.,
\begin{equation}\label{StableBasis}
\mPsi = \left(\mPhi_1\ \mPhi_2\right)\left(\begin{matrix}\mI_N\\\mLambda_2\mPhi_2^T\mPhi_1^{-T}\mLambda_1^{-1}\end{matrix}\right),
\quad
\left(\begin{matrix}\mLambda_1 & \\ & \mLambda_2\end{matrix}\right) = \text{diag}(\lambda_n)_{n=1}^\infty,\ \left(\mPhi_1\ \mPhi_2\right) = \mPhi = \left(\varphi_n(\bx_i)\right)_{i,n=1}^{N,\infty}.
\end{equation}
For practical applications the matrices $\mLambda_2$ and $\mPhi_2$ will therefore have to be truncated appropriately (see Sect.~\ref{Sec_TruncHS}). It is important to note that the matrices $\mPsi$, $\mLambda_1$, $\mLambda_2$, $\mPhi_1$ and $\mPhi_2$ are not found by manipulating $\mK$, but are formed directly from the eigenvalues and eigenfunctions of $\cT_K$. In fact, the ill-conditioned matrix $\mK$ never needs to be formed, nor is precise knowledge of the closed form of the kernel $K$ essential. As long as the eigenfunctions and eigenvalues are known, the Hilbert-Schmidt SVD enables us to formulate stable and efficient algorithms for the associated kernel approximation problems \citep{CFMcC13}.

There are at least two ways to interpret the Hilbert-Schmidt SVD:
(i) we've found an invertible $\mP = \mLambda_1\mPhi_1^T$ such that $\mPsi=\mK\mP^{-1}$ is better conditioned than $\mK$, and therefore we have essentially constructed a ``better basis'' (namely the functions that give rise to the columns of $\mPsi$); and
(ii) we've diagonalized the matrix $\mK$, i.e., we have found a diagonal matrix $\mLambda_1$ of Hilbert-Schmidt singular values along with matrices $\mPsi$ and $\mPhi_1$ which are generated by orthogonal eigenfunctions (but are not orthogonal matrices).

The matrix $\mPsi$ can be computed stably, and we can convert the standard (ill-conditioned) interpolation system \eqref{rbfcoef} into a well-conditioned linear system $\mPsi \bb = \by$ (where $\bb=\mP\bc$). A typical comparison between interpolation errors based on the standard (or direct) approach \eqref{rbfapprox} and the Hilbert-Schmidt approach applied to isotropic Gaussian kernel interpolation is shown in the left plot of Fig.~\ref{Fig_HSSVD}. The most accurate results are obtained for a positive value of $\gamma$ (they compare favorably to a polynomial interpolant corresponding to the ``flat'' $\gamma\to0$ limit). Moreover, in or near the ``flat'' limit regime the standard approach generates completely unreliable results (the dashed lines), i.e., the standard approach yields reliable answers only for relatively spiky kernels. It should also be clear that the traditional heuristic based on the uncertainty principle, i.e., ``improve accuracy by sampling data more densely and then use a rescaled, i.e., spikier, kernel $K$ to prevent the matrix $\mK$ from becoming too ill-conditioned'' does not work.

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=.4\linewidth]{Fig_Error_ex17j}
%    \includegraphics[width=.4\linewidth]{Fig_MLE_ex17j}
%\caption{Interpolation errors (left) and MLE predictions of ``optimal'' $\gamma$ (right) for data sampled at $N$ Halton points from $f(\bx) = \sin\left(\tfrac{x_1+x_2+\ldots+x_5}{5}\right)$, $\bx=(x_1,x_2,\ldots,x_5) \in [-1,1]^5$. Solid lines denote Hilbert-Schmidt SVD, dashed lines standard basis approach.}\label{Fig_HSSVD}
%\end{figure}

While Fig.~\ref{Fig_HSSVD} demonstrates that we are able to solve simple moderate-dimensional problems, the results of \cite{FasHicWoz12b, FasHicWoz12a} show that the use of anisotropic kernels is essential for maintaining good convergence rates in high dimensions. Therefore, \emph{implementation of the Hilbert-Schmidt SVD algorithm needs to be extended to the anisotropic case}.

There are many other kernels for which it would be desirable to have stable and reliable algorithms. Based on the general framework provided by the Hilbert-Schmidt SVD, all that one needs are the eigenvalues and eigenfunctions of $\cT_K$. One family of special interest in the statistics community \citep{Ste99} are the Mat\'ern kernels, $K(\bx,\bt) = K_{m-d/2}\left(\gamma\|\bx-\bt\|\right) \left( \gamma \| \bx - \bt \|\right)^{m-d/2}$, $m > \frac d2$,
where $K_{m-d/2}$ are modified Bessel functions of the second kind and $m$ is a smoothness parameter that should be chosen in a data-dependent way together with the shape parameter $\gamma$. Our work on compact Mat\'ern kernels \citep{CFMcC13}
\begin{equation}\label{CMatern}
K(x,t) = \sum_{n=1}^{\infty} \left(n^2\pi^2 + \gamma^2\right)^{-\beta} 2\sin(n\pi x) \sin(n\pi t), \quad x,t \in [0,1],\ \gamma>0, \beta \in \mathbb{N},
\end{equation}
is a first step in this direction. The Hilbert-Schmidt series \eqref{CMatern} of the compact Mat\'ern kernels is also a perfect candidate for the data-dependent error bounds discussed at the end of Sect.~\ref{SecRecovery} since its eigenfunctions do not depend on the parameters $\beta$ and $\gamma$, while a smaller smoothness parameter $\beta$ leads to slower decay of the eigenvalues. Therefore, \emph{we expect to make progress on developing a Hilbert-Schmidt SVD algorithm for Mat\'ern kernels and on data dependent error bounds for multivariate function approximation}.


\subsection{Making the Most of ``Nuisance'' Parameters}
As Fig.~\ref{Fig_HSSVD} shows, for any given set of data, finding an ``optimal'' (or near-optimal) value of the shape parameter(s) can have a significant impact on the accuracy of the recovered function $\tf$. A number of potential criteria for optimizing the shape parameters were introduced by \cite{Fasshauer11}. However, up to now all of those criteria have been based on the use of the standard approach, i.e., they were subject to the effects of ill-conditioning of the $\mK$ matrix, and thus often unreliable or impractical. Recently, the MLE criterion (or more precisely, negative concentrated log-likelihood) was the first of these criteria to be converted to the stable framework. A plot demonstrating the typical effect of this conversion is given in the right part of Fig.~\ref{Fig_HSSVD}. \emph{We propose to extend the application of the Hilbert-Schmidt SVD to other criteria, and then to investigate which of these criteria is most effective.}

This latter investigation necessitates a framework in which such a comparison can take place. We plan to answer questions regarding the \emph{consistency}, \emph{rate of convergence} and \emph{stability} of various parameter estimation schemes. For consistency we ask whether a scheme is able to recover the ``true'' value of $\gamma$ as we sample more and more densely. The rate of convergence of a scheme determines how fast the ``optimal'' value suggested by the method approaches the ``true'' value as the sampling density increases, and stability of a scheme will be judged by the effects small changes in the data have on the consistency and convergence rate of the scheme. \emph{We intend to investigate and introduce a metric for comparing the quality of different parameter estimation criteria, and we also plan to derive other parameter optimization criteria, including for the low-rank version of the Hilbert-Schmidt SVD mentioned in the next section.}


\subsection{Truncating the Hilbert-Schmidt Series}\label{Sec_TruncHS}
In order to make the Hilbert-Schmidt SVD practical and ensure that it can be used in conjunction with the error guarantees of Sect.~\ref{SectCones}, a theoretical basis for the truncation of the Hilbert-Schmidt series needs to be established. \cite{FMcC12} introduced two different scenarios: (i) truncation at $M\ge N$ terms so that every entry $K(\bx_i,\bx_j)$ of $\mK$ is represented to within a tolerance $\varepsilon$, (ii) truncation at $M < N$. The former approach results in data-dependent basis functions generated by the $N$ first eigenfunctions of $\cT_K$ corrected by linear combinations of the next $M-N$ eigenfunctions (cf.~\eqref{StableBasis}), while the low-rank approach (ii) uses only the first $M$ (data-independent) eigenfunctions as a basis. The method used by \cite{FMcC12} to determine the truncation length $M$ is similar to that used earlier by Fornberg and co-workers in their RBF-QR approach \citep{FornbergPiret08, FornbergFlyerLarsson11}.  We discussed a more straightforward strategy to determine $M$ in \cite{CFMcC13}. However, neither of these methods provide any direct insight on the approximation error. Work in that general direction can be found in a recent preprint \citep{GriebelRiegerZwicknagl13}. Based on this, \emph{we plan to develop improved criteria for the truncation of the Hilbert-Schmidt series.}

\subsection{Other Research Topics Related to the Hilbert-Schmidt SVD}\label{SecMEEG}
There are plenty of other related questions we might tackle if time allows or the opportunity arises. However, we would like to highlight one ongoing collaboration in which kernels are used to solve a system of coupled Poisson and Laplace PDE boundary value problems that arise in MEG and EEG modeling of brain activity. Fig.~\ref{Fig_MFS_BEM} is taken from \citep{AFFGM13} and provides a cost-per-accuracy plot comparing a kernel-based solver using a method of fundamental solutions approach with a boundary element method that is considered state-of-the-art in the community \citep{fieldtrip11}. The coupled system is solved on three nested concentric spherical shells and describes the forward problem corresponding to the electric potential on the outside surface resulting from a single dipole located inside a simplified brain-skull-scalp geometry. The three different low-rank MFS solutions are all more accurate and more efficient than the BEM solution. For any desired accuracy, the MFS solution is more efficient, and this effect is more pronounced as we demand more accuracy.

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=.35\linewidth]{Fig_MFS_BEM.eps}
%\caption{Cost per accuracy plot of several MFS solutions vs. BEM solution for coupled PDE system.}\label{Fig_MFS_BEM}
%\end{figure}

As the next steps on this project we expect (i) \emph{to integrate the forward solver into a full inverse problem framework} which will allow us to identify the locations of activity inside the brain from measurements of the electric potential (instead of simulating the electric surface potential based on a given dipole source as done for the forward problem in Fig.~\ref{Fig_MFS_BEM}), and (ii) \emph{to move to a head model that reflects the geometry of the brain more realistically}.

\section{Broader Impact of Proposed Research}\label{SectBroad}


\subsection{Contributions to Training, Mentoring and Other Human Resource Developments}

The PIs, GEF and FJH, are dedicated to mentoring young researchers from high school through postdoctoral levels.  We are excited to see our mentees transition from curiosity about what scholarship entails to choosing and solving their own problems.  We are committed to our regular Meshfree Methods research seminar because we believe that we learn good research practices by seeing them modeled and we that find good research ideas from a variety of sources. Outside of the Meshfree Methods seminar we meet with members individually or in smaller groups to discuss details of their research projects.

\begin{description}[leftmargin=2.5ex]
\item[Providing Research Experiences for Undergraduates and High School Students]\ We \linebreak[4] strongly believe that students should be introduced to research before graduate school so that they can learn how to discover the unknown, something that is not taught well in a classroom. We request funds to support two summer REU students per year. Having advertised our REU opportunities for several years now has given us some visibility within the community and is prompting inquiries from prospective participants well before we even announce our latest program offerings. As in the past two years, we expect the NSF funds will serve as a catalyst for funds to support additional summer students. In choosing REU students we make a deliberate effort to build a diverse research environment by targeting female and underrepresented minority students as well as students from less research-focused institutions (see Sect.~\ref{SectPrevious}). We will also continue to receive well-prepared high school students to join our research group as we did last summer.

\item[Preparing Students for Academic Careers] We consider mentoring to be a multi-faceted and potentially long-term process continuing even after the mentee has moved on from IIT.  For example, MM was an undergraduate student at IIT who collaborated with GEF during his PhD studies at Cornell University.  We will continue to mentor him as senior personnel for this proposal. Similarly, although QY has left IIT, GEF serves as his designated mentor on his application for travel support to the 2014 ICM in Korea. We have provided and will continue to provide SCC, a post-doctoral scientist and another senior personnel for this project, teaching and mentoring experience by including her in our present NSF project.  We will continue to find opportunities for special mentoring activities for our students, like those GEF organized and that QY and YD were involved in during the recent NSF-supported 14th International Conference on Approximation Theory in San Antonio, TX.  We  will continue our collaboration with Argonne National Laboratory (ANL), which has led to short-term and long-term opportunities for our PhD students and graduates.

\item[Preparing Students for Industry Careers]
In addition to preparing students for the academic landscape, we also help current students land competitive jobs in the business world. The training we provide in the areas of algorithm development and coding tends to give our students the needed edge. For example, WM, who graduated from Brown University in May 2013 is currently working for an internet startup company.

\item[Supervising Visitors]
GEF has established contacts with several Italian universities attracting students and postdoctoral visitors to IIT for extended visits (see Sect.~\ref{SectPrevious}). A visit by a PhD student from the University of Padua is currently in the planning stages for the fall of 2014.  Having lived in Hong Kong for 19 years, FJH has contacts with Chinese scientists that have prompted several long-term visiting Chinese scholars and students to join our research.  These activities will continue.

\item[Giving Short Courses and Invited Lectures]
We will continue our active track record (see Sect.~\ref{SectPrevious}) of providing lectures to students at various stages in their careers, ranging from high school to graduate school. These encourage students to enter STEM and encourage STEM students to engage in research.

\end{description}

\subsection{Contributions to Resources in Research, Education and the Broader Society}

The research we propose straddles mathematics, statistics, computer science, and applications in engineering and related fields.  The two PIs have complementary strengths that facilitate this interdisciplinary research.  GEF has expertise in approximation theory, meshfree methods, and numerical partial differential equations, while FJH has expertise in (quasi-)Monte Carlo methods, kernel-based methods, information-based complexity theory, and experimental design. Our expertise provides both an obligation and an opportunity to interact with a number of diverse communities. We envision the following contributions:

\begin{description}[leftmargin=2.5ex]
\item[Disseminating Research]
The research supported by this grant will result in publications in peer-reviewed journals in a broad spectrum of applied mathematics, computer science, statistics and engineering. These journals will include both those that emphasize theory and those that emphasize applications.

\item[Promoting Cones] The idea of guaranteed, adaptive algorithms via cones of input functions has broad potential application.  We will be promoting this idea among numerical analysts who develop new algorithms and analyze their computational costs, as well as among information-based complexity theorists who analyze the lower bounds on the complexity of numerical problems.

\item[Promoting the Hilbert-Schmidt SVD] Similarly, we will encourage other researchers to take advantage of the stability given to kernel-based methods by the Hilbert-Schmidt SVD, to use our code, and to join our research efforts.

\item[Bridging Mathematics and Statistics]
This project touches on topics that are of interest to the statistics community: kriging, Monte Carlo methods, and design of experiments.  Historically, there has been relatively little interaction between numerical analysts and statisticians.  We have and will continue to engage the statistics community by speaking a their conferences and departmental seminars.  MM is currently delivering a series of lectures to the statistics group at CU-Denver, and GEF will join this effort during several visits during the spring 2014 semester.

\item[Collaborating with Engineers]
As a result of the visit by SG (see Sect.~\ref{SectPrevious}), GEF and MM have recently submitted a bilateral research proposal to the Italian government together with Guido Ala (Dept. Electrical Engineering, U Palermo) and Elisa Francomano (Dept. Chemical Engineering, Management, Computer Science and Mechanical Engineering, U Palermo) entitled ``Novel Numerical Methods and Embedded Systems for the Integration of Neuroimaging Techniques''. In that project kernel methods are being used for the solution of inverse problems arising in the detection of brain activity from MEG or EEG data (see Sect.~\ref{SecMEEG}). This fully non-invasive diagnostic procedure will enable doctors to detect early functional and neurophysiological markers of diseases (e.g., of Alzheimer's disease). It will also result in a potential reduction in doctors' visits, shorter hospitalization periods and a greater longevity with overall improved quality of life. The bilateral project proposal does not include any direct support for GEF and MM. In fact, it depends on travel funds obtained via the current proposal. Since kernel methods are becoming a rather popular numerical tool in science in engineering, other opportunities for collaborations outside mathematics frequently arise. For example, GEF participated in an incubator meeting on freeform surfaces organized by the Optical Society of America.

\item[Organizing and Presenting at Conferences]
We and our students involved in this project will present our results at a variety of conferences and workshops.  These include: (i) specialized meetings focusing on approximation theory, complexity, experimental design, meshfree methods, and Monte Carlo methods; (ii) the national meetings of AMS, SIAM, and the statistical societies; and (iii) conferences devoted to application areas.  We are frequently invited to speak at such conferences, which will give our results a prominent hearing. We will also continue to organize specialized conferences or minisymposia within larger conferences. For example, GEF and MM are planning to organize a minisymposium on \emph{Advances in Kernel Methods for Analysis and Statistics} at the 2014 SIAM Annual Meeting, and FJH and SCC are organizing a minisymposium on \emph{Reliable Computational Science} at the same meeting.

\item[Writing Textbooks and Survey Papers]
GEF and MM are currently under contract with \linebreak[4] World Scientific Publishers to prepare a monograph entitled \emph{Kernel-based Approximation Methods using \Matlab}. This book will provide an exposition of the theory and implementation of the Hilbert-Schmidt SVD along with numerous applications. The book will form a bridge to the GaussQR library currently under development and may serve as a textbook for a graduate class on meshfree methods, such as MATH 590 at IIT. GEF and FJH occasionally publish survey articles (e.g., a 42 page paper on kernel-based methods \citep{Fasshauer11}).

\item[Refreshing Course Syllabi]
MATH 590 (Meshfree Methods), taught by GEF in the fall semester of every even-numbered year, is likely to undergo a major change in the fall of 2014 as the preparation of the monograph mentioned above progresses. MATH 565 (Monte Carlo Methods in Finance), taught every fall by FJH, already includes our new results on guaranteed confidence intervals using IID sampling, and in the future will include our work in progress on guaranteed multi-level and quasi-Monte Carlo sampling (Sect.\ \ref{MC_g_sec}).

\item[Changing How Numerical Analysis Is Taught] Current texts teach students to estimate the error of the trapezoidal rule, $T_n(f)$, by $[T_n(f)-T_{n/2}(f)]/3$ (see, e.g., \cite[p.\ 223--224]{BurFai10}).  The arguments leading to this estimate introduce the valuable concept of extrapolation, however, this is a flawed error estimate for the same reasons that the error estimate in \Matlab's {\tt integral.m} is flawed (see Sect.\ \ref{drugssubsect}).  We will urge numerical analysis textbook authors and educators to change the way that error estimation is taught based on our recent and proposed work.  These ideas will also enter our more traditional numerical analysis courses such as MATH 350 (Introduction to Computational Mathematics) or MATH 577 and 578 (Computational Mathematics I \& II).

\item[Creating Software and Collaborating with Software Developers]
GEF and MM have created the website \citep{McCFBG13} which serves as the home for the software on our stable algorithms built upon the Hilbert-Schmidt SVD. This \Matlab library is freely available and allows others to experiment with our code. Thus far, the library contains routines for Gaussian kernels and for compact Mat\'ern kernels. As our research expands to other kernels and their eigenexpansions the resulting code will be added to the library. The GaussQR library also serves as a sandbox for students---especially REU students---to learn about our research and allows them to contribute pieces of their own work.

We will continue to develop GAIL \citep{ChoEtal14a} as part of our proposed research.  The GAIL software will serve the wider community that relies on numerical approximation and integration algorithms.  It will also demonstrate how automatic algorithms ought to be implemented, which we hope will inspire and inform those working on automatic, adaptive algorithms for other mathematical problems.  We also expect our new algorithms to be incorporated into widely used numerical packages, as was done for the algorithm in \cite{HonHic00a} by \Matlab \citep{MAT8.2} and NAG \citep{NAG23}.  We will continue to discuss with software developers about these issues.

\item[Reaching Out]
GEF and FJH both have a record of reaching out to high school students and we plan to continue. The website {\tt http://math.iit.edu/$\sim$openscholar/meshfree/} helps manage our internal and external communication and dissemination of research findings. Advertising for the summer REU experiences is also facilitated via this website.


\end{description}


\newpage
\clearpage
\pagenumbering{arabic}

\bibliographystyle{spbasic}

\renewcommand{\refname}{\hfill \textbf{\large References Cited} \hfill \hfill}                   %%
\renewcommand{\bibliofont}{\normalsize}

\bibliography{FJH22,FJHown23,GEF}
\end{document}
