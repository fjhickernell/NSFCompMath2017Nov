% !TEX TS-program = PDFLatexBibtex
%&LaTeX
%Greg and Fred's NSF Grant Dec 2014
\documentclass[11pt]{NSFamsart}
\usepackage{latexsym,amsfonts,amsmath,epsfig,multirow}
\usepackage{stackrel,tabularx,enumitem,xspace}
\usepackage[numbers]{natbib}
\usepackage{hyperref,accents}

% This package prints the labels in the margin
%\usepackage[notref,notcite]{showkeys}


%\pagestyle{empty}
\thispagestyle{plain}
\pagestyle{plain}

\headsep-0.6in
%\headsep-0.45in

\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\textheight9in
%\textheight9.1in

\providecommand{\FJHickernell}{Hickernell}
\newcommand{\hI}{\hat{I}}
\newcommand{\hatf}{\hat{f}}
\newcommand{\hatg}{\hat{g}}
\newcommand{\tf}{\tilde{f}}
\newcommand{\tbf}{\tilde{\bff}}
\DeclareMathOperator{\cost}{cost}
\DeclareMathOperator{\loss}{loss}
\DeclareMathOperator{\lof}{lof}
\DeclareMathOperator{\reg}{reg}
\DeclareMathOperator{\CV}{CV}

\def\reals{{\mathbb{R}}}
\def\field{{\mathbb{F}}}
\def\complex{{\mathbb{C}}}
\def\naturals{{\mathbb{N}}}
\def\integer{{\mathbb{Z}}}
\def\expect{{\mathbb{E}}}
\def\il{\left<}
\def\ir{\right>}
\def\e{\varepsilon}
\def\g{\gamma}
\def\l{\lambda}
\def\b{\beta}
\def\a{\alpha}
\def\lall{\Lambda^{{\rm all}}}
\def\lstd{\Lambda^{{\rm std}}}

\newcommand{\bbE}{\mathbb{E}}
\newcommand{\tQ}{\widetilde{Q}}
\newcommand{\mA}{\mathsf{A}}
\newcommand{\mB}{\mathsf{B}}
\newcommand{\mC}{\mathsf{C}}
\newcommand{\mD}{\mathsf{D}}
\newcommand{\mG}{\mathsf{G}}
\newcommand{\mH}{\mathsf{H}}
\newcommand{\mI}{\mathsf{I}}
\newcommand{\mK}{\mathsf{K}}
\newcommand{\tmK}{\widetilde{\mathsf{K}}}
\newcommand{\mL}{\mathsf{L}}
\newcommand{\mM}{\mathsf{M}}
\newcommand{\mP}{\mathsf{P}}
\newcommand{\mQ}{\mathsf{Q}}
\newcommand{\mR}{\mathsf{R}}
\newcommand{\mX}{\mathsf{X}}
\newcommand{\mPhi}{\mathsf{\Phi}}
\newcommand{\mPsi}{\mathsf{\Psi}}
\newcommand{\mLambda}{\mathsf{\Lambda}}

\DeclareMathOperator{\APP}{APP}
\DeclareMathOperator{\INT}{INT}
\DeclareMathOperator{\app}{app}
\DeclareMathOperator{\integ}{int}
\DeclareMathOperator{\hAPP}{\widehat{\APP}}
\DeclareMathOperator{\hINT}{\widehat{\INT}}
\DeclareMathOperator{\happ}{\widehat{\app}}
\DeclareMathOperator{\hint}{\widehat{\integ}}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\oerr}{\overline{\err}}
\DeclareMathOperator{\apperr}{aerr}
\DeclareMathOperator{\oaerr}{\overline{\apperr}}
\DeclareMathOperator{\interr}{ierr}
\DeclareMathOperator{\oierr}{\overline{\interr}}

\DeclareMathOperator{\Spl}{Spline}
\DeclareMathOperator{\SSpl}{SmSpline}
\DeclareMathOperator{\Power}{Power}
\DeclareMathOperator{\RegSpl}{RegSpline}
\DeclareMathOperator{\MLS}{MLS}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\rms}{rms}
\DeclareMathOperator*{\RMSE}{RMSE}
\DeclareMathOperator*{\bias}{bias}
\DeclareMathOperator*{\var}{var}

\newcommand{\bone}{\boldsymbol{1}}
\newcommand{\bzero}{\boldsymbol{0}}
\newcommand{\binf}{\boldsymbol{\infty}}
\newcommand{\ba}{{\boldsymbol{a}}}
\newcommand{\bb}{{\boldsymbol{b}}}
\newcommand{\bc}{{\boldsymbol{c}}}
\newcommand{\bd}{{\boldsymbol{d}}}
\newcommand{\be}{{\boldsymbol{e}}}
\newcommand{\bff}{{\boldsymbol{f}}}
\newcommand{\bhh}{{\boldsymbol{h}}}
\newcommand{\beps}{{\boldsymbol{\varepsilon}}}
\newcommand{\tbeps}{\tilde{\beps}}
\newcommand{\bx}{{\boldsymbol{x}}}
\newcommand{\bX}{{\boldsymbol{X}}}
\newcommand{\bh}{{\boldsymbol{h}}}
\newcommand{\bk}{{\boldsymbol{k}}}
\newcommand{\bg}{{\boldsymbol{g}}}
\newcommand{\bv}{{\boldsymbol{v}}}
\newcommand{\bu}{{\boldsymbol{u}}}
\newcommand{\by}{{\boldsymbol{y}}}
\newcommand{\bt}{{\boldsymbol{t}}}
\newcommand{\bz}{{\boldsymbol{z}}}
\newcommand{\bvarphi}{{\boldsymbol{\varphi}}}
\newcommand{\bgamma}{{\boldsymbol{\gamma}}}
\newcommand{\bphi}{{\boldsymbol{\phi}}}
\newcommand{\bpsi}{{\boldsymbol{\psi}}}
\newcommand{\bpi}{{\boldsymbol{\pi}}}
\newcommand{\balpha}{{\boldsymbol{\alpha}}}
\newcommand{\bbeta}{{\boldsymbol{\beta}}}
\newcommand{\bo}{{\boldsymbol{\omega}}}  %GF added
\newcommand{\newton}[2]{\left(\begin{array}{c} #1\\ #2\end{array}\right)}
\newcommand{\anor}[2]{\| #1\|_{\mu_{#2}}}
\newcommand{\satop}[2]{\stackrel{\scriptstyle{#1}}{\scriptstyle{#2}}}
\newcommand{\setu}{{\mathfrak{u}}}

\newcommand{\me}{\textup{e}}
\newcommand{\mi}{\textup{i}}
\def\d{\textup{d}}
\def\dif{\textup{d}}
%\def\d{{\rm d}}

\def\eavg{{\rm error^{avg}}}
\def\ewc{{\rm error^{wor}}}
\def\ewcavg{{\rm error^{\left\{\substack{\rm wor \\ \rm avg}\right\}}}}
\def\eran{{\rm error^{ran}}}
%\newcommand{\cl}{{\mathfrak{L}}}
\newcommand{\cc}{\mathcal{C}}
\newcommand{\cb}{\mathcal{B}}
\newcommand{\cl}{L}
\newcommand{\cx}{{\Omega}}
\newcommand{\calc}{{\mathcal{C}}}
\newcommand{\calf}{{\mathcal{F}}}
\newcommand{\calfd}{{\calf_d}}
\newcommand{\calh}{{\mathcal{H}}}
\newcommand{\tcalh}{{\widetilde{\calh}}}
\newcommand{\calhk}{\calh_d(K)}
\newcommand{\calg}{{\mathcal{G}}}
\newcommand{\calgd}{{\calg_d}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\fA}{\mathfrak{A}}
\newcommand{\fC}{\mathfrak{C}}
\newcommand{\fF}{\mathfrak{F}}
\newcommand{\fL}{\mathfrak{L}}
\newcommand{\hS}{\widehat{S}}
\DeclareMathOperator{\Prob}{\mathbb{P}}

\def\abs#1{\ensuremath{\left \lvert #1 \right \rvert}}
\newcommand{\bigabs}[1]{\ensuremath{\bigl \lvert #1 \bigr \rvert}}
\newcommand{\norm}[2][{}]{\ensuremath{\left \lVert #2 \right \rVert}_{#1}}
\newcommand{\ip}[3][{}]{\ensuremath{\left \langle #2, #3 \right \rangle_{#1}}}
\newcommand{\bignorm}[2][{}]{\ensuremath{\bigl \lVert #2 \bigr \rVert}_{#1}}
\newcommand{\calm}{{\mathfrak{M}}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\filldis}{fill}
\DeclareMathOperator{\sep}{sep}
\DeclareMathOperator{\avg}{avg}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\cov}{cov}

\newcommand{\des}{\{\bx_i\}}
\newcommand{\desinf}{\{\bx_i\}_{i=1}^{\infty}}
\newcommand{\desn}{\{\bx_i\}_{i=1}^N}
\newcommand{\wts}{\{g_i\}_{i=1}^N}
\newcommand{\wtsn}{\{g_i\}_{i=1}^N}
\newcommand{\datan}{\{y_i\}_{i=1}^N}

%FJH added
\newcommand{\Order}{\mathcal{O}}
\newcommand{\ch}{\mathcal{H}}
\newcommand{\tch}{{\widetilde{\ch}}}
\newcommand{\veps}{\boldsymbol{\varepsilon}}
\DeclareMathOperator{\best}{best}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\hsigma}{\hat{\sigma}}
\newcommand{\tK}{\widetilde{K}}
\newcommand{\Matlab}{{\sc Matlab}\xspace}
\newcommand{\abstol}{\varepsilon_{\text{a}}}
\newcommand{\reltol}{\varepsilon_{\text{r}}}

\newcommand\starred[1]{\accentset{\star}{#1}}

\newtheorem{resproblem}{Research Problem}
\newtheorem{research}{Research Objectives}

\newcommand{\refproba}{\hyperref[SectHSSVD]{Research Project~1}}
\newcommand{\refprobaa}{\hyperref[AnalyticEigensubsec]{Research Project~1.1}}
\newcommand{\refprobab}{\hyperref[NumerEigensubsec]{Research Project~1.2}}
\newcommand{\refprobac}{\hyperref[SectDesignerKernels]{Research Project~1.3}}

\newcommand{\refprobb}{\hyperref[SectGAIL]{Research Project~2}}

\newcommand{\refprobc}{\hyperref[combinesec]{Research Project~3}}
\newcommand{\refprobca}{\hyperref[errestsubsec]{Research Project~3.1}}
\newcommand{\refprobcb}{\hyperref[parestsubsec]{Research Project~3.2}}
\newcommand{\refprobcc}{\hyperref[designsubsec]{Research Project~3.3}}

\newcommand{\refprobd}{\hyperref[appsec]{Research Project~4}}
\newcommand{\refprobda}{\hyperref[PDEsubsec]{Research Project~4.1}}
\newcommand{\refprobdb}{\hyperref[SectMEEG]{Research Project~4.2}}
\newcommand{\refprobdc}{\hyperref[Sec_TruncHS]{Research Project~4.3}}
\newcommand{\refprobdd}{\hyperref[ebolasubsec]{Research Project~4.4}}


%\setcounter{page}{1}


\begin{document}
%\setlength{\leftmargini}{2.5ex}

\centerline{\Large \bf Project Description}
\section{Scientific Context and Timeliness of the Proposed Research}
For a long time, scientific research was classified as being either theoretical or experimental in nature. More recently, however, computer simulation (or computer experimentation) has emerged as a third pillar of science \citep{PITAC05,OdenGhattas14}. Sometimes computer experimentation is the only way to obtain an understanding of a physical process, and sometimes it is just considerably cheaper, safer, ethically more responsible or environmentally friendlier than performing an actual physical experiment.
Computer simulation arises quite naturally in situations where one has a complex mathematical model (e.g., specified in terms of a system of partial differential equations or involving randomness) and one can use a computational approach to produce an output (or response) based on a given set of initial conditions and/or model parameters. A few typical examples of where computer simulations are employed are weather and climate modeling, drug design, financial risk management \citep{Gla03}, and the design of nuclear reactors \citep[Sect.\ 2.4]{Smi14a}.

Computer simulation can be quite costly in terms of computation time and high-performance computing hardware.  The potential cost increases when one wishes to perform uncertainty quantification or sensitivity analysis.  Substantial cost savings can often be realized by constructing inexpensive \emph{surrogate models} from the actual outputs of these complex computer simulations \citep{FangEtAl06,ForEtal09,SantnerWilliamsNotz03}.

The research proposed here focuses primarily on \emph{algorithms for constructing surrogates} that will allow us to quickly and reliably predict the output of the computer simulation for a previously not used combination of input factors with an accuracy guaranteed to lie within a user-specified tolerance.  We want our algorithms to have the following properties:
\begin{description}[leftmargin=2.5ex]
\item[Stability] The computations do not suffer from catastrophic round-off error.

\item[Adaptivity] The algorithms adjust their parameters based on the data they observe---rather than a priori knowledge---to meet the specified error tolerance, and they do so with theoretical guarantees.

\item[Efficiency] The computational cost of the algorithms are asymptotically optimal---compared to the best possible algorithms---as the error tolerance vanishes.
\end{description}

In addition to surrogate construction---essentially a problem of function approximation---we will also research two related problems.  One is integration, a simpler problem that shares many of the same mathematical features of function approximation.  Moreover, we will also propose the use of kernels for the numerical solution of PDEs \citep{ChenEtAl14,FornbergFlyer15,SarraKansa09}.

%For the surrogate construction problem, each data point consists of a combination of the input parameters and the corresponding output value. In other words, one needs to run the computer simulation multiple times in order to produce a set of data, and then one can use a function approximation algorithm to produce a response surface which in turn can be evaluated stably, efficiently and accurately for any desired set of input parameters. The number of these input parameters determines the dimensionality of the data fitting problem, and it is not difficult to imagine that this dimension can be rather large. {\bf [Fred, can you add a similar statement about integration?]}

\section{Overview of Proposed Research}
Kernel methods \citep{Fas07a,SchWen06a,Wen05a} are often viewed as an ideal approach to the function approximation problem, which in turn serves as a fundamental application in the context of scattered data fitting and surrogate modeling, but also for classification and regression approaches in machine learning \citep{HasTibFrie01} and as the starting point for various approaches to the solution of partial differential equations, such as, e.g., pseudospectral collocation methods, partition of unity methods, or RBF-based finite difference methods \citep{Fas07a,FornbergFlyer15}.

In the context of surrogate modeling, we are presented with a (costly) computer simulation which generates an output/response, $y=f(\bx)$, given any input $\bx \in \Omega \subset\reals^d$.  The goal is to construct a surrogate, $\tf$, based on the data $(\bx_1, y_1), \ldots, (\bx_N,y_N)$.  An approach based on kernel methods takes the form
\begin{subequations} \label{rbfapprox}
\begin{equation} \label{rbfform}
\tf(\bx) = \sum_{j=1}^N K(\bx,\bx_j) c_j,
\end{equation}
where $K:\Omega \times \Omega \to \reals$ is a known symmetric, positive definite kernel. One example is the Gaussian (or squared exponential) kernel:
\begin{equation}  \label{gausskernel}
K(\bx,\bt) = \exp\bigl(-\gamma_1^2 (x_1-t_1)^2 - \cdots - \gamma_d^2 (x_d-t_d)^2 \bigr), \quad \bx = (x_1,\ldots,x_d),\ \gamma_1,\ldots,\gamma_d >0,
\end{equation}
but there are also other good choices for the kernel.  In the most-straightforward variant, the unknown $\bc=(c_1, \ldots, c_N)^T$ is determined by imposing interpolation.  Specifically,
\begin{equation} \label{rbfcoef}
\bc=\mK^{-1} \by, \qquad \mK=\bigl(K(\bx_i,\bx_j)\bigr)_{i,j=1}^N, \quad \by=\bigl(f(\bx_i)\bigr)_{i=1}^N.
\end{equation}
\end{subequations}

This numerical algorithm goes by different names, such as, radial basis function approximation \citep{Buh03a}, scattered data approximation \citep{Wen05a}, meshfree approximation \citep{Fas07a}, and kriging \citep{Ste99}. It is part of the JMP \citep{JMP11}, \Matlab \citep{MAT8.4} and R \citep{R3.03_2013} software packages.  It is flexible because the data sites need not follow a regular pattern. There is a deterministic optimality: $\tf$ is the best approximation to $f$ in the Hilbert space whose reproducing kernel is $K$ \citep{Fas07a,Wen05a}.  There is a stochastic optimality: $\tf$ is the best approximation to $f$ in the set of Gaussian stochastic processes whose covariance kernel is $K$ \citep{BerT-A04,Wah90}.  The procedure outlined above can be extended to include the exact reproduction of polynomials and also the situation where the $y_i$ are noisy observations of the $f(\bx_i)$ \citep{Wah90}.
One of the main accomplishments of \emph{NSF-DMS-1115392 Kernel Methods for Numerical Computation} (see Sect.~\ref{SectPrevious}) was the development of the so-called \emph{Hilbert--Schmidt SVD} \citep{CavorettoEtAl14,FMcC12,McCF14,McCourtFas14} (see \refproba). This algorithm provides a framework for finding a new stable basis for often ill-conditioned kernel methods.
A second major accomplishment was the development of \emph{guaranteed, adaptive numerical integration algorithms} \citep{HicEtal14b,HicEtal14a,HicJim16a,JimHic16a} (see \refprobb).

Building upon these previous research accomplishments, we propose to extend our work in several different directions:

\begin{description}[leftmargin=2.5ex]

\item[\refproba] Extend our research on the Hilbert--Schmidt SVD by identifying more kernels and their Mercer series for which the Hilbert--Schmidt SVD can be implemented.
\begin{enumerate}
\renewcommand{\labelenumi}{1.\arabic{enumi}.}
\item Analytically compute eigenvalues and eigenfunctions for new kernels $K$.
\item Numerically compute eigenvalues and eigenfunctions for new kernels $K$.
\item Build \emph{designer kernels}, $K$, starting from a given orthogonal set of functions $\{\varphi_n\}_{n=1}^\infty$ and appropriately decaying scalars $\{\lambda_n\}_{n=1}^\infty$ which will serve as eigenfunctions and eigenvalues for $K$.
\end{enumerate}

\item[\refprobb] Extend our research on guaranteed, adaptive algorithms.
\begin{enumerate}
\renewcommand{\labelenumi}{2.\arabic{enumi}.}
\item Develop adaptive (quasi-)Monte Carlo methods for multivariate integration that satisfy both absolute and relative error tolerances.
\item Develop adaptive univariate integration and function approximation algorithms that are higher order and locally adaptive.
\end{enumerate}

\item[\refprobc] Obtain guaranteed, adaptive algorithms for kernel methods.
\begin{enumerate}
\renewcommand{\labelenumi}{3.\arabic{enumi}.}
\item Use cones to obtain error bounds and guarantees for function approximation with kernels.
\item Use the Hilbert--Schmidt SVD for automatic and adaptive data-based parameter estimation of kernel methods.
\item Use the Hilbert--Schmidt SVD to obtain ``optimal'' automatic and adaptive data-based designs for kernel methods.
\end{enumerate}

\item[\refprobd] Apply new results.
\begin{enumerate}
\renewcommand{\labelenumi}{4.\arabic{enumi}.}
\item Use kernel methods and the Hilbert--Schmidt SVD for the solution of PDEs via so-called RBF-(W)ENO (weighted essentially non-oscillatory) methods.
\item Use kernel methods for the solution of the MEG/EEG forward problem.
\item Use the eigenfunction expansion of kernels for low-rank approximations in machine learning.
\item Use kernel methods to create a surrogate model to predict the spread of Ebola in West Africa.
\end{enumerate}
\end{description}
In the following sections we outline our plans for completing these four projects.

\section*{Research Project 1. Extensions of the Hilbert--Schmidt SVD}\label{SectHSSVD}

Every positive definite kernel $K$ has a Hilbert--Schmidt (or Mercer) series decomposition \citep{CourantHilbert53,RasWil06a}
\begin{equation}\label{HSseries}
K(\bx,\bt) = \sum_{n=1}^\infty \lambda_n \varphi_n(\bx) \varphi_n(\bt),
\end{equation}
where the $\lambda_n$ and $\varphi_n$ are the eigenvalues and eigenfunctions of the associated Hilbert--Schmidt integral operator $\cK : L_2(\Omega, \rho) \to L_2(\Omega, \rho)$ which depends on the probability density $\rho$ and is defined by
\begin{equation}\label{TK}
(\cK f)(\bx) = \int_\Omega K(\bx, \bt)f(\bt)\, \rho(\bt)\, \d \bt.
\end{equation}

The Hilbert--Schmidt SVD \citep{CavorettoEtAl14,FMcC12} of the kernel matrix $\mK$ from \eqref{rbfcoef} is the \emph{formal decomposition}
\begin{subequations}\label{HSSVD}
\begin{equation}
\mK = \mPsi \mLambda_1 \mPhi_1^T,
\end{equation}
where all matrices are of size $N\times N$, but the matrix $\mPsi$ is formed as a product of semi-infinite and bi-infinite matrices, i.e.,
\begin{equation}\label{StableBasis}
\mPsi = \left(\mPhi_1\ \mPhi_2\right)\left(\begin{matrix}\mI_N\\\mLambda_2\mPhi_2^T\mPhi_1^{-T}\mLambda_1^{-1}\end{matrix}\right),
\quad
\left(\begin{matrix}\mLambda_1 & \\ & \mLambda_2\end{matrix}\right) = \text{diag}(\lambda_n)_{n=1}^\infty,\ \left(\mPhi_1\ \mPhi_2\right) = \mPhi = \left(\varphi_n(\bx_i)\right)_{i,n=1}^{N,\infty}.
\end{equation}
\end{subequations}
For practical applications the matrices $\mLambda_2$ and $\mPhi_2$ will therefore have to be truncated appropriately (see Sect. \refprobdc). We refer to the Hilbert--Schmidt SVD \eqref{HSSVD} as a ``formal decomposition'' since it is important to note that the matrices $\mPsi$, $\mLambda_1$, $\mLambda_2$, $\mPhi_1$ and $\mPhi_2$ are not found by manipulating $\mK$, but are formed directly from the eigenvalues and eigenfunctions of $\cK$. In fact, the ill-conditioned matrix $\mK$ never needs to be formed, nor is precise knowledge of the closed form of the kernel $K$ essential. As long as the eigenfunctions and eigenvalues are known, the Hilbert--Schmidt SVD enables us to formulate stable and efficient algorithms for the associated kernel approximation problems \citep{CavorettoEtAl14}.

A typical comparison between interpolation errors based on the standard (or direct) approach \eqref{rbfapprox} and the Hilbert--Schmidt approach $\mPsi \bb = \by$ applied to isotropic Gaussian kernel interpolation is shown in the left plot of Fig.~\ref{Fig_HSSVD}. The most accurate results are obtained for a positive value of $\gamma$ (they compare favorably to a polynomial interpolant corresponding to the ``flat'' $\gamma\to0$ limit). Moreover, in or near the ``flat'' limit regime the standard approach generates completely unreliable results (the dashed lines), i.e., the standard approach yields reliable answers only for relatively spiky kernels.

\begin{figure}[h]
    \centering
    \includegraphics[width=.4\linewidth]{Fig_Error_ex17j}
    \includegraphics[width=.4\linewidth]{Fig_MLE_ex17j}
\caption{Interpolation errors (left) and maximum likelihood estimates of ``optimal'' $\gamma$ (right) for data sampled at $N$ Halton points from $f(\bx) = \sin\left(\tfrac{x_1+x_2+\ldots+x_5}{5}\right)$, $\bx=(x_1,x_2,\ldots,x_5) \in [-1,1]^5$. Solid lines denote Hilbert--Schmidt SVD, dashed lines standard basis approach.}\label{Fig_HSSVD}
\end{figure}

While Fig.~\ref{Fig_HSSVD} demonstrates that we are able to solve simple moderate-dimensional problems, the results of \cite{FasHicWoz12b, FasHicWoz12a} show that the use of anisotropic kernels is essential for maintaining good convergence rates in high dimensions. Therefore, in addition to the research projects we are about to describe, the implementation of the Hilbert--Schmidt SVD algorithm needs to be extended to the anisotropic case.

\subsection*{Research Project 1.1. Analytical Computation of Eigensystems} \label{AnalyticEigensubsec}

In addition to the Gaussian kernel \citep{FMcC12} and the iterated Brownian bridge kernels \citep{CavorettoEtAl14}, there are many other kernels for which it would be desirable to have stable and reliable algorithms. One family of special interest in the statistics community \citep{Ste99} are the Mat\'ern kernels defined on $\reals^d$, $K(\bx,\bt) = K_{\beta-d/2}\left(\gamma\|\bx-\bt\|\right) \left( \gamma \| \bx - \bt \|\right)^{\beta-d/2}$, $\beta > \frac d2$,
where $K_{\beta-d/2}$ are modified Bessel functions of the second kind and $\beta$ is a smoothness parameter that should be chosen in a data-dependent way together with the shape parameter $\gamma$. For $\beta = \frac{d + 2k+1}{2}$, $k=0,1,2,\ldots$, the Mat\'ern kernels take on a simpler form. For example
\begin{align*}
\text{$k = 0$, i.e., $\beta = \frac{d+1}{2}$:}\quad & K(\bx,\bt) = \me^{-\gamma \|\bx-\bt\|}, \\
\text{$k = 1$, i.e., $\beta = \frac{d+3}{2}$:}\quad & K(\bx,\bt) = (1+\gamma \|\bx-\bt\|)\me^{-\gamma \|\bx-\bt\|}, \\
\text{$k = 2$, i.e., $\beta = \frac{d+5}{2}$:}\quad & K(\bx,\bt) = \left(1+\gamma \|\bx-\bt\|+\frac{1}{3}(\gamma \|\bx-\bt\|)^2\right)\me^{-\gamma \|\bx-\bt\|}.
\end{align*}
The integral equation $2\int_0^\infty \me^{-\gamma|x-t|}\varphi(t)\me^{-2t} \dif t = \lambda \varphi(x)$ was solved by \citet{Juncosa45} in the context of random noise in radio receivers. This can be interpreted as the Hilbert--Schmidt integral eigenvalue problem for the $k=0$ Mat\'ern kernel on $[0,\infty)$ with $\rho(x) = 2\me^{-2|x|}$. We plan to generalize the work of \citet{Juncosa45} in two steps. First, we plan to obtain the eigenvalues and eigenfunctions of the $k=0$ Mat\'ern kernel on the entire real line. Next, we will tackle Mat\'ern kernels with higher smoothness on $\reals$ by solving the corresponding ODE boundary value problems of increasingly higher order. While a generalization of this approach to higher dimensions seems unlikely, it will be possible to consider product kernels as a new type of designer kernel (see Section \refprobac).

\subsection*{Research Project 1.2. Numerical Computation of Eigensystems} \label{NumerEigensubsec}

The standard approach to approximating the eigenfunctions of the Hilbert--Schmidt operator $\cK$ uses a Nystr\"om method (see, e.g., \citep{Atkinson97,BachJordan03}). We propose the use of a more general collocation method.
Assuming that the eigenfunctions can be approximated as
$\varphi(\bx) \approx \sum_{j=1}^N h_j(\bx) d_j$, where $\{h_1,h_2,\ldots,h_N\}$, is a basis of a finite-dimensional approximation space, one can use $M$ collocation points $\bx_1,\ldots,\bx_M$ to discretize the Hilbert--Schmidt integral eigenvalue problem resulting in a rectangular generalized eigenvalue problem with $M\times N$ matrices $\mA$ and $\mH$
\begin{equation}\label{GeneralizedEVProblem}
    \mA\bd = \lambda\mH\bd, \quad \mA_{ij}=\int_\Omega K(\bx_i,\bt) h_j(\bt)\d\bt, \quad \mH_{ij}=h_j(\bx_i).
\end{equation}
Depending on the choice of basis and kernel, it may be possible to evaluate these integrals analytically or not. In the latter case we will use the techniques of Section \refprobb. Since the solution of \eqref{GeneralizedEVProblem} in its full generality is not yet a standard numerical linear algebra problem---although some work exists \citep{DasNeumaier13}---we will also consider the simpler case when $M=N$, i.e., the number of collocation points equals the number of basis functions. The benefit of using a linear combinations of the basis functions $h_1,\ldots,h_N$ which approximate the eigenfunctions instead of the basis functions themselves is that the Hilbert--Schmidt SVD ensures that we are using a data-dependent basis as demanded for higher dimensions by the Haar--Mairhuber--Curtis theorem \citep{Fas07a}. The Hilbert--Schmidt SVD provides a new and stable basis $\{\psi_1,\ldots,\psi_N\}$ collected in the vector
\[
\bpsi(\cdot)^T = \bphi(\cdot)^T\left(\begin{matrix}\mI_N\\\mLambda_2\mPhi_2^T\mPhi_1^{-T}\mLambda_1^{-1}\end{matrix}\right)
\]
as an alternative to the standard basis $\{K(\cdot,\bx_1),\ldots,K(\cdot,\bx_N)\}$ collected in the vector $\bk(\cdot)^T = \bpsi(\cdot)^T\mLambda_1\mPhi_1^T$ (see \eqref{StableBasis}), where $\bphi(\cdot)^T$ is a vector of eigenfunctions. Using the numerical eigenfunctions $\bphi(\cdot)^T\approx\bhh(\cdot)^T\mD$ with the coefficients determined via their corresponding eigenvalue problems \eqref{GeneralizedEVProblem} one then has the data-dependent approximate stable basis as a ``corrected'' version of the $h$-basis:
\[
\bpsi(\cdot)^T \approx \bhh(\cdot)^T\left(\begin{matrix}\mI_N\\\mD\mLambda_2\mPhi_2^T\mPhi_1^{-T}\mLambda_1^{-1}\mD^{-1}\end{matrix}\right)\mD.
\]

\subsection*{Research Project 1.3. Designer Kernels}\label{SectDesignerKernels}

It is possible to build a positive definite kernel via Mercer's theorem by combining an appropriate sequence of
``eigenvalues'' $\lambda_n$ with a given set of orthogonal functions. Thus, the closed form of the kernel may or may not be known in this case. For example, one could start with Chebyshev polynomials on $[-1,1]$ and combine them with either a sequence of algebraically or geometrically decaying eigenvalues. This will result in Mercer kernels that provide either algebraic of spectral approximation rates and can be implemented stably and reliably within the Hilbert--Schmidt SVD framework. The iterated Brownian bridge kernels of \cite{CavorettoEtAl14} provide another example of a family of designer kernels where the sinusoidal eigenfunctions (see \eqref{IBBkernel}) were chosen to satisfy boundary conditions generalizing those of natural cubic splines. Kernels with other (e.g., periodic) boundary conditions can be constructed using similar ideas.

\section*{Research Project 2. Extensions of Guaranteed, Adaptive Algorithms}\label{SectGAIL}

In our previous NSF-funded work (see Sect.\ \ref{previousmeritsubsec}) we derived a number of  adaptive algorithms for function approximation and integration, that are \emph{guaranteed} to satisfy the user-specified error tolerance.  This sets them apart from existing adaptive algorithms, which have no such guarantees.  For example, MATLAB's adaptive quadrature algorithm, \texttt{integral}, has no theoretical guarantee of success.  MATLAB has no adaptive (quasi-)Monte Carlo cubature methods (guaranteed or not).  The Chebfun MATLAB toolbox \citep{TrefEtal14} solves a host of problems adaptively, but has no theoretical guarantees.  Our proposed new adaptive algorithms will build on our previous work and also provide guarantees of success, upper bounds on computational cost, and proof of asymptotic optimality.

\subsection*{Research Project 2.1. Adaptive (Quasi-) Monte Carlo Algorithms for Multivariate Integration}
Multivariate integrals may be approximated by an average of function values:
\begin{equation} \label{integprob}
I(f) := \int_{\reals^d} f(\bx)  \, \varrho(\bx) \, \dif \bx \approx
\frac 1 N \sum_{i=1}^N f(\bx_i) =: \hI_N(f).
\end{equation}
Here $\varrho$ is a probability density function, and the well-chosen data sites come from an infinite sequence $\desinf$.  Two popular choices are i) independent and identically distributed (IID) points with density $\varrho$---corresponding to simple Monte Carlo sampling---and ii) suitably transformed low discrepancy sequences, such as digital sequences or integration lattice sequences---corresponding to quasi-Monte Carlo sampling \citep{Nie92,SloJoe94,Lem09a,DicPil10a,DicEtal14a,Owe13a}.

In our previous NSF-funded project we developed a simple Monte Carlo algorithm \citep{HicEtal14b} and two quasi-Monte Carlo algorithms \citep{HicJim16a,JimHic16a}, that \emph{adaptively} choose the sample size, $N$, to satisfy the user-specified error tolerance:
\begin{equation} \label{cubMCguar}
\bigabs{I(f) -I_N(f)} \le \abstol \quad \text{or} \quad \Prob\bigl[\bigabs{I(f) -I_N(f)} \le \abstol \bigr] \le 1-\alpha,
\end{equation}
the former holding for deterministic quasi-Monte Carlo sampling, and the latter holding for the random simple Monte Carlo sampling and confidence level $1-\alpha$.  All three adaptive algorithms base their decisions on data-driven error estimates, rather than a priori knowledge of the integrands.

We will extend these algorithms to handle a hybrid absolute/relative error tolerance of the form $\max(\abstol,\reltol \abs{I(f)})$, which is more flexible than our present pure absolute error tolerance, $\abstol$.  Since we already know how to estimate the error from the data, this problem is partially solved.  The missing piece is how to estimate $I(f)$ appearing in the error tolerance, $\max(\abstol,\reltol \abs{I(f)})$.  This will require an unspecified number of iterations.  For the simple Monte Carlo algorithm we must ensure that all these iterations combined stay within the specified confidence level.

We also propose to add control variates to our adaptive (quasi-)Monte Carlo algorithms.  Given an integrand $g$ whose integral, $I(g)$, is known, the control variate estimator is $\hI_{\CV,N}=\hI_N(f) + \beta [ I(g) - \hI_N(g)]$.  For simple Monte Carlo sampling there is a standard way to estimate the optimal $\beta$, but this method may give the wrong answer for low discrepancy sampling \citep{HicEtal03}.  We will follow this insight and find a better way of determining $\beta$ for low discrepancy sampling.

Option pricing is an important application of multivariate integration, and the case where the stock price is monitored continuously corresponds to the  limiting case of $d\to \infty$.  Multi-level methods \citep{Hei01a,Gil14a} have been found to be successful in efficiently evaluating infinite dimensional integrals.  We will extend our existing adaptive (quasi-)Monte Carlo algorithms to become adaptive multi-level methods.  The unbiased approach of \cite{RheGly12a} looks very promising.

Besides establishing rigorous guarantees for our new algorithms, we also will establish upper bounds on the computational cost of these algorithms.  As in our previous work, the cost depends not only on the known input parameters, such as the error tolerance(s) and the uncertainty, $\alpha$, but also on unknown quantities, such as the variance of $f(\bX)$, $\bX \sim \varrho$ (for simple Monte Carlo) and the rate of decay of the series coefficients for $f$ (for quasi-Monte Carlo).  This is natural since adaptive algorithms by nature expend different amounts of effort depending on the difficulty of the problem.  We also propose to show that these computational costs are asymptotically optimal relative to the best possible algorithms that make the same assumptions on the set of possible integrands.  This will be done by using bump functions, a well-known method in the information-based complexity literature (see \cite{TraWasWoz88,Nov88}).

\subsection*{Research Project 2.2. Higher Order and Locally Adaptive Univariate Function Approximation and Integration}
In our previous NSF-funded project (see Sect.\ \ref{previousmeritsubsec}) we developed adaptive univariate function approximation and integration algorithms based on linear splines, but \emph{with rigorous guarantees of success} \citep{HicEtal14b}.  In our proposed work we will extend these methods in two directions.  One obvious goal is to increase the efficiency by using higher order function approximation than the linear spline. The general framework provided in \cite{HicEtal14b} will facilitate this.

All of our adaptive algorithms for univariate or multivariate problems developed so far have been globally adaptive, i.e., the design, $\desinf$, is fixed but the number of data, $N$, is determined adaptively.  This is called global adaption.  A second goal of this project  is to introduce local adaption, i.e., algorithms where the density of the data sites, as well as their number, depends on the function data.  This is easiest to do for univariate problems.  We will sub-divide the whole domain into sub-domains and solve function approximation or integration problem separately on those.  These subdivisions will be data driven.

Finally, as was done for the original algorithms in\citep{HicEtal14b}, we will derive upper bounds on the costs of our new higher order and locally adaptive algorithms.  We will also demonstrate their asymptotic optimality by using bump functions \cite{TraWasWoz88,Nov88}.

\subsection*{Why Cones} The adaptive algorithms that we have derived in our previous work, and the adaptive algorithms that we propose to derive are or will be \emph{guaranteed} to satisfy the user-specified error tolerance.  This sets them apart from existing adaptive algorithms, which have no such guarantees.  For example, MATLAB's adaptive quadrature algorithm, \texttt{integral}, has no theoretical guarantee of success.  MATLAB has no adaptive (quasi-)Monte Carlo cubature methods (guaranteed or not), and MATLAB's spline functions for univariate function approximation are not adaptive either.

To obtain rigorous guarantees of success for adaptive algorithms requires limiting the functions to some set, $\calc$.  Our past experience and future expectation is that $\calc$ turns out to be a \emph{cone} of functions. There is a good reason for this.

The true absolute error of function approximation or integration, $\err(f,N):=\bignorm[\infty]{f-\tf}$ or $\err(f,N):= \bigabs{I(f) - \hI_N(f)}$, is positively homogeneous, i.e., $\err(cf,N)=\abs{c} \err(f,N)$. So we expect any reasonable data-driven error bound, $\oerr(f,N)$, to also be positively homogeneous.  This means that the set of functions for which the error bound is correct, $\calc:=\{f : \err(f,N) \le \oerr(f,N)\}$, must be a cone, i.e., $f\in \calc \implies cf \in \calc$.

\section*{Research Project 3. Combining Kernel Methods with Guaranteed and Adpative Algorithms} \label{combinesec}

\subsection*{Research Project 3.1. Data-Driven Error Estimation}\label{errestsubsec} Our goal is adaptive meshfree function approximation algorithms.  This requires an error bound, $\oerr(f,N)$, based on the function data, that is no smaller than $\err(f,N):=\bignorm[\infty]{f-\tf}$. Cross-validation (CV) is sometimes used to approximate the error. One form of CV is
\begin{subequations} \label{rbferrbd}
\begin{align} \label{crossvalid}
\err(f,N) &\approx \sqrt{\frac{C}{N} \sum_{i=1}^{N/2} \abs{f(\bx_{2i-1})-\tf(\bx_{2i-1};\{\bx_{2i}\}_{i=1}^{N/2})}^2 + \sum_{i=1}^{N/2} \abs{f(\bx_{2i})-\tf(\bx_{2i};\{\bx_{2i-1}\}_{i=1}^{N/2})}^2}.
\intertext{There also exist rigorous error bounds of the form \citep{Wen05a}}
\label{powfunerrbd}
\err(f,N) & \le \sqrt{\sup_{\bx \in \cx}[K(\bx,\bx)-\bk^T(\bx)\mK^{-1} \bk(\bx)]} \norm[\ch]{f} \\
\label{filldiserrbd}
\err(f,N) & \le C [\filldis(\desn)]^r \norm[\ch]{f}, \qquad 
\filldis(\desn): = \max_{\bx \in \cx} \min_{i=1,  \ldots, N} \norm[2]{\bx - \bx_i}. 
\end{align}
\end{subequations}
All of these expressions have drawbacks as candidates for a data-driven $\oerr(f,N)$.  CV has no finite-sample size guarantees of successfully providing an upper bound on the true error.  The error bounds in \eqref{numanalerrbd}

Here $C$ is some constant, and $\filldis(\{\bx_i\}_{i=1}^N)$ is the fill or maximin distance of the data sites.  Moreover, $\norm[\ch]{f}$ is the Hilbert space norm of $f$, where the reproducing kernel for $\ch$ is the $K$ used in \eqref{rbfapprox}. Finally, $r$ reflects the smoothness of $K$.  Error bound \eqref{rbferrbd} tells us that the error depends on both the roughness (Hilbert space norm) of $f$ and the unevenness (fill distance) of the data sites.

However, the error of the meshfree approximation cannot be bounded in terms of the data, $(\bx_1, y_1), \ldots, (\bx_N,y_N)$, because $\norm[\ch]{f}$ is unknown a priori.  The user does not know how large the error is in practice and whether $N$ needs to be increased to meet a desired tolerance. Thus, \eqref{rbferrbd} by itself does not lead to a guaranteed, adaptive algorithm.

Sect.\ \ref{SectCones} introduces the idea of \emph{cones} of input functions, where a stronger norm of $f$ is bounded above in terms of a weaker norm of $f$.  We have already successfully used this idea to construct guaranteed, adaptive, automatic algorithms for integration and for univariate function approximation.  Moreover, we have derived known upper bounds on their computational costs. \emph{We aim to apply the cones idea to multivariate function approximation using meshfree methods.}

The Hilbert--Schmidt series of the iterated Brownian bridge kernels \citep{CavorettoEtAl14} is given by
\begin{equation}\label{IBBkernel}
K(x,t) = \sum_{n=1}^{\infty} \left(n^2\pi^2 + \gamma^2\right)^{-\beta} 2\sin(n\pi x) \sin(n\pi t), \quad x,t \in [0,1],\ \gamma>0, \beta \in \mathbb{N},
\end{equation}
and therefore it is a perfect candidate for the data-dependent error bounds discussed above since the eigenfunctions do not depend on the parameters $\beta$ and $\gamma$, while a smaller smoothness parameter $\beta$ leads to slower decay of the eigenvalues.

\subsection*{Research Project 3.2. Data-Driven Parameter Estimation} \label{parestsubsec}
The parameters of the kernel may be estimated by maximum likelihood estimation (MLE).  Another serious practical problem with meshfree approximation is that the matrix $\mK$ in \eqref{rbfcoef} can easily become \emph{ill-conditioned for moderate $N$}---especially if the kernel $K$ contains shape parameters, $\gamma_\ell>0$, such as the anisotropic Gaussian kernel in \eqref{gausskernel}.  The ill-conditioning will arise if the kernel has a high degree of smoothness ($r$ is large), and/or if it becomes ``flat'' (the $\gamma_\ell$ are near zero). This means that $f$ is very smooth and the discretization error \eqref{rbferrbd} vanishes rapidly as the fill distance decreases.
Traditionally, this has led to a widely accepted trade-off or uncertainty principle, according to which one must find values of the shape parameters that optimize the balance between (decreased) stability and (increased) discretization error.  This is explained in Sect.\ \ref{SectMisconcept}.

Moreover, the CV and MLE computations often used to optimize the parameters in the kernel also depend on working with the possibly ill-conditioned $\mK$.  This can make CV and MLE unreliable.

We intend to use equivalent, but ``better'' basis representations for kernel-based approximation methods of the form \eqref{rbfform} that lead to \emph{reliable and numerically stable algorithms} and to new and more dependable ways for estimating the shape parameters that provide optimal accuracy. We note here that such non-zero optima have been shown to exist in many cases (see, e.g., Fig.~\ref{Fig_HSSVD})---due to reasons other than the trade-off principle such as Runge-like boundary effects. In particular, we plan to develop practical, reliable and automatic algorithms that take advantage of the additional flexibility provided by the kernel parameters, instead of viewing them as a nuisance.

As Fig.~\ref{Fig_HSSVD} shows, for any given set of data, finding an ``optimal'' (or near-optimal) value of the shape parameter(s) can have a significant impact on the accuracy of the recovered function $\tf$. A number of potential criteria for optimizing the shape parameters were introduced by \cite{Fasshauer11}. However, up to now all of those criteria have been based on the use of the standard approach, i.e., they were subject to the effects of ill-conditioning of the $\mK$ matrix, and thus often unreliable or impractical. Recently, the MLE criterion (or more precisely, negative concentrated log-likelihood) was the first of these criteria to be converted to the stable framework. A plot demonstrating the typical effect of this conversion is given in the right part of Fig.~\ref{Fig_HSSVD}. \emph{We propose to extend the application of the Hilbert-Schmidt SVD to other criteria, and then to investigate which of these criteria is most effective.}

This latter investigation necessitates a framework in which such a comparison can take place. We plan to answer questions regarding the \emph{consistency}, \emph{rate of convergence} and \emph{stability} of various parameter estimation schemes. For consistency we ask whether a scheme is able to recover the ``true'' value of $\gamma$ as we sample more and more densely. The rate of convergence of a scheme determines how fast the ``optimal'' value suggested by the method approaches the ``true'' value as the sampling density increases, and stability of a scheme will be judged by the effects small changes in the data have on the consistency and convergence rate of the scheme. \emph{We intend to investigate and introduce a metric for comparing the quality of different parameter estimation criteria, and we also plan to derive other parameter optimization criteria, including for the low-rank version of the Hilbert-Schmidt SVD mentioned in the next section.}

\subsection*{Research Project 3.3. Data-Driven Designs} \label{designsubsec}
An important issue is not only the surface fitting problem itself, but also the choice of \emph{design}, i.e., the data locations or input parameter settings. The main objectives for finding a ``good'' design are that it successfully deal with the \emph{curse of dimensionality}, i.e., that it effectively fill out the high-dimensional design space, and that it ensure an accurate solution of the scattered data fitting problem. In many realistic surrogate modeling applications we are free to decide where and how often we want to sample the computer experiment to generate the data required for our surrogate model. A popular design in the statistics literature looks at so-called \emph{$D$-optimality}, i.e., it aims to maximize the determinant of the Gram matrix $\mK^T\mK$ (see, e.g., \citep{FangEtAl06, MorrisEtAl93}). In approximation theory, so-called \emph{Fekete points} are analogous to $D$-optimal designs, i.e., they maximize the determinant of the interpolation matrix $\mK$ (see, e.g., \citep{BrianiEtAl12, DeMarchi03}). Since Fekete points also approximately minimize the Lebesgue constant
\begin{equation}\label{LebesgueConst}
\Lambda_{K,\desn} = \max_{\bx \in \Omega} \sum_{j=1}^N |\starred{w}_j(\bx)|,
\end{equation}
another effect of such a design is a near-optimal approximation error $\| \tf - f \| \le \left( 1 + \Lambda_{K,\desn} \right) \| \starred{f} - f \|$,
where $\starred{f}$ is the $L_\infty$-best approximation to $f$ from the finite-dimensional space $\ch_K(\desn)$.
The value at $\bx$ of the cardinal function $\starred{w}_j$ from \eqref{LebesgueConst} can be computed via a ratio of determinants
\[
\starred{w}_j(\bx) = \frac{\det_{K,\desn}(\bx_1,\ldots,\bx_{j-1},\bx,\bx_{j+1},\ldots,\bx_N)}{\det_{K,\desn}(\bx_1,\ldots,\bx_N)},
\]
where $\det_{K,\desn}(\bt_1,\ldots,\bt_N)=\det\left(K(\bt_i,\bx_j)\right)_{i,j=1}^N$.
Without the use of the Hilbert--Schmidt SVD, evaluation of these determinants is subject to severe numerical instabilities and therefore has been avoided in practice \citep{DeMarchi03}. While the Hilbert--Schmidt SVD directly applies to the determinant in the denominator, we will need to develop an extension in order to be able to handle the numerator-determinant. Once this has been accomplished, we find a $D$-optimal design by stably solving a multivariate maximization problem.

\section*{Research Project 4. Applications of Kernels}
\label{appsec}
\subsection*{Research Project 4.1. Numerical Solution of PDEs via Kernel ENO Methods}  \label{PDEsubsec}
The use of kernel methods for the solution of hyperbolic PDEs via a (weighted) essentially non-oscillatory (W)ENO) approach is not new \citep{CecilEtAl04,IskeSonar96}. However, previous attempts have suffered either from numerical instabilities \citep{CecilEtAl04} or from the limitations of using a low-order method \citep{IskeSonar96}. In \citep{McCourt13} the Hilbert--Schmidt SVD was used within the context of kernel collocation methods such as Kansa's approach \citep{Fas07a} yielding solutions that were vastly superior in accuracy to the standard (ill-conditioned) kernel approach, and also to polynomial spectral methods (see Fig.~\ref{Fig_MFS_BEM}). We expect similar benefits of using the Hilbert--Schmidt SVD within the (W)ENO framework.

\subsection*{Research Project 4.2. Using Kernels to Solve the MEG and EEG Forward Problems} \label{SectMEEG}
There are plenty of other related questions we might tackle if time allows or the opportunity arises. However, we would like to highlight one ongoing collaboration in which kernels are used to solve a system of coupled Poisson and Laplace PDE boundary value problems that arise in MEG and EEG modeling of brain activity. Fig.~\ref{Fig_MFS_BEM} is taken from \citep{AFFGM13} and provides a cost-per-accuracy plot comparing a kernel-based solver using a method of fundamental solutions approach with a boundary element method that is considered state-of-the-art in the community \citep{fieldtrip11}. The coupled system is solved on three nested concentric spherical shells and describes the forward problem corresponding to the electric potential on the outside surface resulting from a single dipole located inside a simplified brain-skull-scalp geometry. The three different low-rank MFS solutions are all more accurate and more efficient than the BEM solution. For any desired accuracy, the MFS solution is more efficient, and this effect is more pronounced as we demand more accuracy.

\begin{figure}[h]
    \centering
    \includegraphics[width=.35\linewidth]{HSSVD_BVP}
    \includegraphics[width=.35\linewidth]{Fig_MFS_BEM}
\caption{Left: Comparison between use of the Hilbert--Schmidt SVD (GaussQR), standard kernel approach (Fasshauer) and Chebyshev polynomial spectral method (Trefethen) to solve a boundary value problem (from \citep{McCourt13}). Right: Cost per accuracy plot of several MFS solutions vs. BEM solution for coupled PDE system.}\label{Fig_MFS_BEM}
\end{figure}

As the next steps on this project we expect (i) \emph{to integrate the forward solver into a full inverse problem framework} which will allow us to identify the locations of activity inside the brain from measurements of the electric potential (instead of simulating the electric surface potential based on a given dipole source as done for the forward problem in Fig.~\ref{Fig_MFS_BEM}), and (ii) \emph{to move to a head model that reflects the geometry of the brain more realistically}.

\subsection*{Research Project 4.3. Low-rank Approximations for Kernel Classification}\label{Sec_TruncHS}

One way to make support vector machines (SVMs) perform more efficiently is to consider a low-rank representation for the kernel as in \citep{FineScheinberg02}. We anticipate that we can use the eigenfunction expansion of $K$ to decrease the cost involved in the quadratic program required for the SVM solution. The standard SVM quadratic program can be written in matrix form as
\begin{align}
    \min_{\balpha} \;&
        \frac{1}{2}\balpha^T\mD_{\by}\mK\mD_{\by}\balpha - \be^T\balpha \\
    \text{subject to } & \by^T\balpha = 0, \nonumber\\
    & \balpha\in[0,C]^N, \nonumber
\end{align}
where $\mD_{\by}$ is a diagonal matrix with $\by$ on the diagonal,
and $\be$ is a vector of all ones.  If we can state
$\mK\approx(\mLambda^{1/2}\mPhi)^T(\mLambda^{1/2}\mPhi)$, then this
problem can be rephrased \citep{FineScheinberg02} as
\begin{align*}
    \min_{\bpi,\balpha} \;&
        \frac{1}{2}\begin{pmatrix}\bpi^T&\balpha^T\end{pmatrix}
        \begin{pmatrix}\mI_M&0\\0&0\end{pmatrix}
        \begin{pmatrix}\bpi\\\balpha\end{pmatrix}
        - \begin{pmatrix}0&\be^T\end{pmatrix}\begin{pmatrix}\bpi\\\balpha\end{pmatrix} \\
    \text{subject to } &
        \begin{pmatrix}0&\by^T\\-\mI_M&\mLambda^{1/2}\mPhi^T\mD_{\by}\end{pmatrix}
        \begin{pmatrix}\bpi\\\balpha\end{pmatrix} = 0, \nonumber\\
    & \balpha\in[0,C]^N,\quad \bpi\in\reals^M. \nonumber
\end{align*}
Although
this system is of size $N+M$ (and the original system was only size $N$), the
cost of solving this system may be much lower because of the extremely simple
structure of the Hessian.  This sparsity, in comparison to $\mH$ which may be
fully dense, allows for cheap matrix-vector products and decompositions, both
of which may all for a faster quadratic program solve.  Note that the $\bpi$
values are inconsequential in making predictions with the SVM.


In order to make the Hilbert-Schmidt SVD practical and ensure that it can be used in conjunction with the error guarantees of Sect.~\ref{SectCones}, a theoretical basis for the truncation of the Hilbert-Schmidt series needs to be established. \cite{FMcC12} introduced two different scenarios: (i) truncation at $M\ge N$ terms so that every entry $K(\bx_i,\bx_j)$ of $\mK$ is represented to within a tolerance $\varepsilon$, (ii) truncation at $M < N$. The former approach results in data-dependent basis functions generated by the $N$ first eigenfunctions of $\cT_K$ corrected by linear combinations of the next $M-N$ eigenfunctions (cf.~\eqref{StableBasis}), while the low-rank approach (ii) uses only the first $M$ (data-independent) eigenfunctions as a basis. The method used by \cite{FMcC12} to determine the truncation length $M$ is similar to that used earlier by Fornberg and co-workers in their RBF-QR approach \citep{FornbergPiret08, FornbergFlyerLarsson11}.  We discussed a more straightforward strategy to determine $M$ in \cite{CavorettoEtAl14}. However, neither of these methods provide any direct insight on the approximation error. Work in that general direction can be found in a recent preprint \citep{GriebelRiegerZwicknagl13}. Based on this, \emph{we plan to develop improved criteria for the truncation of the Hilbert-Schmidt series.}

\subsection*{Research Project 4.4. Using Kernels to Create a Surrogate Model to Predict the Spread of Ebola} \label{ebolasubsec}
Right now, Loren Cobb, Jan Mandel and myself (members of the math department at University of Colorado Denver) are submitting an NSF Rapid grant soon (next 2 weeks) tentatively titled "Modeling the Ebola Epidemic with Discrete Time Filters".  This project uses the DEXES II model, modified to fit this particular epidemic and with some additional content, within a data assimilation framework to predict the state of the Ebola epidemic in West Africa some months into the future.  A significant component of this work involves formalizing the mathematics of general discrete time filters, specifically the assimilation of data we assume to be Poisson distributed, and implementing such a Poisson filter efficiently in software.  We also define key quantities of interest we plan to study which are random variables with densities related to the posteriors produced by the discrete time filter; possible quantities of interest include: the probability of any cases developing in Mali, the probability of a multimodal evolution within a region (after an initial peak is hit, can the disease regain momentum), or the total number of cases outside of presently infected regions.

After the modeling and predictive components of this project are completed, we plan to study the effect of various parameters input to the DEXES II model on the evolution of the Ebola crisis.  Such parameters may include: border permeability (the probability that someone can enter a country without screening), the development of quarantines within a region, and the availability of clean water and waste disposal facilities.  If we think of QoI as a quantity of interest computed at the end of a predictive session, we may write QoI (permeability, quarantine, clean water) to denote the dependence of that quantity of interest on a variety of model parameters.  As a result, studying the impact of these parameters on this Quantity of Interest is a problem of surrogate modeling in potentially high dimensions (though we plan to start at 1, of course).  Furthermore, as the number of dimensions we want to simultaneously consider increases, the experimental design for generating this surrogate model becomes increasingly relevant.  Because both of these topics have been successfully researched at IIT, I am inclined to believe that I would be able to use data from this project in research at IIT, and use ideas from IIT to help this Ebola study.

\section{Results of Previous NSF-Funded Research}\label{SectPrevious}

Greg Fasshauer (GEF) and Fred Hickernell (FJH) are the PIs for \emph{NSF-DMS-1115392 Kernel Methods for Numerical Computation}, July 1, 2011 -- June 30, 2014, \$320,000.  Since Fall 2005, we have organized a weekly Meshfree Methods research seminar that draws regular participation from all of our local collaborators, including faculty members, visitors, and students.  All participants take turns posing interesting research problems, reporting work-in-progress, and presenting relevant work of others.  The atmosphere is informal; interruption and discussion is encouraged.  Occasionally we have speakers from outside applied mathematics and/or IIT. Regular participants for a month or more during the timeframe of this project include the following (``$\fF$'' = female,  ``$\fA$'' = African-American, ``$\fL$'' = Latina/Latino).

\begin{itemize}[leftmargin=2.5ex]
\item IIT Full-Time Faculty: GEF, FJH, Igor Cialenco (IC), John Erickson (JE, $\fA$), and Lulu Kang (LK, $\fF$);
\item Long-Term Visitors: Roberto Cavoretto (RC), University of Torino; Sou-Cheng Choi (SCC, $\fF$), Research Assistant Professor, IIT, National Opinion Research Center, University of Chicago; Salvatore Ganci (SG), University of Palermo; YungWook Jung, Gyeonggi College of Science and Technology; Junbin Li, Dalian University of Technology; Yiuwei Liu (YiL), Lanzhou University; Michael McCourt (MM), Cornell University, Argonne National Laboratory, University of Colorado, Denver; Jinming Wu, Zhejiang Gongshang University;
\item IIT Graduate Students: Aleks Borkovskiy (AB), Siyuan Deng (SD), Yuhan Ding (YD, $\fF$), Llu\'is Antoni Jim\'enez Rugama (LAJR), Lan Jiang (LJ, $\fF$), Yao Li (YaL), Yiou Li (YL, $\fF$), Jagadeeswaran Rathinavel (JR), Tiago Silva (TS), Xin Tong (XT, $\fF$), Qi Ye (QY), Xiaodong Zhang (XiZ), Yizhi Zhang (YZ), Xuan Zhou (XZ);
\item IIT Undergraduate Students: Haocheng Bian (HB), Nick Clancy (NC), Martin Dillon (MD), Caleb Hamilton (CH), Joseph Kupiec, Barrett Leslie (BL), Timothy McCollam (TM), Martha Razo (MR, $\fL$, $\fF$);
\item Other Undergraduate Students: Casey Bylund (CB, $\fF$), University of San Francisco; Matthew Gliebe (MG), Northwestern University; Palmer Lao (PL), Clarkson University; William Mayner (WM), Brown University;
\item High School Student: Sunny Yun (SY).
\end{itemize}

\subsection{Intellectual Merit from Previous NSF Funding}
\label{previousmeritsubsec}

With a collaborator we have investigated the convergence and tractability of meshfree methods, \eqref{rbfapprox}, using the Gaussian reproducing kernel with non-isotropic shape parameters, $\{\gamma_{\ell}\}_{\ell =1}^{\infty}$, \eqref{gausskernel}. Dimension \emph{independent} convergent rates for function approximation were demonstrated to exist, and they depend on the rate of decay of the $\gamma_{\ell}$ as $\ell \to \infty$, not on the smoothness of the reproducing kernel \citep{FasHicWoz12b, FasHicWoz12a}. These results were extended by XZ to a more general class of reproducing kernels with that have a sequence of scale parameters $\{\alpha_{\ell}\}_{\ell=1}^{\infty}$ as well as shape parameters.  In this case the dimension independent convergence rates depend on the rate of decay of the product $\alpha_{\ell} \gamma_{\ell}$ as $\ell \to \infty$ \citep{ZhoHic15a}.

Together with QY and other collaborators, we have shown how the optimal solution of problems posed on reproducing kernel Hilbert spaces can be extended to Banach spaces \citep{SonZhaHic12a, FasHicYe13a}.

Existing algorithms for multivariate or even infinite-dimensional approximation and integration problems require hand-tuning based on the parameters that define the spaces of functions to which they apply, e.g., the shape or scale parameters of the associated reproducing kernel.  In our search for adaptive algorithms that automatically select these parameters, we started by looking for adaptive algorithms for even simpler problems. The adaptve algorithms that we developed are described below.  They all come with rigorous guarantees and are published in our Guaranteed Automated Integration Library (GAIL) \citep{ChoEtal14a}.  This \Matlab library is publicly available and under ongoing development.

FJH, JL, YiL, and a collaborator developed an adaptive simple Monte Carlo algorithm for calculating means of random variables and multidimensional integrals \citep{HicEtal14a}.  This algorithm is based on Cantelli's inequality to guarantee our estimate of the variance and on a Berry-Esseen inequality (a finite sample Central Limit Theorem).  It is guaranteed to succeed for the cone of random variables or integrands whose kurtosis satisfies some known bound.  FJH and LAJR developed adaptive quasi-Monte Carlo cubature algorithms based on digital nets \citep{HicJim16a} and rank-1 integration lattices \citep{JimHic16a} by expressing the integrands as infinite series, $f=\sum_{\bk} \hatf_\bk \varphi_{\bk}$, where the $\varphi_{\bk}$ are Walsh functions or complex exponential functions.  The cubature error in terms of the discrete  approximations to the coefficients, $\hatf_\bk$.  These algorithms must succeed for a cone of integrands whose true series coefficients do not decay erratically.

FJH, NC, YD, CH, and YZ constructed adaptive algorithms for univariate integration and function approximation \citep{HicEtal14b}.  This paper also sets out a general framework for constructing guaranteed adaptive algorithms for any problem where the solution operator is homogeneous by considering cones if input functions.  FJH, MR, and SY are preparing a paper to explain these ideas to a broader audience.  XT completed her MS thesis \citep{Ton14a} on guaranteed adaptive optimization of univariate functions.


GEF and QY have formulated a theory of generalized Sobolev spaces for reproducing kernel Hilbert spaces that connects kernel-based approximation methods to fundamental solutions of differential operators \cite{FasshauerYe11, FasshauerYe13}. An important contribution is the fact that the definition of these generalized Sobolev spaces involves both a notion of smoothness and of scale, while traditional Sobolev spaces take into account only the smoothness of functions in the space.

As an application of the generalized Sobolev space theory, GEF, QY and IC have developed a framework for the numerical solution of stochastic partial differential equations with kernel-based methods \citep{CFY12,FasshauerYe13b,FasshauerYe14}.

GEF and MM have developed an RBF-QR algorithm for the stable computation with ``flat'' kernels as explained in Sect.~\ref{SectRBFQR}. This approach takes advantage of the Hilbert-Schmidt series of the kernels \eqref{HSseries}. The basic framework was reported in a paper on stable computations with Gaussian kernels \citep{FMcC12}, and further extensions to the solution of boundary value problems for PDEs \citep{McCourt13}, a fast recursive regression algorithm \citep{McCourt13b}, and coupled PDE problems \citep{McCF13} have been submitted for publication. A publicly available library of \Matlab code \citep{McCFBG13} is under ongoing development.

A new class of so-called \emph{compact Mat\'ern kernels} was investigated by RC, GEF and MM \citep{CavorettoEtAl14}. The paper discusses these kernels, their connection to piecewise polynomial splines and an implementation of the associated Hilbert-Schmidt SVD algorithm. Special cases of these kernels were studied by CB and WM during their 2012 summer REU under the guidance of GEF and MM. During summer 2013, GEF and MM worked with two other undergraduate REU students, HB and MG, on the numerical computation of kernel eigenvalues and eigenfunctions and on an application of the Hilbert-Schmidt SVD to the study of the Hilbert space norm of the kernel interpolant. The latter problem is important for the error estimation of kernel-based methods, and for the determination of optimal kernel parameters such as the shape parameter.

During SG's visit to IIT (08/2012-02/2013) he performed joint work with GEF and MM on the use of kernel-based methods for the solution of coupled boundary value problems for the application in EEG and MEG (see Sect.~\ref{SecMEEG}). This work made use of the coupling framework of \cite{McCF13}. A joint paper \citep{AFFGM13} has been submitted.

Understanding the covariance kernels, eigenfunctions and associated Hilbert spaces of certain stochastic processes, in particular fractional Brownian motion, was the focus of a research project during summer 2013 involving MD under the guidance of JE and GEF. Some outcomes of this research were presented at the 2014 Joint Mathematics Meeting.

\subsection{Broader Impacts from Previous NSF Funding}

We have trained a number of PhD, MS, BS, and high school students in research, as evidenced by the list above.  QY defended his PhD thesis in April 2012 and became a Philip T. Church postdoc in the Department of Mathematics at Syracuse University starting August 2012. He has four published papers, with an additional two accepted, one submitted, and two more in preparation.  YL defended her thesis in August 2014 and is a visiting assistant professor at DePaul University.  Of the other nine IIT PhD students listed, YD, YL, XZ, and YZ have completed their comprehensive exams and are expected to graduate by the end of 2015.  AB has passed his qualifying exams.  LAJR, JR, TS, and XiZ are in the earlier stages of their PhD studies. SD, YaL, TS, XT, and XiZ completed their MS theses. The latter three are now PhD students.  QY, YD, LJ, LAJR, YL, XZ, HB, MR, SY, NC, MD, CH, BL, CB, WM, and SY have all presented their work at academic meetings in talks.

We have been pleased to have two underrepresented minority scientists and several female scientists (undergraduate through to faculty members) in our research group.  To their credit, both LJ and YL have successfully pursued their PhDs while at the same time carrying the responsibility of marriage and motherhood.

GEF presented a week-long tutorial to students from many different engineering disciplines at the University of Palermo. As a result, SG, a PhD student from Palermo supported by Italian funds, visited IIT from 08/2012-02/2013 to receive guidance on his research from GEF and MM.

RC, a postdoctoral researcher from the University of Torino supported by Italian funds, visited IIT from 03/2012-05/2012 and also in April 2013. He received advanced training in the area of meshfree approximation and has since been able to secure a position as a research fellow at the University of Torino.

Every fall semester in even years GEF teaches the graduate course MATH 590: Meshfree Approximation Methods, which focuses on kernel based methods, and uses his monograph \citep{Fas07a} as a text. The new developments using eigenfunction expansions of positive definite kernels were integrated for the first time in fall 2012. GEF and MM have used the updated lecture notes of this course as a basis for a new book manuscript entitled ``Kernel-based Approximation Methods using \Matlab'', currently under contract with World Scientific Publishers.

Every fall semester FJH teaches the graduate course MATH 565: Monte Carlo Methods in Finance.  The new results on automatic (quasi-)Monte Carlo algorithms have been taught there since fall 2012, and the GAIL library is used by the students.

SCC and FJH offered an experimental graduate seminar course for seven students, MATH 573: Reliable Mathematical Software, during fall 2013. Our goal was to help computational mathematicians understand how to develop software that can be used by others.  Topics included guarantees for automatic algorithms, reproducible computational science research, efficient coding, thorough documentation, careful testing, convenient user interfaces with parameter parsing and validation, and software publication.  We intend to offer this course again to an expanded audience because the topics are not normally covered in standard mathematics or computer science courses.

Members of the research team have organized conferences. Together with Larry L.~Schumaker, GEF organized the NSF-supported 14th International Conference on Approximation Theory in San Antonio, TX, in April 2013. A special mentoring session for PhD students and postdocs was included in the program of this conference. QY was the main organizer for the SIAM Student Chapter Conference on Recent Advances in Computational Science and Statistics held at IIT Oct. 29--30, 2011 ({\tt http://www.math.iit.edu/$\sim$siam/workshop/}). BL was a co-organizer. This conference brought together graduate students and postdocs from various Chicago area universities and Argonne National Lab and featured three plenary lectures (by Jerry L. Bona, Charles K. Chui, and Wei B. Wu) along with 25 contributed talks and a poster session. Since then, other Chicago-Area SIAM chapters have joined the IIT chapter to organize this conference on a yearly basis.

The research team has attended conferences, presented talks at departmental colloquia, and given talks to general audiences, both inside the mathematical sciences and beyond. Here are the highlights of the PIs.

\begin{itemize}[leftmargin=2.5ex]
\item GEF gave minisymposium talks at the 2012, 2013, and 2014 SIAM Annual Meetings.  He also gave invited presentations at the workshop on Multivariate Approximation and Interpolation with Applications in Erice, Italy in Sept.~2013, at the Freeform Optics Incubator Meeting of the Optical Society of America, Washington, D.C., in Oct.~2011, and at the NSF-CBMS Conference on Radial Basis Functions: Mathematical Developments and Applications, at UMass-Dartmouth, in June 2011. He gave a talk to the Level Set Collective at the NSF-sponsored Institute of Pure and Applied Mathematics (IPAM) and an invited talk at the 2013 Midwest Numerical Analysis Day. GEF is also a steering committee member of the Midwest Numerical Analysis Conference series. In 2011, he gave a week-long workshop at the U of Palermo (Italy, engineering). GEF gave departmental colloquium/seminar talks at Chapman U (math \& computer science), U Chicago (statistics), Middle Tennessee State U (math), Northwestern U (industrial engineering \& management sciences), and U of Padua (Italy, math).  GEF attended the 2010-2012 Intel International Science and Engineering Fairs (2011-2012 as chair of the AMS Menger Prize committee), where he judged projects in the mathematical sciences and interviewed high school students about their research. These interviews can be viewed on the YouTube channel of the AMS. He has also given presentations to students at Neuqua Valley and Carl Sandburg high schools.

\item FJH gave a talk at a special session at the 2012 Joint Mathematics Meetings (JMM), a plenary lecture at the Tenth International Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing (MCQMC) in 2012, an invited talk at the 2013 Midwest Numerical Analysis Day, and an invited session talk at the Eleventh MCQMC in April, 2014, an invited session talk at the 2014 SIAM Annual Meeting, and an invited talk at an ICERM workshop in September, 2014.  He spoke at the 2014 JMM and will speak at the 2015 JMM. FJH gave departmental colloquium/seminar talks at Argonne National Laboratory (mathematics and computer science), U Chicago (statistics), DePaul U (mathematics), Georgia Tech (industrial and systems engineering), and Illinois Institute of Technology (computer science, mathematics).  FJH was invited to speak to STEM students at Oakton Community College and to students at Hinsdale Central High School.

\end{itemize}


\section{Broader Impact of Proposed Research}\label{SectBroad}


\subsection{Contributions to Training, Mentoring and Other Human Resource Developments}

The PIs, GEF and FJH, are dedicated to mentoring young researchers from high school through postdoctoral levels.  We are excited to see our mentees transition from curiosity about what scholarship entails to choosing and solving their own problems.  We are committed to our regular Meshfree Methods research seminar because we believe that we learn good research practices by seeing them modeled and we that find good research ideas from a variety of sources. Outside of the Meshfree Methods seminar we meet with members individually or in smaller groups to discuss details of their research projects.

\begin{description}[leftmargin=2.5ex]
\item[Providing Research Experiences for Undergraduates and High School Students]\ We \linebreak[4] strongly believe that students should be introduced to research before graduate school so that they can learn how to discover the unknown, something that is not taught well in a classroom. We request funds to support two summer REU students per year. Having advertised our REU opportunities for several years now has given us some visibility within the community and is prompting inquiries from prospective participants well before we even announce our latest program offerings. As in the past two years, we expect the NSF funds will serve as a catalyst for funds to support additional summer students. In choosing REU students we make a deliberate effort to build a diverse research environment by targeting female and underrepresented minority students as well as students from less research-focused institutions (see Sect.~\ref{SectPrevious}). We will also continue to receive well-prepared high school students to join our research group as we did last summer.

\item[Preparing Students for Academic Careers] We consider mentoring to be a multi-faceted and potentially long-term process continuing even after the mentee has moved on from IIT.  For example, MM was an undergraduate student at IIT who collaborated with GEF during his PhD studies at Cornell University.  We will continue to mentor him as senior personnel for this proposal. Similarly, although QY has left IIT, GEF serves as his designated mentor on his application for travel support to the 2014 ICM in Korea. We have provided and will continue to provide SCC, a post-doctoral scientist and another senior personnel for this project, teaching and mentoring experience by including her in our present NSF project.  We will continue to find opportunities for special mentoring activities for our students, like those GEF organized and that QY and YD were involved in during the recent NSF-supported 14th International Conference on Approximation Theory in San Antonio, TX.  We  will continue our collaboration with Argonne National Laboratory (ANL), which has led to short-term and long-term opportunities for our PhD students and graduates.

\item[Preparing Students for Industry Careers]
In addition to preparing students for the academic landscape, we also help current students land competitive jobs in the business world. The training we provide in the areas of algorithm development and coding tends to give our students the needed edge. For example, WM, who graduated from Brown University in May 2013 is currently working for an internet startup company.

\item[Supervising Visitors]
GEF has established contacts with several Italian universities attracting students and postdoctoral visitors to IIT for extended visits (see Sect.~\ref{SectPrevious}). A visit by a PhD student from the University of Padua is currently in the planning stages for the fall of 2014.  Having lived in Hong Kong for 19 years, FJH has contacts with Chinese scientists that have prompted several long-term visiting Chinese scholars and students to join our research.  These activities will continue.

\item[Giving Short Courses and Invited Lectures]
We will continue our active track record (see Sect.~\ref{SectPrevious}) of providing lectures to students at various stages in their careers, ranging from high school to graduate school. These encourage students to enter STEM and encourage STEM students to engage in research.

\end{description}

\subsection{Contributions to Resources in Research, Education and the Broader Society}

The research we propose straddles mathematics, statistics, computer science, and applications in engineering and related fields.  The two PIs have complementary strengths that facilitate this interdisciplinary research.  GEF has expertise in approximation theory, meshfree methods, and numerical partial differential equations, while FJH has expertise in (quasi-)Monte Carlo methods, kernel-based methods, information-based complexity theory, and experimental design. Our expertise provides both an obligation and an opportunity to interact with a number of diverse communities. We envision the following contributions:

\begin{description}[leftmargin=2.5ex]
\item[Disseminating Research]
The research supported by this grant will result in publications in peer-reviewed journals in a broad spectrum of applied mathematics, computer science, statistics and engineering. These journals will include both those that emphasize theory and those that emphasize applications.

\item[Promoting Cones] The idea of guaranteed, adaptive algorithms via cones of input functions has broad potential application.  We will be promoting this idea among numerical analysts who develop new algorithms and analyze their computational costs, as well as among information-based complexity theorists who analyze the lower bounds on the complexity of numerical problems.

\item[Promoting the Hilbert-Schmidt SVD] Similarly, we will encourage other researchers to take advantage of the stability given to kernel-based methods by the Hilbert-Schmidt SVD, to use our code, and to join our research efforts.

\item[Bridging Mathematics and Statistics]
This project touches on topics that are of interest to the statistics community: kriging, Monte Carlo methods, and design of experiments.  Historically, there has been relatively little interaction between numerical analysts and statisticians.  We have and will continue to engage the statistics community by speaking a their conferences and departmental seminars.  MM is currently delivering a series of lectures to the statistics group at CU-Denver, and GEF will join this effort during several visits during the spring 2014 semester.

\item[Collaborating with Engineers]
As a result of the visit by SG (see Sect.~\ref{SectPrevious}), GEF and MM have recently submitted a bilateral research proposal to the Italian government together with Guido Ala (Dept. Electrical Engineering, U Palermo) and Elisa Francomano (Dept. Chemical Engineering, Management, Computer Science and Mechanical Engineering, U Palermo) entitled ``Novel Numerical Methods and Embedded Systems for the Integration of Neuroimaging Techniques''. In that project kernel methods are being used for the solution of inverse problems arising in the detection of brain activity from MEG or EEG data (see Sect.~\ref{SecMEEG}). This fully non-invasive diagnostic procedure will enable doctors to detect early functional and neurophysiological markers of diseases (e.g., of Alzheimer's disease). It will also result in a potential reduction in doctors' visits, shorter hospitalization periods and a greater longevity with overall improved quality of life. The bilateral project proposal does not include any direct support for GEF and MM. In fact, it depends on travel funds obtained via the current proposal. Since kernel methods are becoming a rather popular numerical tool in science in engineering, other opportunities for collaborations outside mathematics frequently arise. For example, GEF participated in an incubator meeting on freeform surfaces organized by the Optical Society of America.

\item[Organizing and Presenting at Conferences]
We and our students involved in this project will present our results at a variety of conferences and workshops.  These include: (i) specialized meetings focusing on approximation theory, complexity, experimental design, meshfree methods, and Monte Carlo methods; (ii) the national meetings of AMS, SIAM, and the statistical societies; and (iii) conferences devoted to application areas.  We are frequently invited to speak at such conferences, which will give our results a prominent hearing. We will also continue to organize specialized conferences or minisymposia within larger conferences. For example, GEF and MM are planning to organize a minisymposium on \emph{Advances in Kernel Methods for Analysis and Statistics} at the 2014 SIAM Annual Meeting, and FJH and SCC are organizing a minisymposium on \emph{Reliable Computational Science} at the same meeting.

\item[Writing Textbooks and Survey Papers]
GEF and MM are currently under contract with \linebreak[4] World Scientific Publishers to prepare a monograph entitled \emph{Kernel-based Approximation Methods using \Matlab}. This book will provide an exposition of the theory and implementation of the Hilbert-Schmidt SVD along with numerous applications. The book will form a bridge to the GaussQR library currently under development and may serve as a textbook for a graduate class on meshfree methods, such as MATH 590 at IIT. GEF and FJH occasionally publish survey articles (e.g., a 42 page paper on kernel-based methods \citep{Fasshauer11}).

\item[Refreshing Course Syllabi]
MATH 590 (Meshfree Methods), taught by GEF in the fall semester of every even-numbered year, is likely to undergo a major change in the fall of 2014 as the preparation of the monograph mentioned above progresses. MATH 565 (Monte Carlo Methods in Finance), taught every fall by FJH, already includes our new results on guaranteed confidence intervals using IID sampling, and in the future will include our work in progress on guaranteed multi-level and quasi-Monte Carlo sampling (Sect.\ \ref{MC_g_sec}).

\item[Changing How Numerical Analysis Is Taught] Current texts teach students to estimate the error of the trapezoidal rule, $T_n(f)$, by $[T_n(f)-T_{n/2}(f)]/3$ (see, e.g., \cite[p.\ 223--224]{BurFai10}).  The arguments leading to this estimate introduce the valuable concept of extrapolation, however, this is a flawed error estimate for the same reasons that the error estimate in \Matlab's {\tt integral.m} is flawed (see Sect.\ \ref{drugssubsect}).  We will urge numerical analysis textbook authors and educators to change the way that error estimation is taught based on our recent and proposed work.  These ideas will also enter our more traditional numerical analysis courses such as MATH 350 (Introduction to Computational Mathematics) or MATH 577 and 578 (Computational Mathematics I \& II).

\item[Creating Software and Collaborating with Software Developers]
GEF and MM have created the website \citep{McCFBG13} which serves as the home for the software on our stable algorithms built upon the Hilbert-Schmidt SVD. This \Matlab library is freely available and allows others to experiment with our code. Thus far, the library contains routines for Gaussian kernels and for compact Mat\'ern kernels. As our research expands to other kernels and their eigenexpansions the resulting code will be added to the library. The GaussQR library also serves as a sandbox for students---especially REU students---to learn about our research and allows them to contribute pieces of their own work.

We will continue to develop GAIL \citep{ChoEtal14a} as part of our proposed research.  The GAIL software will serve the wider community that relies on numerical approximation and integration algorithms.  It will also demonstrate how automatic algorithms ought to be implemented, which we hope will inspire and inform those working on automatic, adaptive algorithms for other mathematical problems.  We also expect our new algorithms to be incorporated into widely used numerical packages, as was done for the algorithm in \cite{HonHic00a} by \Matlab \citep{MAT8.2} and NAG \citep{NAG23}.  We will continue to discuss with software developers about these issues.

\item[Reaching Out]
GEF and FJH both have a record of reaching out to high school students and we plan to continue. The website {\tt http://math.iit.edu/$\sim$openscholar/meshfree/} helps manage our internal and external communication and dissemination of research findings. Advertising for the summer REU experiences is also facilitated via this website.
\end{description}

\newpage
\clearpage
\pagenumbering{arabic}

\bibliographystyle{spbasic}

\renewcommand{\refname}{\hfill \textbf{\large References Cited} \hfill \hfill}                   %%
\renewcommand{\bibliofont}{\normalsize}

\bibliography{FJH22,FJHown23,GEF}
\end{document}
