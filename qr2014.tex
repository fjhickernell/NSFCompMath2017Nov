\section{Stable Computation via the Hilbert-Schmidt SVD}\label{SectHSSVD}

\subsection{The Uncertainty Principle---A Major Misconception}\label{SectMisconcept}
Before we discuss how to compute stably with ``flat'' kernels, let us first consider \emph{why} one might want to care about ``flat'' kernels in the first place. In fact, let's start by looking at what happens if we consider using ``flat'' kernels with the standard approach as outlined in the introduction. It has been a long-held belief among many practitioners that ``one can't simultaneously have high accuracy and stability when computing with radial basis functions'' (or other positive definite kernels).
This belief is
(i) based on the observations made by countless practitioners, and
(ii) assumed to rest on a rigorous mathematical foundation, the so-called \emph{uncertainty principle} due to \cite{Schaback95b, Schaback95c}.
Unfortunately, item (ii) is the cause of a major misconception. Taking into consideration the hypotheses for Schaback's work, one should more carefully paraphrase his uncertainty principle as
\[
\text{``\emph{Using the standard basis}, one can't simultaneously have high accuracy and stability.''}
\]
Therefore, use of a ``better'' basis may very well allow us to obtain results that are both stable \emph{and} accurate. This idea should not come as a surprise to anyone familiar with the use of B-splines instead of truncated power functions, or Chebyshev polynomials instead of monomials. The quest for such a ``better'' basis was the starting point for some of the research performed under NSF Grant DMS-1115392, and it is the fundamental idea behind the research proposed in this section.

\subsection{The Hilbert-Schmidt SVD---A Framework for Stable Computation}\label{SectRBFQR}
Every positive definite kernel $K$ has a Hilbert-Schmidt (or Mercer) series decomposition \citep{CourantHilbert53,RasWil06a}
\begin{equation}\label{HSseries}
K(\bx,\bt) = \sum_{n=1}^\infty \lambda_n \varphi_n(\bx) \varphi_n(\bt),
\end{equation}
where the $\lambda_n$ and $\varphi_n$ are the eigenvalues and eigenfunctions of the associated integral operator $\cT_K : L_2(\Omega, \rho) \to L_2(\Omega, \rho)$ defined by
\begin{equation}\label{TK}
(\cT_Kf)(\bx) = \int_\Omega K(\bx, \bt)f(\bt)\, \rho(\bt)\, \d \bt.
\end{equation}

\cite{FMcC12} and \cite{CFMcC13} combined this decomposition of the kernel $K$ with some basic block matrix manipulations to show that the kernel matrix $\mK$ from \eqref{rbfcoef} can be formally decomposed into its \emph{Hilbert-Schmidt SVD}
\[
\mK = \mPsi \mLambda_1 \mPhi_1^T,
\]
where all matrices are of size $N\times N$, but the matrix $\mPsi$ is formed as a product of semi-infinite and bi-infinite matrices, i.e.,
\begin{equation}\label{StableBasis}
\mPsi = \left(\mPhi_1\ \mPhi_2\right)\left(\begin{matrix}\mI_N\\\mLambda_2\mPhi_2^T\mPhi_1^{-T}\mLambda_1^{-1}\end{matrix}\right),
\quad
\left(\begin{matrix}\mLambda_1 & \\ & \mLambda_2\end{matrix}\right) = \text{diag}(\lambda_n)_{n=1}^\infty,\ \left(\mPhi_1\ \mPhi_2\right) = \mPhi = \left(\varphi_n(\bx_i)\right)_{i,n=1}^{N,\infty}.
\end{equation}
For practical applications the matrices $\mLambda_2$ and $\mPhi_2$ will therefore have to be truncated appropriately (see Sect.~\ref{Sec_TruncHS}). It is important to note that the matrices $\mPsi$, $\mLambda_1$, $\mLambda_2$, $\mPhi_1$ and $\mPhi_2$ are not found by manipulating $\mK$, but are formed directly from the eigenvalues and eigenfunctions of $\cT_K$. In fact, the ill-conditioned matrix $\mK$ never needs to be formed, nor is precise knowledge of the closed form of the kernel $K$ essential. As long as the eigenfunctions and eigenvalues are known, the Hilbert-Schmidt SVD enables us to formulate stable and efficient algorithms for the associated kernel approximation problems \citep{CFMcC13}.

There are at least two ways to interpret the Hilbert-Schmidt SVD:
(i) we've found an invertible $\mP = \mLambda_1\mPhi_1^T$ such that $\mPsi=\mK\mP^{-1}$ is better conditioned than $\mK$, and therefore we have essentially constructed a ``better basis'' (namely the functions that give rise to the columns of $\mPsi$); and
(ii) we've diagonalized the matrix $\mK$, i.e., we have found a diagonal matrix $\mLambda_1$ of Hilbert-Schmidt singular values along with matrices $\mPsi$ and $\mPhi_1$ which are generated by orthogonal eigenfunctions (but are not orthogonal matrices).

The matrix $\mPsi$ can be computed stably, and we can convert the standard (ill-conditioned) interpolation system \eqref{rbfcoef} into a well-conditioned linear system $\mPsi \bb = \by$ (where $\bb=\mP\bc$). A typical comparison between interpolation errors based on the standard (or direct) approach \eqref{rbfapprox} and the Hilbert-Schmidt approach applied to isotropic Gaussian kernel interpolation is shown in the left plot of Fig.~\ref{Fig_HSSVD}. The most accurate results are obtained for a positive value of $\gamma$ (they compare favorably to a polynomial interpolant corresponding to the ``flat'' $\gamma\to0$ limit). Moreover, in or near the ``flat'' limit regime the standard approach generates completely unreliable results (the dashed lines), i.e., the standard approach yields reliable answers only for relatively spiky kernels. It should also be clear that the traditional heuristic based on the uncertainty principle, i.e., ``improve accuracy by sampling data more densely and then use a rescaled, i.e., spikier, kernel $K$ to prevent the matrix $\mK$ from becoming too ill-conditioned'' does not work.

\begin{figure}[h]
    \centering
    \includegraphics[width=.4\linewidth]{Fig_Error_ex17j.eps}
    \includegraphics[width=.4\linewidth]{Fig_MLE_ex17j.eps}
\caption{Interpolation errors (left) and MLE predictions of ``optimal'' $\gamma$ (right) for data sampled at $N$ Halton points from $f(\bx) = \sin\left(\tfrac{x_1+x_2+\ldots+x_5}{5}\right)$, $\bx=(x_1,x_2,\ldots,x_5) \in [-1,1]^5$. Solid lines denote Hilbert-Schmidt SVD, dashed lines standard basis approach.}\label{Fig_HSSVD}
\end{figure}

While Fig.~\ref{Fig_HSSVD} demonstrates that we are able to solve simple moderate-dimensional problems, the results of \cite{FasHicWoz12b, FasHicWoz12a} show that the use of anisotropic kernels is essential for maintaining good convergence rates in high dimensions. Therefore, \emph{implementation of the Hilbert-Schmidt SVD algorithm needs to be extended to the anisotropic case}.

There are many other kernels for which it would be desirable to have stable and reliable algorithms. Based on the general framework provided by the Hilbert-Schmidt SVD, all that one needs are the eigenvalues and eigenfunctions of $\cT_K$. One family of special interest in the statistics community \citep{Ste99} are the Mat\'ern kernels, $K(\bx,\bt) = K_{m-d/2}\left(\gamma\|\bx-\bt\|\right) \left( \gamma \| \bx - \bt \|\right)^{m-d/2}$, $m > \frac d2$,
where $K_{m-d/2}$ are modified Bessel functions of the second kind and $m$ is a smoothness parameter that should be chosen in a data-dependent way together with the shape parameter $\gamma$. Our work on compact Mat\'ern kernels \citep{CFMcC13}
\begin{equation}\label{CMatern}
K(x,t) = \sum_{n=1}^{\infty} \left(n^2\pi^2 + \gamma^2\right)^{-\beta} 2\sin(n\pi x) \sin(n\pi t), \quad x,t \in [0,1],\ \gamma>0, \beta \in \mathbb{N},
\end{equation}
is a first step in this direction. The Hilbert-Schmidt series \eqref{CMatern} of the compact Mat\'ern kernels is also a perfect candidate for the data-dependent error bounds discussed at the end of Sect.~\ref{SecRecovery} since its eigenfunctions do not depend on the parameters $\beta$ and $\gamma$, while a smaller smoothness parameter $\beta$ leads to slower decay of the eigenvalues. Therefore, \emph{we expect to make progress on developing a Hilbert-Schmidt SVD algorithm for Mat\'ern kernels and on data dependent error bounds for multivariate function approximation}.


\subsection{Making the Most of ``Nuisance'' Parameters}
As Fig.~\ref{Fig_HSSVD} shows, for any given set of data, finding an ``optimal'' (or near-optimal) value of the shape parameter(s) can have a significant impact on the accuracy of the recovered function $\tf$. A number of potential criteria for optimizing the shape parameters were introduced by \cite{Fasshauer11}. However, up to now all of those criteria have been based on the use of the standard approach, i.e., they were subject to the effects of ill-conditioning of the $\mK$ matrix, and thus often unreliable or impractical. Recently, the MLE criterion (or more precisely, negative concentrated log-likelihood) was the first of these criteria to be converted to the stable framework. A plot demonstrating the typical effect of this conversion is given in the right part of Fig.~\ref{Fig_HSSVD}. \emph{We propose to extend the application of the Hilbert-Schmidt SVD to other criteria, and then to investigate which of these criteria is most effective.}

This latter investigation necessitates a framework in which such a comparison can take place. We plan to answer questions regarding the \emph{consistency}, \emph{rate of convergence} and \emph{stability} of various parameter estimation schemes. For consistency we ask whether a scheme is able to recover the ``true'' value of $\gamma$ as we sample more and more densely. The rate of convergence of a scheme determines how fast the ``optimal'' value suggested by the method approaches the ``true'' value as the sampling density increases, and stability of a scheme will be judged by the effects small changes in the data have on the consistency and convergence rate of the scheme. \emph{We intend to investigate and introduce a metric for comparing the quality of different parameter estimation criteria, and we also plan to derive other parameter optimization criteria, including for the low-rank version of the Hilbert-Schmidt SVD mentioned in the next section.}


\subsection{Truncating the Hilbert-Schmidt Series}\label{Sec_TruncHS}
In order to make the Hilbert-Schmidt SVD practical and ensure that it can be used in conjunction with the error guarantees of Sect.~\ref{SectCones}, a theoretical basis for the truncation of the Hilbert-Schmidt series needs to be established. \cite{FMcC12} introduced two different scenarios: (i) truncation at $M\ge N$ terms so that every entry $K(\bx_i,\bx_j)$ of $\mK$ is represented to within a tolerance $\varepsilon$, (ii) truncation at $M < N$. The former approach results in data-dependent basis functions generated by the $N$ first eigenfunctions of $\cT_K$ corrected by linear combinations of the next $M-N$ eigenfunctions (cf.~\eqref{StableBasis}), while the low-rank approach (ii) uses only the first $M$ (data-independent) eigenfunctions as a basis. The method used by \cite{FMcC12} to determine the truncation length $M$ is similar to that used earlier by Fornberg and co-workers in their RBF-QR approach \citep{FornbergPiret08, FornbergFlyerLarsson11}.  We discussed a more straightforward strategy to determine $M$ in \cite{CFMcC13}. However, neither of these methods provide any direct insight on the approximation error. Work in that general direction can be found in a recent preprint \citep{GriebelRiegerZwicknagl13}. Based on this, \emph{we plan to develop improved criteria for the truncation of the Hilbert-Schmidt series.}

\subsection{Other Research Topics Related to the Hilbert-Schmidt SVD}\label{SecMEEG}
There are plenty of other related questions we might tackle if time allows or the opportunity arises. However, we would like to highlight one ongoing collaboration in which kernels are used to solve a system of coupled Poisson and Laplace PDE boundary value problems that arise in MEG and EEG modeling of brain activity. Fig.~\ref{Fig_MFS_BEM} is taken from \citep{AFFGM13} and provides a cost-per-accuracy plot comparing a kernel-based solver using a method of fundamental solutions approach with a boundary element method that is considered state-of-the-art in the community \citep{fieldtrip11}. The coupled system is solved on three nested concentric spherical shells and describes the forward problem corresponding to the electric potential on the outside surface resulting from a single dipole located inside a simplified brain-skull-scalp geometry. The three different low-rank MFS solutions are all more accurate and more efficient than the BEM solution. For any desired accuracy, the MFS solution is more efficient, and this effect is more pronounced as we demand more accuracy.

\begin{figure}[h]
    \centering
    \includegraphics[width=.35\linewidth]{Fig_MFS_BEM.eps}
\caption{Cost per accuracy plot of several MFS solutions vs. BEM solution for coupled PDE system.}\label{Fig_MFS_BEM}
\end{figure}

As the next steps on this project we expect (i) \emph{to integrate the forward solver into a full inverse problem framework} which will allow us to identify the locations of activity inside the brain from measurements of the electric potential (instead of simulating the electric surface potential based on a given dipole source as done for the forward problem in Fig.~\ref{Fig_MFS_BEM}), and (ii) \emph{to move to a head model that reflects the geometry of the brain more realistically}. 