%% Accomplishments
% Major Goals

This research is comprised of four projects related to function approximation and integration:

1. Extending our research on the Hilbert-Schmidt SVD by identifying more kernels and their Mercer series for which the Hilbert- SVD can be implemented,

2. Extending our research on guaranteed, adaptive algorithms, 

3. Obtaining guaranteed, adaptive algorithms for kernel methods, and 

4. Applying our new results to various problems.


% Major Activities

Project 1. 
a) We have extended the ideas of PI Fasshauer and Qi Ye from an earlier project to find a more elegant way for deriving an explicit expression for the reproducing kernel Hilbert space (RKHS) norm, if only the reproducing kernel and its Hilbert-Schmidt decomposition is known.  This is involving relating the eigenvalues in the Hilbert-Schmidt decomposition to those of a known Sturm-Liouville problem with the same eigenfunctions but different eigenvalues.  This work has been performed by the PIs with Tanner Johnson, a 2016 summer undergraduate student supported by this grant.

Project 2. 
a) Hickernell, Choi, Ding and Xin Tong have developed a guaranteed, locally adaptive algorithm for function approximation and optimization.  This extends the earlier work of Hickernell and collaborators that produced guaranteed, globally adaptive algorithms. While many previous algorithms are locally adaptive, our new algorithm has theoretical guarantees.

b) Hickernell, Li, and Jiménez Rugama have been developing a guaranteed, adaptive quasi-Monte Carlo method that incorporates control variates.  One non-trivial challenge is that the optimal coefficient of the control variate is not the same for quasi-Monte Carlo methods as it is for IID Monte Carlo methods.  Li has completed his master's thesis on this topic.

Project 3.



Project 4.



% Specific Objectives
Project 1. 
a) We want to connect designer kernels, those defined in terms of eigenfunctions and eigenvalues chosen for their desirable properties, to explicit expressions for corresponding the RKHS norms.  We want to construct a general recipe that is demonstrated to succeed in several interesting cases. 

Project 2. 
a) Now that we have constructed a low order algorithm for univariate approximation and optimization, we would like to extend these ideas to univariate quadrature.  We also want to construct algorithms with higher order accuracy.

b) Hickernell, Li, and Jiménez Rugama have been developing a guaranteed, adaptive quasi-Monte Carlo method that incorporates control variates.  One non-trivial challenge is that the optimal coefficient of the control variate is not the same for quasi-Monte Carlo methods as it is for IID Monte Carlo methods.  Li has completed his master's thesis on this topic.

% Significant Results
Project 1. 
a) We have one new case of our method, but the details have not been fleshed out yet.  A manuscript is being prepared for publication.

Project 2. 
a) The new algorithm for univariate optimization has computational cost that is proportional to \sqrt{||f''||_{1/2}/\epsilon}, where \epsilon is the desired error tolerance.  This can be significantly smaller than the \sqrt{||f''||_{\infty}/\epsilon} cost of globally adaptive algorithms, because the 1/2-quasi-norm of a somewhat spiky function may be much smaller than the sup-norm of that same function.  The asymptotic optimality of our algorithm has been established.  A manuscript has been submitted for publication.

b) Hickernell, Li, and Jiménez Rugama have been developing a guaranteed, adaptive quasi-Monte Carlo method that incorporates control variates.  One non-trivial challenge is that the optimal coefficient of the control variate is not the same for quasi-Monte Carlo methods as it is for IID Monte Carlo methods.  Li has completed his master's thesis on this topic.

% Key Outcomes and Achievements
Project 1. 
a)

Project 2. 
a) This is the first instance we know of where the computational cost of an adaptive algorithm is expressed in terms of a quasi-norm, as we have found.  The popular Chebfun package, which performs numerical computations on Chebyshev polynomial approximations of functions, does not have guarantees that the answers they provide satisfy a given error tolerance.  Our algorithms have such guarantees.

%% Training and professional development

%% Dissemination of results

%% What are we planning to do

