%Fred's NSF Grant Dec 2017
\documentclass[11pt]{NSFamsart}
\usepackage{latexsym,amsfonts,amsmath,epsfig,multirow}
\usepackage{stackrel,tabularx,mathtools,enumitem,xspace}
\usepackage[dvipsnames]{xcolor}
\usepackage[numbers]{natbib}
\usepackage{hyperref,accents, booktabs}
\usepackage{algorithm, algorithmic}
\usepackage{cleveref}
\newcommand\myshade{85}
\colorlet{mylinkcolor}{violet}
\colorlet{mycitecolor}{Aquamarine}
%\colorlet{mycitecolor}{OliveGreen}
\colorlet{myurlcolor}{YellowOrange}

\hypersetup{
	linkcolor  = mylinkcolor!\myshade!black,
	citecolor  = mycitecolor!\myshade!black,
	urlcolor   = myurlcolor!\myshade!black,
	colorlinks = true,
}


% This package prints the labels in the margin
%\usepackage[notref,notcite]{showkeys}


%\pagestyle{empty}
\thispagestyle{plain}
\pagestyle{plain}

\headsep-0.6in
%\headsep-0.45in

\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\textheight9in
%\textheight9.1in


\providecommand{\FJHickernell}{Hickernell}
\newcommand{\hf}{\widehat{f}}
\newcommand{\hI}{\hat{I}}
\newcommand{\hatf}{\hat{f}}
\newcommand{\hatg}{\hat{g}}
\newcommand{\tf}{\widetilde{f}}
\newcommand{\tbf}{\tilde{\bff}}
%\DeclareMathOperator{\Pr}{\mathbb{P}}
\DeclareMathOperator{\cost}{cost}
\DeclareMathOperator{\comp}{comp}

\DeclareMathOperator{\loss}{loss}
\DeclareMathOperator{\lof}{lof}
\DeclareMathOperator{\reg}{reg}
\DeclareMathOperator{\CV}{CV}
\DeclareMathOperator{\size}{wd}
\DeclareMathOperator{\GP}{\mathcal{G} \! \mathcal{P}}

\newcommand{\reals}{{\mathbb{R}}}
\newcommand{\naturals}{{\mathbb{N}}}
\newcommand{\integers}{{\mathbb{Z}}}
\def\expect{{\mathbb{E}}}
\def\il{\left<}
\def\ir{\right>}
\def\e{\varepsilon}
\def\g{\gamma}
\def\l{\lambda}
\def\b{\beta}
\def\a{\alpha}
\def\lall{\Lambda^{{\rm all}}}
\def\lstd{\Lambda^{{\rm std}}}

\newcommand{\vf}{\boldsymbol{f}}
\newcommand{\hV}{\widehat{V}}
\newcommand{\tV}{\widetilde{V}}
\newcommand{\hcut}{\frak{h}}

\newcommand{\bbE}{\mathbb{E}}
\newcommand{\tQ}{\widetilde{Q}}
\newcommand{\mA}{\mathsf{A}}
\newcommand{\mB}{\mathsf{B}}
\newcommand{\mC}{\mathsf{C}}
\newcommand{\mD}{\mathsf{D}}
\newcommand{\mG}{\mathsf{G}}
\newcommand{\mH}{\mathsf{H}}
\newcommand{\mI}{\mathsf{I}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\mK}{\mathsf{K}}
\newcommand{\tmK}{\widetilde{\mathsf{K}}}
\newcommand{\mL}{\mathsf{L}}
\newcommand{\mM}{\mathsf{M}}
\newcommand{\mP}{\mathsf{P}}
\newcommand{\mQ}{\mathsf{Q}}
\newcommand{\mR}{\mathsf{R}}
\newcommand{\mX}{\mathsf{X}}
\newcommand{\mPhi}{\mathsf{\Phi}}
\newcommand{\mPsi}{\mathsf{\Psi}}
\newcommand{\mLambda}{\mathsf{\Lambda}}

\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\oerr}{\overline{\err}}
\DeclareMathOperator{\herr}{\widehat{\err}}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\INT}{INT}
\DeclareMathOperator{\APP}{APP}
\DeclareMathOperator{\OPT}{MIN}

\newcommand{\bone}{\boldsymbol{1}}
\newcommand{\bzero}{\boldsymbol{0}}
\newcommand{\binf}{\boldsymbol{\infty}}
\newcommand{\ba}{{\boldsymbol{a}}}
\newcommand{\bb}{{\boldsymbol{b}}}
\newcommand{\bc}{{\boldsymbol{c}}}
\newcommand{\bd}{{\boldsymbol{d}}}
\newcommand{\be}{{\boldsymbol{e}}}
\newcommand{\bff}{{\boldsymbol{f}}}
\newcommand{\bhh}{{\boldsymbol{h}}}
\newcommand{\beps}{{\boldsymbol{\varepsilon}}}
\newcommand{\tbeps}{\tilde{\beps}}
\newcommand{\bx}{{\boldsymbol{x}}}
\newcommand{\bX}{{\boldsymbol{X}}}
\newcommand{\bh}{{\boldsymbol{h}}}
\newcommand{\bk}{{\boldsymbol{k}}}
\newcommand{\bg}{{\boldsymbol{g}}}
\newcommand{\bv}{{\boldsymbol{v}}}
\newcommand{\bu}{{\boldsymbol{u}}}
\newcommand{\by}{{\boldsymbol{y}}}
\newcommand{\bt}{{\boldsymbol{t}}}
\newcommand{\bz}{{\boldsymbol{z}}}
\newcommand{\bvarphi}{{\boldsymbol{\varphi}}}
\newcommand{\bgamma}{{\boldsymbol{\gamma}}}
\newcommand{\bphi}{{\boldsymbol{\phi}}}
\newcommand{\bpsi}{{\boldsymbol{\psi}}}
\newcommand{\bnu}{{\boldsymbol{\nu}}}
\newcommand{\balpha}{{\boldsymbol{\alpha}}}
\newcommand{\bbeta}{{\boldsymbol{\beta}}}
\newcommand{\bo}{{\boldsymbol{\omega}}}  %GF added
\newcommand{\newton}[2]{\left(\begin{array}{c} #1\\ #2\end{array}\right)}
\newcommand{\anor}[2]{\| #1\|_{\mu_{#2}}}
\newcommand{\satop}[2]{\stackrel{\scriptstyle{#1}}{\scriptstyle{#2}}}
\newcommand{\setu}{{\mathfrak{u}}}

\newcommand{\me}{\textup{e}}
\newcommand{\mi}{\textup{i}}
\def\d{\textup{d}}
\def\dif{\textup{d}}
\newcommand{\cc}{\mathcal{C}}
\newcommand{\cb}{\mathcal{B}}
\newcommand{\cl}{L}
\newcommand{\ct}{\mathfrak{T}}
\newcommand{\cx}{{\Omega}}
\newcommand{\calc}{{\mathcal{C}}}
\newcommand{\calf}{{\mathcal{F}}}
\newcommand{\calfd}{{\calf_d}}
\newcommand{\calh}{{\mathcal{H}}}
\newcommand{\tcalh}{{\widetilde{\calh}}}
\newcommand{\calI}{{\mathcal{I}}}
\newcommand{\calhk}{\calh_d(K)}
\newcommand{\calg}{{\mathcal{G}}}
\newcommand{\calgd}{{\calg_d}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\fA}{\mathfrak{A}}
\newcommand{\fC}{\mathfrak{C}}
\newcommand{\fF}{\mathfrak{F}}
\newcommand{\fL}{\mathfrak{L}}
\newcommand{\hS}{\widehat{S}}
\DeclareMathOperator{\Prob}{\mathbb{P}}

\def\abs#1{\ensuremath{\left \lvert #1 \right \rvert}}
\newcommand{\bigabs}[1]{\ensuremath{\bigl \lvert #1 \bigr \rvert}}
\newcommand{\norm}[2][{}]{\ensuremath{\left \lVert #2 \right \rVert}_{#1}}
\newcommand{\ip}[3][{}]{\ensuremath{\left \langle #2, #3 \right \rangle_{#1}}}
\newcommand{\bignorm}[2][{}]{\ensuremath{\bigl \lVert #2 \bigr \rVert}_{#1}}
\newcommand{\calm}{{\mathfrak{M}}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\filldis}{fill}
\DeclareMathOperator{\sep}{sep}
\DeclareMathOperator{\avg}{avg}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\cov}{cov}

\newcommand{\des}{\{\bx_i\}}
\newcommand{\desinf}{\{\bx_i\}_{i=1}^{\infty}}
\newcommand{\desn}{\{\bx_i\}_{i=1}^n}
\newcommand{\wts}{\{g_i\}_{i=1}^N}
\newcommand{\wtsn}{\{g_i\}_{i=1}^N}
\newcommand{\datan}{\{y_i\}_{i=1}^N}

%FJH added
\newcommand{\Order}{\mathcal{O}}
\newcommand{\ch}{\mathcal{H}}
\newcommand{\tch}{{\widetilde{\ch}}}
\newcommand{\veps}{\boldsymbol{\varepsilon}}
\DeclareMathOperator{\best}{best}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\hsigma}{\hat{\sigma}}
\newcommand{\tK}{\widetilde{K}}
\newcommand{\Matlab}{{\sc Matlab}\xspace}
\newcommand{\abstol}{\varepsilon_{\text{a}}}
\newcommand{\reltol}{\varepsilon_{\text{r}}}

\newcommand\starred[1]{\accentset{\star}{#1}}


\definecolor{MATLABOrange}{rgb}{0.85,  0.325, 0.098}

\newtheorem{resproblem}{Research Problem}
\newtheorem{research}{Research Objectives}



%\setcounter{page}{1}

\setlist[description]{font=\normalfont\itshape}



\begin{document}
%\setlength{\leftmargini}{2.5ex}

\centerline{\Large \textbf{Project Description}}
\vspace{-2ex}

\setcounter{tocdepth}{1}
\tableofcontents

\vspace{-6ex}

\section{Scientific Context and Timeliness of the Proposed Research}
Integration, function approximation, and optimization are the nuts and bolts calculations when 
solving practical problems in a variety of areas, including financial risk management, particle physics, 
Bayesian inference, and uncertainty quantification.  Practitioners would like to expend the  
computational effort required to meet the desired error criterion, but not expend unneeded 
computational effort. 
Practitioners also want the algorithm to require no more than a specification of the problem and the 
error criterion.   Adaptive algorithms fill this need by determining when to stop the computation 
based on data-driven error bounds, $\herr_n(f)$, where $f$ is the input function and $n$ is the 
number of nodes used in the calculation.

Common adaptive algorithms employ \emph{heuristic} error bounds, that sometimes fail, and for 
unknown reasons. Traditional error 
analysis for numerical algorithms 
yields error bounds, $\oerr_n(f)$, which involve norms of the input functions, $\norm[\calf]{f}$, lying 
in 
some Banach space of input functions, $\calf$.  However, $\norm[\calf]{f}$ is typically unknown a 
priori.  Thus, traditional  error 
bounds by themselves are inadequate  to produce rigorous stopping criteria for adaptive algorithms.

Our project will develop \emph{rigorous} data-driven error bounds that will underpin new adaptive 
numerical algorithms for integration, function approximation, and optimization.  The 
key---discovered in our recent research---is to identify a cone of \emph{reasonable} input functions, 
$\cc 
\subset \calf $,  for which the traditional error bounds can certainly be bounded in terms of 
data-driven error bounds, i.e., $\oerr_n(f) \le \herr_n(f)$ for all $f \in \cc$.  
Our new adaptive algorithms will yield answers to with desired accuracy and require only 
specification of the problem and the error tolerance---not a bound on $\norm[\calf]{f}$. 
These adaptive algorithms will be implemented in the freely available GAIL software library.  They will 
be tested on various applications problems to ensure that theory matches practice.  The 
development, implementation and application of these algorithms will involve undergraduate 
students through senior faculty in a variety of disciplines.

\subsection{An Illustrative Example: Integration by the Trapezoidal Rule}  
\label{TrapIllSect}
We introduce some 
key ideas in our proposed research via the problem of univariate integration.  Calculus 
students are taught to approximate the definite integral by the trapezoidal rule 
\cite[Sect.\ 7.2, (7.15)]{BraPet11a}:
\begin{gather}
\label{traprule}
\int_0^1 f(x) \, \dif x \approx T_n(f):= \frac1{2n} \left[f(0) + 2f(1/n) + \cdots + 2f(1-1/n) + f(1) \right ], \\
\label{traperrbd}
\err_n(f) := \abs{\int_0^1 f(x) \, \dif x - T_n(f)}  \le \frac{\Var(f')}{8 n^2} =: \oerr_n(f),
\end{gather}
where the total variation of the first derivative is defined as 
\begin{equation*}
\Var(f') : = \sup \left \{\widehat{V}(f',\{t_i\}_{i=0}^n) := \sum_{i=2}^{n-1}  \abs{f'(t_i) - f'(t_{i-1})} \, \Big 
\vert \, 
\forall \text{ partitions }  \{t_i\}_{i=0}^n \text{ of } [0,1], \ \forall n \in \naturals \right \}.
\end{equation*} 
A partition $\{t_i\}_{i=0}^n$ of an interval is an ordered subset including both 
endpoints.
The practitioner wants to know how large $n$ 
must be to ensure that $\err_n(f)  \le \varepsilon$, for a prescribed 
absolute error tolerance $\varepsilon$.  Ensuring that $\oerr_n(f) \le \varepsilon$ is sufficient, but 
this requires an upper bound on $\Var(f')$, which is normally unavailable  unless $f$ has a simple 
form.

Numerical analysis texts  \cite[p.\ 223--224]{BurFai10}, \cite[p.\ 233]{CheKin12a}, 
and  \cite[p.\ 270]{Sau12a} propose estimating the absolute error of $T_n(f)$ by 
\begin{equation} \label{tradtraperrest}
 \err_n(f) \approx  \abs{\frac{T_n(f) - T_{n/2}(f)}{3}} =: \herr_n(f).
\end{equation}
Although this $\herr_n(f)$ is valid in some cases, it is flawed.  This $\herr_n(f)$  fails for 
spiky integrands whose 
spikes fall  between the nodes as in Figure \ref{quadfailfig}(a).  Worse, this $\herr_n(f)$ can fail 
for 
integrands that are \emph{not spiky} if $T_n(f)$ is coincidentally (nearly) 
the same as  $T_{n/2}(f)$ (see Figure \ref{quadfailfig}(b)).  Over 30 years ago, James 
Lyness \cite[p.\ 69]{Lyn83} pointed out the flaws of nearly all adaptive quadratures:
\begin{quote}
	While prepared to take the risk of being misled by chance alignment of zeros in the integrand 
	function, or by narrow peaks which are ``missed,'' the user may wish to be reassured that for 
	``reasonable'' integrand functions which do not have these characteristics all will be well. It is the 
	purpose of the rest of this section to demonstrate by example that he cannot be reassured on this 
	point. In fact the routine is likely to be unreliable in a significant proportion of the problems it faces 
	(say $1$ to $5\%$) and there is no way of predicting in a straightforward way in which of any set 
	of apparently reasonable problems this will happen.
\end{quote}
Figure \ref{quadfailfig}(b) depicts a reasonable integrand for which $\herr_n(f)$ in 
\eqref{tradtraperrest} fails.  Figure 
\ref{quadfailfig}(c) depicts a reasonable integrand for which the error estimate, $\herr_n(f)$, 
used 
by  
MATLAB's adaptive quadrature 
\texttt{integral} \cite{MAT9.3} fails.  

\begin{figure}[h]
	\begin{tabular}{>{\centering}m{0.31\textwidth}@{\quad}>{\centering}m{0.31\textwidth}@{\quad}>{\centering}m{0.3\textwidth}}
		\includegraphics[width=0.3\textwidth]{ProgramsImages/SpikyFoolTrapezoidalcolor.eps} &
		\includegraphics[width=0.3\textwidth]{ProgramsImages/FlukyFoolTrapezoidalcolor.eps} &
		\includegraphics[width=0.3\textwidth]{ProgramsImages/FlukyFoolIntegralcolor.eps} 
		\tabularnewline
		(a) $\err_{100}(f) = 1 $ &
		(b) $\err_{100}(f) =  1.4\text{E}{-4}$ &
		(c) $\err_{150}(f) =  2.8\text{E}{-5}$
		\end{tabular}
	\caption{Integrands for which $\herr_n(f) = 0$ but 
	$\err_n(f) \ne 0$. \label{quadfailfig}}
\end{figure}

Despite Lyness's gloomy portrayal of adaptive quadratures, the PI and his collaborators have 
developed an adaptive trapezoidal algorithm that \emph{succeeds for all reasonable integrands}, 
although 
it 
may fail for spiky integrands \cite{HicEtal14a,HicRazYun15a}. The key is a 
data-based upper bound on $\Var(f')$.  

Here we outline the key arguments from \cite{HicRazYun15a}.  Let 
$\size(\{t_i\}_{i=0}^n) : = \max_{1 \le i \le n} t_i - t_{i-1}$ denote the partition width.  By 
definition, $\Var(f')$ is bounded below by $\widehat{V}(f',\{t_i\}_{i=0}^n)$ for any partition. 
Reasonable integrands are defined as those functions for which an inflated lower bound on  
$\Var(f')$ provides an \emph{upper bound} on $\Var(f')$:
\begin{multline} \label{conedef}
\cc := \Bigl \{ f \in C^1[0,1]: \Var(f') \le \fC(\size(\{t_i\}_{i=0}^n)) \hV(f',\{t_i\}_{i=0}^n) \text{ for all }   n 
\in \naturals \\
 \text{and partitions } \{t_i\}_{i=0}^n \text{ with } 
\size(\{t_i\}_{i=0}^n) < \hcut \Bigr \}, \qquad  \fC(h) = \frac{\fC_0 \hcut}{\hcut - h}, \quad \fC_0 
> 1, \ 
\ 0 < \hcut < 1.
\end{multline}
This set of reasonable integrands, $\cc$, is a \emph{cone} since $f \in \cc \implies cf \in \cc$ 
for 
any 
constant $c$.  The 
parameters $\hcut$ and $\fC_0$ determine how inclusive this 
set of reasonable functions is.  It is shown in \cite{HicRazYun15a} that $\Var(f')$ can rigorously be 
bounded above by an 
inflated variation of the derivative of the linear spline, which depends only on function values, not on 
derivative values:
\begin{equation*} %\label{tVdef}
\tV_n(f) 
% & : = \sum_{i=1}^{n-1} \abs{ \frac{f((i+1)/n)-f(i/n)}{1/n} - \frac{f(i/n)-f((i-1)/n)}{1/n}}  \\& 
:= n\sum_{i=1}^{n-1} \abs{f((i+1)/n)-2f(i/n)+f((i-1)/n)}, 
\quad \Var(f') \le \fC(2/n) \tV_n(f),  \ \ 
\forall 
f \in \fC,  \ n > 2 /\hcut.
\end{equation*}
This provides a data-driven upper bound on the error  of the trapezoidal rule, namely,
\begin{equation} \label{guartraperrest}
\err_n(f) \le \oerr_n(f) \le \herr_n(f) := \frac{\fC(2/n) \tV_n(f)}{8n^2} \qquad 
\forall 
f \in \fC,  \ n > 2 \hcut.
\end{equation}
An adaptive quadrature, Algorithm \ref{Trapalg},  is constructed by increasing $n$ until 
$\herr_n(f)$ does 
not exceed the error tolerance, $\varepsilon$.

\begin{algorithm}
	\caption{Adaptive Trapezoidal Rule \label{Trapalg}} 
	\begin{algorithmic}[1]
		\REQUIRE a black-box function, $f$; an absolute error tolerance,
		$\varepsilon>0$; the maximum partition width, $\hcut$; the constant $\fC_0 > 1$;  the 
		maximum number of trapezoids, $n_{\max}$

\ENSURE $\abs{\int_0^1 f(x) \dif x - \ct(f,\varepsilon)} \le \varepsilon$, provided $f \in \cc$

\STATE $n \leftarrow \lfloor 1 /\hcut \rfloor + 1$

\REPEAT

\STATE $n \leftarrow 2n$

\STATE Compute $\herr_n(f)$ according to \eqref{guartraperrest}

\UNTIL  $\herr_n(f) \le \varepsilon$ or $n > n_{\max}$

\STATE Compute $\ct(f,\varepsilon) = T_n(f)$ according to \eqref{traprule}.
		
\end{algorithmic}
	\end{algorithm}


\subsection{Key Ideas Illustrated by the Adaptive Trapezoidal Rule Algorithm 
\ref{Trapalg}} 
\label{subsect:trap}

\begin{description}[leftmargin=2.5ex]
	\item[Bound the error by bounding the semi-norm of $f$] Algorithm \ref{Trapalg} succeeds 
	by 
	\emph{bounding} 
	$\Var(f')$---a key term in 
	error 
	bound \eqref{traperrbd}---for reasonable functions.  Typical  adaptive quadrature algorithms 
	remain 
	flawed because they assume that the difference between two quadrature rules is indicative of the 
	error of one of them as in \eqref{tradtraperrest}.  
	
	\item[Identify a cone, $\cc$, of reasonable functions] Our set of reasonable functions is a 
	\emph{cone} 
	defined precisely in \eqref{conedef}.  The widest spike must be wider than the minimum 
	width, $\hcut$.  
	The bound of $\Var(f')$ in terms of an inflated $ \hV(f',\{t_i\}_{i=0}^n)$ excludes functions with 
	large
	changes in $f'$ over small intervals.
	
	\item[$\cc$ is non-convex] A convex combination of two reasonable 
	functions, $f_1$ and 
	$f_2$, may produce 
	a spiky function as illustrated below (the narrow width, not the height makes it a spike):
	
	\centerline{\includegraphics[width = 0.35\textwidth]{ProgramsImages/broadpk.png}
	\includegraphics[width = 0.35\textwidth]{ProgramsImages/choppedpk.png}
\includegraphics[width = 0.35\textwidth]{ProgramsImages/narrowpk.png}}

\vspace{-4ex}
  \noindent A non-convex cone of input functions, $\cc$, is key to an adaptive algorithm 
  that outperforms non-adaptive algorithms.  For convex sets of input functions---such as 
  balls---adaptive algorithms have no 
  advantage under rather 
  general conditions  \cite[Chap.\ 4, Corollary 5.2.1]{TraWasWoz88}.
	

	\item[Bound the computational cost of the algorithm] Let $\cost(\ct,f,\varepsilon)$ denote the 
	computational cost or number of function values  required by 
	Algorithm \ref{Trapalg} to obtain the required $\ct(f,\varepsilon)$ for $f \in \cc$.  It is shown that 
	$\cost(\ct,f,\varepsilon) = \Order (\sqrt{\Var(f')/\varepsilon})$ in \cite{HicRazYun15a}.  
	(Since $\Var(f')$ is unbounded for $f \in \cc$, we 
	include it in the expression for the computational cost.)  The computational cost for Algorithm 
	\ref{Trapalg} is asymptotically the same as that for a non-adaptive trapezoidal rule when 
	$\Var(f')$ has a known 
	upper bound.  For 
	Algorithm \ref{Trapalg} an upper bound on $\Var(f')$ is \emph{not required}.
	
	\item[Show that the adaptive algorithm is optimal] The \emph{computational complexity} of this 
	integration problem is the computational cost of the best possible algorithm that is based on 
	function values only,  
	\[
	\comp(v,\varepsilon)  := \inf_ {\ct'} \sup_{f \in \cc, \Var(f') \le v} \cost(\ct',f,\varepsilon)
	\]
	It is also shown in \cite{HicRazYun15a} that $\comp(v,\varepsilon) = \Order 
	(\sqrt{v/\varepsilon})$.  This is 
	established by constructing fooling functions that look like zero to the algorithm, but whose 
	integral is nonzero. Thus, our algorithm is asymptotically optimal.
	
	\item[The definition of adaptive algorithm includes parameters] MATLAB's 
	\texttt{integral} \cite{MAT9.3}
	assumes an initial number of nodes and a maximum sample size.  Chebfun 
	\cite{TrefEtal17a}, a 
	numerical 
	library that adaptively integrates functions via Chebyshev polynomial approximations 
	assumes 
	default values for these parameters as well as others.
	These parameters are set to be reasonable for most situations, but may be modified if 
	necessary.  The parameters  $\hcut = 0.02, \fC_0 = 
	1.5$, and 
	$n_{\max} = 10^7$ are meant to remain the same from problem to problem, but may be 
	modified 
	to 
	enhance speed or increase robustness.
	
	\item[The adaptive algorithm is available]  Algorithm 1 and related adaptive algorithms 
	developed 
	by the PI and his collaborators have been published in the Guaranteed Automatic 
	Integration 
	Library (GAIL) \cite{ChoEtal17b}.  We propose to further develop this library (Sect.\ 
	\ref{}).
	
	\item[Error bound exists even if  time is the limiting factor]  Specifying the number of 
nodes rather than the error tolerance is accomplished by setting $n_{\max}$ to be this
 number and $\varepsilon = 0$.  The error bound, $\herr_n(f)$, is not needed to 
stop the algorithm but is available to inform the user how large the error might be.

	\item[This algorithm is globally adaptive] The  quadrature algorithm 
	described above adaptively determines $n$ based on function data, which we term \emph{global 
	adaptivity}.  Algorithms that determine the placement of the nodes, as well as the number, based 
	on function data are termed \emph{locally adaptive}.  Sect.\ 
	\ref{previousmeritsubsec} below describes locally adaptive algorithms for function 
	approximation and optimization.
	
	\item[Optimality is only for integrands with limited smoothness] This adaptive quadrature 
	algorithm is 
	optimal for the cone of integrands, $\cc$, however, it is 
	sub-optimal for smoother integrands.  See Sect.\ \ref{SectUniProb} for proposed research in 
	this direction.
	
\end{description}

Our proposed research addresses a class of problems beyond univariate integration.  The next 
subsections provide the notation for these problems and outline our research goals.


\subsection{Traditional Error Analysis} Let $\calf$ be a Banach space of input functions 
defined on the domain $\cx$, let
$\calg$ be a Banach space of possible 
solutions, and let $S:\calf \to \calg$ be a solution operator.  Ignoring for now the precise definitions 
of $\calf$ and $\calg$, some examples of solution 
operators for integration, function approximation (recovery), and optimization are
\begin{equation} \label{probs}
S = \INT: f \mapsto \int_{\Omega} f(\bx) \, \dif \bx, \qquad S = \APP:f \mapsto f, \qquad S = 
\OPT :f \mapsto 
\min_{\bx \in \Omega} f(\bx).
% \\S: f \mapsto u, \text{ where } u'' + u = f(x), \quad u(0) = u(1) = 0.
\end{equation}
Let $\{A_n: 
\calf \to \calg\}_{n \in \calI}$ be a sequence of algorithms.  Each $A_n$  samples the 
input function, 
$f$, at $n$ nodes
(data sites) $\desn \subset \cx$, and produces an approximation to $S(f)$ based 
on $\{(\bx_i,f(\bx_i))\}_{i=1}^n$.
Here 
$\calI$ is an index set of positive integers. 

The error bounds  for $A_n$ typically take the  form
\begin{gather} \label{typicalerr}
\err_n(f): = \norm[\calg]{S(f) - A_n(f)} \le \norm{A_n} \norm[\calf]{f} =: \oerr_n(f) \qquad \forall f \in 
\calf, \ n \in 
\calI, 
\\ \text{where }\norm{A_n}  : = \sup_{0 \ne f \in \calf} \frac{\norm[\calg]{S(f) - A_n(f)}} 
{\norm[\calf]{f}}.
\end{gather} 
In some cases, such as  \eqref{traperrbd}, $\norm[\calf]{f}$ is a 
semi-norm rather than a norm.  The term $\norm{A_n}$
represents the worst case error for functions in the unit ball in $\calf$ and is a quality measure for 
the algorithm, including the
choice of nodes.  In some cases 
$\norm{A_n}$ has an explicit expression ($1/(8n^2)$ in \eqref{traperrbd}), whereas in other 
cases one 
can only derive an upper bound on $\norm{A_n}$ or the convergence rate of 
$\norm{A_n}$. 
Error bounds of the form  \eqref{typicalerr} are typically 
illustrated by plotting $\norm[\calg]{S(f) - A_n(f)}$ versus $n$ for some example $f$ and showing 
that the 
convergence rate has the same order as $\norm{A_n}$.

\subsection{Adaptive Algorithms} From a practical standpoint, users would like an adaptive 
algorithm, 
$\fA$, which only requires as input a black box function algorithm, $f$, and an error tolerance, 
$\varepsilon$, and which satisfies the error criterion
\begin{equation} \label{errorcrit} \tag{CRIT}
\norm[\calg]{S(f) - \fA(f,\varepsilon)} \le \varepsilon.
\end{equation}
In theory, one may define $\fA(f,\varepsilon) = A_n(f)$, where $n = \min \{ n' \in \calI : \norm{A_n} \le 
\varepsilon / 
\norm[\calf]{f}\}$.  But, even if $\norm{A_n}$ is known explicitly, $\norm[\calf]{f}$ is 
typically 
unknown and cannot be 
bounded easily.  Thus, the error bound $\oerr_n(f)$ in \eqref{typicalerr} does not directly support an 
adaptive algorithm.

Adaptive algorithms require error bounds, $\herr_n(f)$, computed from function data, 
$\{(\bx_i,f(\bx_i))\}_{i=1}^n$, and not a priori knowledge of the (semi-)norm of the function.  Those 
commonly used in practice are inadequate because there is no theory that justifies
$\oerr_n(f)  \le \herr_n(f)$.  As mentioned earlier MATLAB's \texttt{integral} bases its $\herr_n(f)$ on 
the comparison of two Gaussian quadratures.  Chebfun determines the number of polynomials 
required to approximate a function approximation of functions by Chebyshev polynomials by 
determining when the coefficients of the higher degree polynomials have reached round-off 
error.

There is a family of adaptive algorithms with a theoretical foundation based on interval 
arithmetic 
described in \cite{MoKeCl09} and \cite{Rum10a} and implemented in INTLAB \cite{Rum99a}.  
This 
approach requires the function subroutines to take interval arguments and return interval 
outputs.  
We do not pursue this approach because existing black-box function algorithms
normally 
do not meet these requirements.

\subsection{What We Propose}
The numerical algorithm that computes $\cos(x)$ to the desired accuracy---typically machine 
precision---does not require the user to determine how many terms in the polynomial 
approximation are required.  The computation is \emph{automatic}.  Furthermore, theory 
underpins this algorithm.  We believe that the same should be possible for problems that are 
somewhat more difficult than evaluating elementary or special functions, such as the problems in 
\eqref{probs}.  

As computing power grows, and is matched by the complexity of the simulations attempted, certain 
fundamental problems, such as integration, function approximation, and optimization should become 
as effortless as computing $\cos(x)$.  If the user specifies the problem and the error tolerance, the 
algorithm should be able to provide the answer meeting the accuracy requirement.  Unfortunately, 
that is not the case yet because our current adaptive integration, function approximation, and 
optimization algorithms do not have sound theoretical underpinnings.

To bring us closer to our ideal, we propose the following research program:
\begin{itemize}
	\item Develop adaptive algorithms, $\fA$, which depend on the 
	problem definition, the cone of reasonable functions, $\cc$, and the
	function data, and which certainly satisfy error criterion \eqref{errorcrit} for all $f \in \cc$.  
	
	\item The adaptive algorithms will likely be based on familiar algorithms, $A_n$, with 
	well-understood theoretical error bounds, $\oerr_n(\cdot)$.  
	
	\item The cones, $\cc$, will be defined in a way that facilitates a data-driven error 
	bound, $\herr_n(\cdot)$, which satisfies $\oerr_n(f) \le \herr_n(f)$ for all $f\in \cc$.  A key 
	challenge is to identify the right $\cc$ and $\herr_n(\cdot)$.  For any silly 
	$\herr_n(\cdot)$, one could define $\cc = \{f \, \vert \, \oerr_n(f) 
	\le \herr_n(f) \}$, but we insist that the definition of $\cc$ \emph{not} depend on specific data, 
	such as those used to define $\herr_n(f)$.
	
	\item Then $\fA(f,\varepsilon) = A_n(f)$ will be defined by choosing $n$ such that $\herr_n(f) \le 
	\varepsilon$.  This will ensure that \eqref{errorcrit} is satisfied for all $f \in \cc$.  
	
	\item Letting $\cost(\fA,f,\varepsilon)$ denote the computational cost of $\fA$, we will
	determine $p$ such that 
	$\cost(\fA,f,\varepsilon)  = \Order\bigl((\norm[\calf]{f}/\varepsilon)^p\bigr)$ as 
	$\norm[\calf]{f}/\varepsilon 
	\to 
	\infty$.  
	
	\item Letting $\comp(v,\varepsilon)$ denote the computational complexity of the problem, i.e., 
	the computational cost of the best algorithm that succeeds for all $f \in \cc$ with $\norm[\calf]{f} 
	\le v$, we aim to demonstrate that 
	our 
	adaptive algorithms are asymptotically optimal, i.e., $\comp(v,\varepsilon) = \Order 
	\bigl((v/\varepsilon)^p \bigr)$ for the same $p$ as above.
	
	\item These adaptive algorithms, $\fA$, will be implemented in the freely-available GAIL 
	\cite{ChoEtal17b}.  Their 
	effectiveness will be demonstrated on various practical problems.
	
	\item Academics, practitioners and students will be introduced to these 
	new adaptive algorithms.  Students (high school through PhD) contributing to GAIL will learn to 
	collaboratively develop robust, tested, 
	and documented numerical software that supports reproducible research.  This will support their 
	future careers and strengthen our country's computational science capability.
	
\end{itemize}



Our recent NSF-Funded research---summarized in the next section---has obtained some success in 
this direction.  The proposed research detailed 
in Sect.\ \ref{secProposed} aims to significantly enlarge the family of adaptive numerical 
algorithms that have both a theoretical basis and practical application.

\section{Results of Previous NSF-Funded Research,
NSF-DMS-1522687\except{toc}{, \emph{Stable, Efficient, Adaptive Algorithms for 
Approximation and 
Integration}, 
\$270,000, 2015--2018}} \label{SectPrevious}

G. E. Fasshauer (GEF, co-PI) and F. J. Hickernell (PI) have led this project.  In the summer of 
2016, Fasshauer left Illinois Tech and has participated since then as a sub-contractor.  
Sou-Cheng Choi (SCC) has contributed as a senior personnel.  Other major contributors 
have been the PI's research students Yuhan Ding (YD, PhD 2015), Lan Jiang (LJ, PhD 2016), 
Llu\'is Antoni Jim\'enez Rugama (LlAJR, PhD 2016), Da Li (DL, MS 2016), Jagadees Rathinavel (JR, 
PhD student)
Xin Tong (XT, MS 2014, PhD student, University of Illinois at Chicago), Kan Zhang (KZ, PhD 
student), Yizhi Zhang (YZ, PhD student), and Xuan Zhou (XZ, PhD 2015).  Articles, theses,  
software, and preprints supported in 
part by this 
grant 
include 
\cite{ala_augmented_2017, 
	ChoEtal17a,
	ChoEtal17b,
	Din15a, 
	GilEtal16a,
	GilJim16b,
	Hic17a,
	HicJim16a,
	HicEtal18a,
	HicEtal17a,
	JimHic16a,
	JohFasHic18a,
	Li16a,
	Liu17a,
	mccourt_stable_2017,
	mishra_hybrid_nodate,
	mishra_stable_nodate, 
	rashidinia_stable_nodate,
	vu_rbf-fd_nodate,
	Zha17a,
	Zho15a,
	ZhoHic15a}.


\subsection{Intellectual Merit from Previous NSF Funding}
\label{previousmeritsubsec}

\subsubsection{Local Adaption for Univariate Problems} The PI, SC, YD, and XT developed 
locally adaptive 
algorithms for univariate function 
approximation and 
global optimization \cite{ChoEtal17a,Din15a}, i.e., cases $\APP$ and $\OPT$ in 
\eqref{probs} for $\Omega = [a,b]$.  
The error of the \emph{linear spline}, $A_n$, using the 
nodes $a = x_0 < x_1 < \cdots < x_n = b$,  in approximating a function is 
\begin{equation} \label{linsplineerror}
\norm[{\infty,[x_{i-1},x_i]}]{f - 
	A_n(f)} \le \frac{(x_i-x_{i-1})^2\norm[\infty,{[x_{i-1},x_i]}]{f''}}{8} =: \oerr_n(f,[x_{i-1},x_i]),
\end{equation}
where $\norm[{\infty,[\alpha,\beta]}]{\cdot}$ denotes the sup-norm over the interval 
$[\alpha,\beta]$.  The cone of reasonable functions, $\cc$, consists essentially of those 
functions for 
which the local supremum of $f''$ is bounded above by an inflated value of the local infimum 
of $f''$ to the left or the right (see \cite{ChoEtal17a} for details):
\[
\cc: = \left \{ f \in W^{2,\infty} : \norm[{\infty,[\alpha,\beta]}]{f''} \le \max\left(\fC(h_{-}) 
\norm[-\infty,{[\beta-h_-,\alpha]}]{f''},\fC(h_{+})
\norm[-\infty,{[\beta, \alpha+h_+]}]{f''}\right) \  \forall h_{\pm} \le \hcut \right\},
\]
where $\fC(\cdot)$ is the inflation factor in \eqref{conedef}, and $\norm[-\infty,{[\alpha, 
\beta]}]{f''} = \inf_{\alpha \le x \le \beta} \abs{f''(x)}$.  Because one looks at the infimum of the 
second derivative on both sides, 
a reasonable function may have inflection points, but two inflection points may not be too 
close together.  

For $f \in \cc$, there exists a data-based error bound 
$\herr_n(f,[x_{i-1},x_i]) \ge \oerr_n(f,[x_{i-1},x_i])$ defined in terms of second order 
difference 
quotients on the 
left and right sides of $[x_{i-1},x_i]$.  Each interval $[x_{i-1},x_i]$ is refined until  
$\herr_n(f,[x_{i-1},x_i]) \le \varepsilon$.  The resulting linear spline, $\fA(f,\varepsilon)  = A_n(f)$, 
satisfies 
error 
criterion \eqref{errorcrit} for the function approximation problem. Figure 
\ref{localadaptfig} (left) displays an example of the function data required.  The function is 
sampled sparsely where the second derivative is small.

\begin{figure}[h]
	\centering
	\vspace{-1ex}
	\includegraphics[width = 0.48\textwidth]{ProgramsImages/sampling-funappxg.png}
	\includegraphics[width = 0.48\textwidth]{ProgramsImages/sampling-funming.png}
	
	\vspace{-2ex}
	\caption{The function data ({\color{MATLABOrange}$\bullet$}) for  locally adaptive 
	function approximation  (left) and locally adaptive optimization (right) \label{localadaptfig}}
\end{figure}


For this locally adaptive approximation algorithm 
$\cost(\fA,f,\varepsilon) = \Order\left(\sqrt{\norm[1/2]{f''}/\varepsilon} \right)$ \cite{ChoEtal17a}, 
where 
$\norm[1/2]{\cdot}$ is the $1/2$-quasinorm, which is weaker than the sup-norm.  This is  
asymptotically the same cost as for the best possible algorithm for this problem. In contrast, 
the globally adaptive function approximation algorithm in \cite{HicEtal14b} has a 
computational cost of $\Order\left(\sqrt{\norm[\infty]{f''}/\varepsilon} \right)$.  For a spiky 
function inside $\cc$, the local adaption algorithm may require significantly less effort 
than a global adaption algorithm because $\norm[1/2]{f''}$ will be much smaller than 
$\norm[\infty]{f''}$.

Similar arguments to those above lead to a locally adaptive algorithm for the 
optimization problem, $\OPT$ in \eqref{probs}.  In this case, those subintervals for which the 
function is definitely greater than the minimum found so far need not be 
refined further.  Figure 
\ref{localadaptfig} (right) gives an example of the function data required.  The function is 
sampled sparsely where the second derivative is small or the function is large.

YZ's PhD research is developing a locally 
adaptive Simpson's rule for univariate integration.  It will have higher order accuracy than Algorithm 
\ref{Trapalg} above.

\subsubsection{Globally  Adaptive Quasi-Monte Carlo (QMC) Cubature} The PI, LlAJR, and DL 
have 
developed adaptive quasi-Monte Carlo algorithms for integration over the unit cube, $\Omega = 
[0,1]^d$ \cite{HicJim16a,JimHic16a}.  For multidimensional integrals over different domains, a 
transformation of variables can 
often change the domain to the unit cube.  QMC algorithms are commonly 
chose the 
sequence of nodes, $\{\bx_i\}_{i=0}^\infty$, to be an integration lattice node sequence  or a digital 
sequence, especially the Sobol' sequence \cite{DicEtal14a}.  QMC cubature is superior to 
independent and identically distributed (IID) Monte Carlo cubature because the nodes are more 
evenly spread throughout the domain.   Fig.\ \ref{PtsFig} illustrates this fact.

\begin{figure}[h] % MATLAB Driver: PlotPoints.m
	\centering
	\includegraphics[width = 0.31\textwidth]{ProgramsImages/IIDPoints.eps} \quad
	\includegraphics[width = 0.31\textwidth]{ProgramsImages/ShiftedLatticePoints.eps}  \quad
	\includegraphics[width = 0.31\textwidth]{ProgramsImages/SSobolPoints.eps} 
	
	\caption{IID nodes contrasted with nodes used for QMC cubature.\label{PtsFig}}
\end{figure}

For the two kinds of QMC sampling considered here and illustrated in Fig.\ \ref{PtsFig}, the number 
of nodes used is commonly restricted to a power of two, which we follow.  Moreover, it is common 
to index the nodes starting with $0$, however, we start with $1$ to be consistent with the notation 
elsewhere in the proposal.  The well-known theoretical error bound for 
quasi-Monte Carlo cubature can be expressed in terms of some of the Fourier coefficients of the 
integrand \cite{DicEtal14a, DicPil10a, HicJim16a,JimHic16a, Nie92, SloJoe94}:
\begin{gather}
\nonumber
f(\bx) = \sum_{\bk \in \bbK} \hf(\bk) \phi_{\bk} (\bx),  \qquad \text{where } \hf(\bk) := \int_{[0,1]^d} 
f(\bx) \, \overline{\phi}_{\bk}(\bx) \quad \forall \bk \in \bbK, \\
\label{multiInt} \tag{M-INT} S(f): = \int_{[0,1]^d} f(\bx) \, \dif \bx, 
\qquad A_n(f) = \frac 1n \sum_{i=1}^{n} f(\bx_i), \\
\nonumber
\err_n(f) := \abs{S(f) - A_n(f)} \le \sum_{\bk \in P^\perp_n \setminus\{\bzero\}} \abs{\hf(\bk)} = : 
\oerr_n(f), \\
\nonumber
\text{Dual set: }P_n^\perp : = \{\bk \in \bbK \, \vert \, \phi_{\bk}(\bx_i) = \phi_{\bk}(\bx_1) \text{ for } 
i=1, \ldots, n \} \\
\nonumber
\begin{array}{ccc}
\text{QMC Nodes} & \bbK & \phi_{\bk} \\
\toprule
\text{Shifted integration lattice sequence} & \integers^d & \text{Multivariate complex 
exponentials}\\
\text{Shifted digital sequence, e.g., Sobol' sequence} & \naturals_0^d & \text{Multivariate Walsh 
functions}
\end{array}
\end{gather}
The error bound, $\oerr_n(f)$, arises because the basis, 
$\{\phi_{\bk}\}_{\bk \in \bbK}$ is 
chosen to match the particular QMC nodes.  As $n \to \infty$, the dual set, $P^\perp_n$, loses 
elements, and $\oerr_n(f) \to 0$.

The PI and LlAJR in \cite{HicJim16a,JimHic16a} took $\oerr_n(f)$ as a starting point for their 
adaptive QMC rules.   Although true Fourier 
coefficients, $\hf(\bk)$, are rarely 
known, they may be approximated 
by discrete Fourier coefficients,
\begin{equation*}
\tf(\bk)  = \frac{1}n \sum_{i=1}^{n} f(\bx_i) \overline{\phi}_{\bk}(\bx_i)
\end{equation*}
The $n$ unique discrete Fourier coefficients can be evaluated with a computational cost of 
$\Order(n \log(n))$.  However, the index set $P^\perp_n \setminus\{\bzero\}$ contains large 
wavenumbers, for which the discrete Fourier coefficients approximate poorly the true Fourier 
coefficients.  So, PI and LlAJR defined a cone of reasonable integrands, $\cc$, 
whose Fourier coefficients do not decay erratically.  We omit the important technical details, but 
provide an intuitive explanation.  The true Fourier coefficients of a reasonable integrand
need not decay monotonically.  However, if the average size of the true Fourier coefficients for 
moderate-sized (relative to $n$) wavenumbers is small, then the average size of the true 
coefficients for large-sized wavenumbers must also be small.  Fig.\ \ref{GoodBadWalshFig} 
illustrates an example of a reasonable function and an unreasonable function and their Fourier 
(Walsh) coefficients.  Here and in the error bound below, $\{\bk(\kappa)\}_{\kappa = 1}^\infty$ 
denotes an ordering of 
the $d$-dimensional wavenumbers in $\bbK$.

\begin{figure}[h]
	\centering
	\includegraphics[width = 0.23\textwidth] 
	{ProgramsImages/FunctionWalshFourierCoeffDecay.eps} \ \ 
	\includegraphics[width = 0.23\textwidth] 
	{ProgramsImages/WalshFourierCoeffDecay128.eps} \ \ 
	\includegraphics[width = 0.23\textwidth] 
	{ProgramsImages/FilteredFunctionWalshFourierCoeffDecay.eps} \ \ 
	\includegraphics[width = 0.23\textwidth] 
	{ProgramsImages/WalshFourierCoeffDecayFilter.eps}
	\caption{One reasonable and one unreasonable function 
		%and their Fourier coefficients 
	\label{GoodBadWalshFig}}
\end{figure}

Under this definition of $\cc$, a 
theoretically justified, data-based error bound can be derived:
\begin{equation*}
\herr_n(f) = \fC(n) \sum_{\kappa = n/(2n_1) + 1}^{n/n_1} \abs{\tf(\bk(\kappa))}.
\end{equation*}
Here $\fC(n)$ is an inflation factor and $n_1$ is a power of $2$ used to define what is meant by 
``moderate-sized'' wavenumbers.  This data-driven error bound in terms of the discrete Fourier 
coefficients is used in \cite{HicJim16a,JimHic16a} to construct guaranteed adaptive QMC cubature 
algorithms: $n$ is increased by doubling until $\herr_n(f) \le \varepsilon$.  Then choosing 
$\fA(f,\varepsilon) = A_n(f)$ satisfies  error criterion 
\eqref{errorcrit}.  

If $n$ is the final number of nodes required, the computational cost of these adaptive QMC 
cubature algorithms  is $\Order\bigl(\$(f)n + n \log(n)\bigr)$, 
where $\$(f)$ denotes the cost of one function value.  For practical values of $n$,  $\$(f)n$ and 
$n \log(n)$ are comparable.

The PI, LlAJR, and DL have generalized the adaptive QMC rules to accommodate control variates.  
This means replacing the integrand $f$ by $f - \bbeta^T[\bg - S(\bg)]$, where $\bg$ is a vector 
function whose integral, $S(\bg)$, is known exactly.  Note that $S(f - \bbeta^T[g - S(g)]) = 
S(f)$ for any constant $\bbeta$.  The data-driven error bound now is $\herr_n(f -  
\bbeta^T[g - S(g)])$, and the choice of $\bbeta$ can significantly decrease or increase this error 
bound, 
and consequently the computational cost.  For IID MC cubature, a nearly optimal $\bbeta$ is 
computed by linear regression, but as pointed out by \cite{HicEtal03}, this is not necessarily the 
correct $\bbeta$ for QMC cubature.  The PI, LlAJR, and DL provide an alternative method for 
determining $\bbeta$ that looks directly at $\herr_n(f -  
\bbeta^T[g - S(g)])$ \cite{HicEtal17a}.

\subsubsection{Adaptive Bayesian Cubature}  Recent research by the PI and JR has assumed 
that the integrand in the multiple integration problem \eqref{multiInt} is an instance of a Gaussian 
Process with constant mean $m$, and covariance kernel, $C:[0,1]^d \times [0,1]^d \to \reals$, i.e., 
$f \sim \GP (m,C)$.  This approach to computational mathematics has been pursued by several 
scholars in the past \cite{Dia88a, OHa91a, RasGha03a, Rit00a} and has gained recent interest 
under the name \href{http://www.probabilistic-numerics.org}{probabilistic numerics}.  Under this 
approach one 
infers the mean, $m$, and the parameters defining the covariance kernel, $C$, using maximum 
likelihood estimation, and then constructs a credible interval in the Bayesian sense, $\Prob[\abs{S(f) 
- A_n(f)} \le \herr(f)] = 99\%$, where $A_n$ is the statistically best approximation to $S(f)$, and 
$\herr_n(f)$ depends on the function data and the inferred values of $m$ and $C$. 

A longstanding drawback of this approach has been the computational cost of the matrix operations 
involving the Gram matrix $\mC = \bigl(C(\bx_i,\bx_j)\bigr)_{i,j=1}^n$, which are ordinarily at least 
$\Order(n^3)$.  The 
contribution of the PI and JR has been to choose families of covariance kernels with Hilbert-Schmidt 
decompositions of the form $C(\bx,\bt) = \sum_{\bk \in \bbK} \lambda_\bk \phi_{\bk}(\bx) 
\phi_{\bk}(\bt)$, where the $\phi_{\bk}$ are the same as those used in the previous section.  The 
eigenvalues $\lambda_\bk$ must be chosen so that the infinite sum has a closed form.  Kernels of 
this form used with the corresponding QMC nodes yield Gram matrices $\mC$ for which the 
necessary vector-matrix operations can be accomplished with computational cost $\Order(n 
\log(n))$, making Bayesian cubature practical.  This work has been presented at conferences and 
as in the process of being written up for publication.

\subsubsection{Generalized Error Criteria}  Some practitioners may prefer the adaptive algorithm 
to satisfy a more general error 
criterion than \eqref{errorcrit}, say, 
\begin{multline}
\label{generrorcrit} \tag{G-CRIT}
\abs{V(S(\vf)) - \fA(\vf,\varepsilon, \reltol) } \le \max(\varepsilon, \reltol \abs{V(S(\vf))} ),  \\
 \vf: 
\Omega \to \reals^p, \ \ V: \reals^p \to \reals, \ \ S(\vf) = \bigl(S(f_1), \ldots, S(f_p) \bigr)^T, \ \  0 \le 
\reltol \le 1,
\end{multline}
where now the value sought, $V(S(\vf))$, is a function of say, the integral of $p$-vector function, 
$\vf$. Examples of situations where there the value sought is a function of several intergrals include 
Sobol' indices \cite{} and Bayesian inference \cite{}.  The approximate solution is acceptable if it 
satisfies either an absolute error tolerance, 
$\varepsilon$, \emph{or} a relative error tolerance, $\reltol$.  The PI, LlAJR, and DL showed in 
\cite{HicEtal17a} how to use data-driven error bounds, $\abs{S(f_j) - A_n(f_j)} \le \herr_{n}(f_j)$, $j = 
1, \ldots, p$,  to 
choose an appropriate $\fA(\vf,\varepsilon, \reltol)$.  In general, the 
best approximation is \emph{not} $V(A_n(\vf))$.  


\subsubsection{Multivariate Function Approximation}
GEF has led the effort in accurate, well-conditioned function approximation and solution of 
differential equations using reproducing kernel Hilbert space methods.  Much of his effort has been 
developing truncated Hilbert-Schmidt decomposition of the reproducing kernels using appropriate 
eigenfunctions.  For example, an expansion of the squared exponential (Gaussian) kernel using 
Chebyshev polynomials instead of Hermite polynomials has been used for the numerical solution of 
nonlinear unsteady 
convection-diffusion-reaction equations.  GEF and his collaborators have been extending 
decompositions of the Matern kernels on the half line to the entire real line, the first such analytical 
derivations.


\subsection{Broader Impacts from Previous NSF Funding}

\subsubsection{Publications , Conference Participation, and Conference Organization} This 
project has resulted in a number of publications by the PI, GEF, students, and collaborators listed at 
the beginning of this section.  We have also spoken at a number of applied mathematics, statistics, 
and computational science conferences and given colloquium/seminar talks to mathematics and 
statistics departments.  As highlights, the PI gave invited tutorials at the Twelfth International 
Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing (MCQMC 
2016) 
\cite{Hic17a}, a biennial conference for which he also serves on the steering committee.  The PI also 
serves as a program leader for the tatistical and Applied Mathematical Sciences 
Institute (SAMSI) 2017--18 
\href{https://www.samsi.info/programs-and-activities/year-long-research-programs/2017-18-program-quasi-monte-carlo-high-dimensional-sampling-methods-applied-mathematics-qmc/
}{Program on Quasi-Monte Carlo and High-Dimensional Sampling Methods for Applied 
	Mathematics (SAMSI-QMC)}.  He  gave an invited tutorial at the Opening Workshop, leads one of 
	the working groups, and participates in two others.  
	
	
The PI's leadership in the MCQMC conferences  and SAMSI-QMC is 
	designed to bring QMC to other areas of potential application.  One concrete example, was the 
	collaboration between the PI, LlAJR and French collaborators working in uncertainty quantification 
	that resulted in \cite{GilEtal16a, GilJim16b}.  Another example, is the collaboration of LlAJR with 
	high energy physicists at Fermilab, in the Chicago suburbs, where LlAJR was helping improve the 
	efficiency of their Monte Carlo methods by using QMC.
	
\subsubsection{GAIL Software} The theoretical results of this research have been implemented in 
GAIL, the open source MATLAB library hosted on Github at 
\href{http://gailgithub.github.io/GAIL_Dev/} {\nolinkurl{gailgithub.github.io/GAIL_Dev/}}. This software 
has been implemented with input parsing, input validation, unit tests, inline documentation, and 
demonstrations.  GAIL makes it easier for practitioners to try our new adaptive algorithms.  

\subsubsection{Boosting the STEM Workforce} The PI and GEF have mentored a number of 
research students associated with this project.  The main ones are mentioned at the beginning of 
this section.  Female students mentored include YD, LJ, XT, and XZ.   There have also been several 
undergraduate students who the PI, GEF, and SCC have mentored including more than a dozen 
Brazilian Science Mobility Program in the summers of 2015 and 2016, Tanner Johnson (BS, U 
Minnesota, PhD? student at U British Columbia), Tianci Zhu 
(female, BS IIT, MS student at NYU),  Cu Hauw Hung (BS student at Biola University, applying for 
graduate study), and Yueyi Li (BS student at Macalester University, applying for 
graduate study).  As part of our team, these students conducted theoretical an practical research 
that resulted in publications and new features for GAIL.  They have learned how to contribute to a 
substantial, long-term software project in a way that preserves the integrity of the software library.

The PI teaches a course on Monte Carlo methods each fall.  For the past several years students 
have been introduced to GAIL and used it in their coursework.  GAIL has been used to teach how to 
know how large $n$ must be for an IID MC or QMC simulation to reach the desired accuracy 
requirement.  Some students have chosen as their class project to improve GAIL.


\section{Intelletual Merit of Proposed Research} \label{secProposed}


\subsection{Higher Order Adaptive Algorithms for Univariate Problems}\label{SectUniProb}

\subsubsection{A Theoretically Justified Alternative to MATLAB's 
\textup{\texttt{integral.m}}} 
MATLAB's premier adaptive quadrature algorithm is based on composite high order Gaussian 
quadrature rules, so it converges much faster than our adaptive trapezoidal rule (Algorithm 
\ref{Trapalg}). However, as noted in Sect.\ \ref{TrapIllSect} and illustrated in Fig.\ 
\ref{quadfailfig}(c), 
\texttt{integral.m} can fail even for reasonable functions, which our Algorithm 
\ref{Trapalg} does not. The adaptive quadrature algorithms in the other popular scientific libraries, 
such as the NAG library, GSL, and SciPy, have the same defect as MATLAB's \texttt{integral.m}.  

YZ is already working on a locally adaptive Simpson's rule.  Our next step will be a 
locally adaptive high order algorithm that is competitive with MATLAB's \texttt{integral.m} but with 
rigorous data-driven error bounds and a stopping criterion that succeeds for all reasonable 
functions, i.e., all functions in a cone, $\cc$, to be defined.  Our goal is to provide replacement for 
MATLAB's \texttt{integral.m}.  

Rather than use Gauss quadrature rules, we will likely use composite Clenshaw-Curtis quadrature, 
which is based on integrating Chebyshev polynomial approximations to the function.  
Trefethen \cite{Tre08a} has pointed out that Clenshaw-Curtis quadrature is competitive with Gauss 
quadrature.  

\subsubsection{Chebfun and related algorithms}
The Chebfun software \cite{TrefEtal17a} approximates functions by Chebyshev polynomial series of 
sufficiently high degree.  These polynomials can then be used for integration, optimization, 
root-finding, and other numerical computations.  Although Chebfun claims to perform computations 
to nearly machine precision, there are no theoretical guarantees.

We propose to provide a theoretical basis for Chebfun by defining a cone of reasonable functions, 
$\cc$, for which 

\subsubsection{Algorithms based on piecewise approximation} 

\subsection{Adaptive Algorithms for Multivariate Problems}\label{SectMultiProb}

\subsubsection{Integration over domains with unbounded dimension}

\subsubsection{Function approximation using matching kernels and sampling schemes}

doit Roshan

\subsection{Adaptive Algorithms for Problems with Expensive Function Values} 
\label{SectExpensive}

\subsubsection{}




\section{Broader Impact of Proposed Research}\label{SectBroad}


\subsection{Contributions to Training, Mentoring and Other Human Resource Developments}

\begin{description}[leftmargin=0ex]
\item[Providing Research Experiences for Undergraduates and High School Students]\ We 
\linebreak[4]    strongly believe that students should be introduced to research before graduate 
school so that they can learn how to discover the unknown, something that is not taught well in a 
classroom. We request funds to support two summer REU students per year. Having advertised our 
REU opportunities for several years now has given us some visibility within the community and is 
prompting inquiries from prospective participants well before we even announce our latest program 
offerings. As in the past several years, we expect the NSF funds will serve as a catalyst for funds to 
support additional summer students. In choosing REU students we make a deliberate effort to build 
a diverse research environment by targeting female and underrepresented minority students as well 
as students from less research-focused institutions (see Sect.~\ref{SectPrevious}). We will also 
continue to receive well-prepared high school students to join our research group as we did for the 
past two summers.

\item[Preparing Students for Academic Careers] We consider mentoring to be a multi-faceted and potentially long-term process continuing even after the mentee has moved on from IIT.  For example, MM was an undergraduate student at IIT who collaborated with GEF during his PhD studies at Cornell U.  We will continue to mentor him as senior person for this proposal.
    %Similarly, although QY has left IIT, he has continued his collaboration with GEF and FJH.
    We have provided and will continue to provide SCC, a Research Assistant Professor at IIT and another senior person for this project, teaching and mentoring experience.  We will continue to find opportunities for special mentoring activities for our students, like those QY and YD participated in at the NSF-supported 14th International Conference on Approximation Theory in San Antonio organized by GEF in April 2013.  We  will continue our collaboration with Argonne National Laboratory, which has led to short-term and long-term opportunities for our PhD students and graduates.

\item[Preparing Students for Industry Careers]
In addition to preparing students for the academic landscape, we also help current students land competitive jobs in the business world. Our training in the areas of algorithm development and coding tends to give our students the needed edge. For example, WM graduated from Brown U in 2013 and then joined an internet startup company. AB, SD, YaL, and TS are working in the financial services industry.

\item[Supervising Visitors]
GEF has established contacts with several Italian universities attracting students and postdoctoral 
visitors to IIT for extended visits (see Sect.~\ref{SectPrevious}). Having lived in Hong Kong for 19 
years, FJH has contacts with Chinese scientists that have prompted several long-term visiting 
Chinese scholars and students to join our research.  These activities will continue.

\item[Giving Short Courses and Invited Lectures]
We will continue our active track record of providing lectures to students at various stages in their careers, ranging from high school to graduate school. These encourage students to enter STEM and encourage STEM students to engage in research.

\end{description}

\subsection{Contributions to Resources in Research, Education and the Broader Society}

The research we propose straddles mathematics, statistics, computer science, and applications in engineering and related fields.  The two PIs have complementary strengths that facilitate this interdisciplinary research.  GEF has expertise in approximation theory, meshfree methods, and numerical PDEs, while FJH has expertise in (quasi-)Monte Carlo methods, kernel-based methods, information-based complexity theory, and experimental design. Our expertise provides both an obligation and an opportunity to interact with a number of diverse communities. We envision the following contributions:

\begin{description}[leftmargin=0ex]
\item[Disseminating Research]
The research supported by this grant will result in publications in peer-reviewed journals in a broad spectrum of applied mathematics, computer science, statistics and engineering. These journals will include both those that emphasize theory and those that emphasize applications.

\item[Promoting Cones] The idea of guaranteed, adaptive algorithms via cones of input functions has broad potential application.  We will be promoting this idea among numerical analysts who develop new algorithms and analyze their computational costs, as well as among information-based complexity theorists who analyze the lower bounds on the complexity of numerical problems.

\item[Promoting the Hilbert--Schmidt SVD] Similarly, we will encourage other researchers to take advantage of the stability given to kernel-based methods by the Hilbert--Schmidt SVD, to use our code, and to join our research efforts (\citep{FMcC12} has already more than 40 citations on Google Scholar).

\item[Bridging Mathematics and Statistics]
This project touches on topics that are of interest to the statistics community: kriging, Monte Carlo methods, and design of experiments.  Historically, there has been relatively little interaction between numerical analysts and statisticians.  We have and will continue to engage the statistics community by speaking a their conferences and departmental seminars.  For example, in 2015 FJH will give a colloquium talk in the Department of Statistics at Virginia Tech and an invited talk at the $47^{\text{th}}$ Days of Statistics of the Soci\'et\'e Fran\c{c}aise de Statistique. The book \citep{FMcC15} by GEF and MM will contain several chapters bridging this gap.

\item[Collaborating with Engineers]
GEF and MM have an ongoing collaboration with engineering colleagues at U Palermo (see ). We 
have submitted two joint proposals to the Italian government and to U Palermo requesting support 
for our work (but without direct support for GEF and MM). Thus this collaboration will benefit from 
travel funds via the current proposal.
%Since kernel methods are becoming a rather popular numerical tool in science and engineering, other opportunities for collaborations outside mathematics frequently arise. For example, GEF participated in an incubator meeting on freeform surfaces organized by the Optical Society of America.

\item[Promoting the Well-being of the Broader Society]
 uses kernel methods for the solution of inverse problems arising in the detection of brain activity 
 from MEG or EEG data. This non-invasive diagnostic procedure might help doctors detect early 
 functional and neurophysiological markers of diseases (e.g., Alzheimer's). It might also lead to a 
 reduction in doctors' visits, shorter hospitalization periods and a greater longevity with overall 
 improved quality of life. Similarly, may enable better management of the Ebola epidemic.

\item[Organizing and Presenting at Conferences]
We and our students involved in this project will present our results at a variety of conferences and workshops.  These include: (i) specialized meetings focusing on approximation theory, complexity, experimental design, meshfree methods, and Monte Carlo methods; (ii) the national meetings of AMS, SIAM, and the statistical societies; and (iii) conferences devoted to application areas.  We are frequently invited to speak at such conferences, which will give our results a prominent hearing. We will also continue to organize specialized conferences or minisymposia within larger conferences.

\item[Writing Textbooks and Survey Papers]
GEF and MM are currently preparing \citep{FMcC15}. This book will provide an exposition of the theory and implementation of the Hilbert--Schmidt SVD along with numerous applications. The book will form a bridge to the GaussQR library \citep{McCFBG13} and may serve as a textbook for a graduate class on meshfree methods, such as MATH 590 at IIT. GEF and FJH occasionally publish survey articles (e.g., a 42 page paper on kernel-based methods \citep{Fasshauer11}).

\item[Refreshing Course Syllabi]
MATH 590 (Meshfree Methods), taught by GEF in the fall semester of every even-numbered year, just underwent a major change as the preparation of \citep{FMcC15} progresses.
MATH 565 (Monte Carlo Methods in Finance), taught every fall by FJH, has incorporated our new 
results on guaranteed (quasi-)Monte Carlo multivariate integration. In the future it will include our 
new results on control variates and guaranteed multi-level and (quasi-)Monte Carlo sampling (see ).
Current texts teach students to estimate the error of the trapezoidal rule, $T_n(f)$, by $[T_n(f)-T_{n/2}(f)]/3$ (see, e.g., \cite[pp.\ 223--224]{BurFai10}).  The arguments behind this estimate introduce the valuable concept of extrapolation, however, this is a flawed error estimate as pointed out in \cite{Lyn83} and highlighted by our recent work \cite{HicEtal14b}.  This same flawed idea underlies \Matlab's {\tt integral.m}.  We will urge numerical analysis textbook authors and educators to change the way that error estimation is taught based on our recent and proposed work.  These ideas will also enter our more traditional numerical analysis courses such as MATH 350 (Intro to Computational Math) or MATH 577 and 578 (Computational Math I \& II).  SCC and FJH will continue to develop MATH 573 (Reliable Mathematical Software) as a valuable course for our own students and an example that we wish to propagate to other universities.

\item[Creating Software and Collaborating with Software Developers]
GEF and MM have created the website \citep{McCFBG13} which serves as the home for the software on our stable algorithms built upon the Hilbert--Schmidt SVD. This \Matlab library is freely available and allows others to experiment with our code. Thus far, the library contains routines for Gaussian and iterated Brownian bridge kernels. As our research expands to other kernels and their eigenexpansions the resulting code will be added to the library. The GaussQR library also serves as a sandbox for students---especially REU students---to learn about our research and allows them to contribute pieces of their own work.
We will continue to develop GAIL \citep{ChoEtal14a} (now up to version 2) as part of our proposed research.  The GAIL software will serve the wider community that relies on numerical approximation and integration algorithms.  It will also demonstrate how adaptive algorithms ought to be implemented, which we hope will inspire and inform those working on adaptive algorithms for other mathematical problems.  We also expect our new algorithms to be incorporated into widely used numerical packages, as was done for our algorithm in \cite{HonHic00a} by \Matlab \citep{MAT8.4} and NAG \citep{NAG23}.  We will continue to discuss with software developers about these issues.

\item[Reaching Out]
GEF and FJH both have a record of reaching out to high school students which we plan to continue. The website \href{http://math.iit.edu/~openscholar/meshfree/}{\nolinkurl{math.iit.edu/~openscholar/meshfree/}} helps manage our internal and external communication and dissemination of research findings. Advertising for the summer REU experiences is also facilitated via this website.  Information about GAIL is at \href{http://code.google.com/p/gail}{\nolinkurl{code.google.com/p/gail}}.
\end{description}

\newpage
\clearpage
%\pagenumbering{arabic}
\setcounter{page}{1}

\bibliographystyle{spbasic}


{\renewcommand\addcontentsline[3]{} 
\renewcommand{\refname}{{\Large\textbf{References Cited}}}                   %%
\renewcommand{\bibliofont}{\normalsize}

\bibliography{FJH23,FJHown23,GregPapers}}
\end{document}
